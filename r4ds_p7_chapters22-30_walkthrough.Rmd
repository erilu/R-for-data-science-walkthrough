---
title: "R for Data Science Walkthrough Chapters 22-30"
author: "Erick Lu"
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
    fig_width: 5
    fig_height: 3.5
---

This my walkthrough for chapters 22-30 for the book: _R for Data Science_ by Hadley Wickham and Garrett Grolemund. Here I provide solutions to their exercises and some of my own notes and explorations.

# Chapter 22 - No Exercises

# Chapter 23

```{r}
library(tidyverse)
library(modelr)
```

The datasets that are used in this chapter are simulated datasets, including the one below (sim1)

```{r}
sim1
```

Below are the functions used in this chapter, written by Hadley for demonstration purposes:

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```


## 23.2.1 Exercises

### 1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

In the simulated dataset, there are a couple of outliers that are far displaced from the rest of the points. These outliers can skew the linear approximation, because these points are so 'distant' from the other points in the dataset. Because the linear model tries to minimize the distance between each point and the fitted model (the "residuals"), these outliers will skew the approximation, pulling the line closer to them. The larger the residual, the more it contributes to the RMSE. We notice that the fitted line is slightly skewed towards the direction of the outlying point.

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

# first, take a look at the data
sim1a
ggplot(sim1a, aes (x, y)) +
  geom_point()

mod <- lm(y~x, data = sim1a)
summary(mod)

# add the fitted linear model to the scatterplot
ggplot(sim1a, aes (x, y)) +
  geom_point()+
  geom_abline(intercept = mod$coefficients[1], slope = mod$coefficients[2])

# compare with the baseR lm with geom_smooth() overlay, looks like they overlap, as expected
ggplot(sim1a, aes (x, y)) +
  geom_point()+
  geom_abline(intercept = mod$coefficients[1], slope = mod$coefficients[2], size=3, color = "red")+
  geom_smooth(method = 'lm')

```


### 2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}
# use optim() function to 
best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par

# compare the parameters from optim() with the parameters obtained from lm()
mod <- lm (y~x, data = sim1a)
coef(mod)

# plot the two lines on the scatterplot to observe differences in fit
ggplot(data = sim1a, aes(x, y))+
  geom_point()+
  geom_abline(slope = mod$coefficients[2], intercept = mod$coefficients[1], color = "red")+
  geom_abline(slope = best$par[2], intercept = best$par[1], color = "blue")+
  labs(title = "Red = root-mean-squared distance fit using lm() \n Blue = mean-absolute distance fit using optim()")

```

Use optim() to fit this model to the simulated data above and compare it to the linear model.

The measure_distance() function provided above uses the absolute-mean distance (mean(abs(diff))) instead of the root-mean-squared distance, sqrt(mean(diff^2)). Using optim() and the absolute-mean distance, we find that the line is less skewed by the outlying points. The red line is "pulled" more towards the outliers, whereas the blue line remains more embbeded with the bulk of the data. This is because squaring the residuals results in much greater values when the residuals are large, so minimizing the residuals for outliers takes more priority when using the squared distance.

### 3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?

A quadratic or higher order function may have more than one local minimum / maximum. This may result in the optim() function providing an unideal result. In the provided function, since a[1] and a[3] are both constants that are not multiplied by a column in data (such as ```data$x```), they can be added together and represent the intercept of the line. This results in the sum of a[1] and a[3] equalling the intercept we found before using the equation ```a[1] + data$x * a[2]```.

a[1] and a[3] can therefore have infinite possibilites of values, as long as the sum of a[1] and a[3] are equal to the local optimum of ```a[1] + data$x * a[2]```. In the example below, if we use the dataset sim1, we find that a[1] and a[3] must sum to 4.220074. So, depending on where you start with the optim() function, a[1] and a[3] will have differing values, but still add up to 4.220074.

We see in the graph that the optim() function and lm() again provde the same result.

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

best <- optim(c(0, 0, 0), measure_distance, data = sim1)
best$par
best <- optim(c(0, 0, 1), measure_distance, data = sim1)
best$par

# since in the model above, a[1] and a[3] may be theoretically combined to represent the intercept of the line, we can graph it as such:
ggplot(data = sim1, aes(x, y))+
  geom_point()+
  geom_smooth(method = "lm", color = "red", size = 2)+
  geom_abline(slope = best$par[2], intercept = best$par[1] + best$par[3], color = "blue")+
  labs(title = "Red = using lm() \n Blue = optim() using the provided 3 parameter model")

```


## 23.3.3 Exercises

### 1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?

Using the loess() function instead of lm(), the line curves more towards the direction of variation and is not strictly a straight line. The default method for fitting using geom_smooth() is loess(), so the line that is superimposed on the ggplot is the same as the line generated by the predictions using the loess() model. When we superimpose all 3 options (loess() prediction, lm() prediction, and geom_smooth()), we see that geom_smooth() and the loess() prediction precisely overlap, whereas the lm() prediction does not.

```{r}
# using lm()
sim1_mod_lm <- lm(y ~ x, data = sim1)

grid <- sim1 %>% 
  data_grid(x) 
grid
grid <- grid %>% 
  add_predictions(sim1_mod_lm) 
grid

sim1 <- sim1 %>% 
  add_residuals(sim1_mod_lm)
sim1

ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)

ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 

```


```{r}
# using loess()
sim1_mod_loess <- loess(y ~ x, data = sim1)

grid <- sim1 %>% 
  data_grid(x) 
grid
grid <- grid %>% 
  add_predictions(sim1_mod_loess) 
grid

sim1 <- sim1 %>% 
  add_residuals(sim1_mod_loess)
sim1

# residuals plot
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 

# plot the regression line
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)

# compare to geom_smooth() and lm()
ggplot(sim1, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 3)+
  geom_smooth(color = "blue", se = F)+
  geom_smooth(method = "lm", color = "green", se = F)

```

### 2. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ?

add_predictions() can only work with one supplied model, in which it will generate a new column named "pred" in your data frame. gather_predictions() and spread_predictions() can work with multiple supplied models and generate predictions for each model supplied. gather_predictions() appends the model name to the data frame along with the predictions as new rows to the data frame, in a tidy fashion. spread_predictions() appends the new predictions to the data frame as separate columns. You can visualize the differences below, in which spread_predictions makes the data frame "wider" and gather_predictions() makes the data frame "taller".

```{r}
grid <- sim1 %>% 
  data_grid(x) 
grid

grid_add <- grid %>% 
  add_predictions(sim1_mod_lm) 
grid_add

grid_gather <- grid %>% 
  gather_predictions(sim1_mod_lm, sim1_mod_loess) 
grid_gather

grid_spread <- grid %>% 
  spread_predictions(sim1_mod_lm, sim1_mod_loess) 
grid_spread
```


### 3. What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

geom_ref_line() adds either a horizontal or vertical line at a specified position to your ggplot, of a specified color (default white). It comes from ```modelr```. This is useful when plotting residuals because ideally the residuals should be centered around 0. Having a reference line helps the viewer judge how the residuals behave. Conceptually, we can think of the horizontal line in the residuals plot as the prediction, and of the residual as how far off the true value is from that prediction.

### 4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

The residuals, ideally, should approximately be normally distributed. Examining the frequency polygon is useful as a visual assessment for whether or not the residuals follow the Normal distribution. This graph will also more easily capture any abnormal pattern in the residuals, in which there are over-representations of either + or - residuals. The cons are that this graph masks some of the variability in the residuals by binning them, and you lose the relationship between the residual and the predictor variable. This is best paired with a scatterplot of the residuals so you can observe exactly where each point lies in relation to the predictor variable.

```{r}
# example from book
ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)

# make an example where residuals approximate normal distribution
x = seq(1:1000)
y = 12 + 5 * x + rnorm(1000,0,100) # add random white noise
mysim <- as_tibble(cbind(x, y))

#fit model
mysim_mod <- lm(y ~ x, data = mysim)
summary(mysim_mod)

# make predictions
mygrid <- mysim %>% 
  data_grid(x) 
mygrid

mygrid <- mygrid %>% add_predictions(mysim_mod)
mygrid

mysim <- mysim %>% add_residuals(mysim_mod)
mysim

# plot prediction line
ggplot(mysim, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data =  mygrid, colour = "red", size = 1)

# plot freqpoly() of residuals
ggplot(mysim, aes(resid)) + 
  geom_freqpoly()

# plot scatterplot of residuals
ggplot(mysim, aes (x = x, y = resid))+
  geom_ref_line(h=0)+
  geom_point()

```


## 23.4.5 Exercises

For categorical variables, the book performs a similar prediction workflow:

```{r}
# use the sim2 dataset from modelr
sim2

# examine how the data are distributed
ggplot(sim2) + 
  geom_point(aes(x, y))

# generate a model (R automatically recognizes that the predictor variables are categorical)
mod2 <- lm(y ~ x, data = sim2)

# generate predictions based on model
grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)
grid

# plot the predictions overlaid on the graph
ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```

For using more than one predictor variable (can be a combination of categorical and continuous variables), a similar approach is used:

```{r}
# examine data and build models
sim3
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

# inputting multiple variables into data_grid results in it returning all the possible combinations between x1 and x2
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
grid

```

Inputting multiple continuous variables into data_grid will also return all possible combinations, unless you manually specify the combinations as done in the book chapter.

```{r}
# examine data and build models
sim4
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

# with manual specification of range
grid <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5), 
    x2 = seq_range(x2, 5) 
  ) %>% 
  gather_predictions(mod1, mod2)
grid

# without manual specification of range, provides all combinations of values
grid2 <- sim4 %>% 
  data_grid(
    x1, 
    x2
  ) 
grid2
```


### 1. What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions?

To fit a model to sim2 without an intercept, use "-1" with the formula, as shown. We observe that the model matrix loses the "intercept" column when we use "-1". We then generate predictions using gather_predictions() on both models. We find that both models generate the same predictions! This is because in both cases, the mean of the observations for each categorical possibility is the optimal fit. 

```{r}
mod1 <- lm(y ~ x, data = sim2)
model_matrix(sim2, y ~x)

mod2 <- lm(y ~ x - 1, data = sim2)
model_matrix(sim2, y ~x -1)

grid <- sim2 %>% 
  data_grid(x) %>% 
  gather_predictions(mod1, mod2)
grid

ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred, shape = model, color = model), size = 4)
```


### 2. Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is \* a good shorthand for interaction?

The model matrix for y ~ x1 + x2 for sim3 has an intercept, an x1 column, and 3 columns for x2, corresponding to each of the possibilities for the categories in x2. The model matrix for y ~ x1 \* x2 has an these columns as well, but also has x1:x2b, x1:x2c, and x1:x2d, which correspond to the interaction between x1 and each of the categories in x2.

For sim4, since the values of the predictor variables x1 and x2 are both continuous variables, the x2 column does not need to be split up as in sim3. The model matrix consists of 3 columns, an intercept, x1, and x2. Similarly, y ~ x1 \* x2 uses a model matrix of 4 columns, which consists of the 3 mentioned previously and an additional column x1:x2.

 is a good shorthand for this interaction because the additional columns that it adds are products. it suggests that changes in values of one variable will affect the value of the other, which is what it is trying to model.


```{r}
sim3
# models fit to sim3
model_matrix(sim3, y ~ x1 + x2 )
model_matrix(sim3, y ~ x1 * x2 )

sim4
# models fit to sim4
model_matrix(sim4, y ~ x1 + x2 )
model_matrix(sim4, y ~ x1 * x2 )

```


### 3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

The formulas below use the model matricies that were described in exercise 2 of this section. We can re-create the model matricies by writing a function that accepts the predictor variables as input, as well as the type of operator. In the example below, I input sim3 along with either "+" or "\*" and show that the model matricies generated match those made by the modelr function model_matrix().

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

# check to see if the column to add is a categorical variable (factor), and split it up if true
add_column <- function ( mycolumn, colname ) {
  # test whether the columns are factors or not
  if (is.factor (mycolumn)){
    my_levels <- levels(mycolumn)
    
    # split into separate columns
    output <- vector("list")
    for (i in 2: length(my_levels)) {
      level_name <- str_c(colname, my_levels[i])
      output[[level_name]] <- 1*(mycolumn == my_levels[i])
    }
  }
  # if not factor, return the column as-is
  else {
    output <- list( mycolumn)
    names(output) <- colname
  }
  output
}

# check the type of operator supplied (+ or *) and create the columns as necessary, calling the add_column function
make_matrix <- function (data, operator) {
  my_colnames <- c("x1", "x2")
  # store the columns of the model matrix in "mm"
  mm <- list()
  
  # make the default intercept column
  mm$intercept <- rep(1, length(data$x2))
  
  # add the base columns using add_column()
  for (item in my_colnames) {
    mm <- c(mm, add_column (data[[item]], item))
  }
  mm <- bind_cols(mm)
  
  # if the operator is *, add the appropriate columns based on vector multiplication
  if (operator == "*") {
    x1cols <- colnames(mm)[grep("x1", colnames(mm))]
    x2cols <- colnames(mm)[grep("x2", colnames(mm))]
    newcols <- vector("list")
    for (i in x1cols) {
      print(i)
      for (j in x2cols) {
        print(j)
        new_level_name <- str_c(i,j, sep = ":")
        print(new_level_name)
        newcols[[new_level_name]] <- mm[[i]]* mm[[j]]
      }
    }
    newcols <- bind_cols(newcols)
    mm <- cbind(mm, newcols)
  }
  mm
}

make_matrix( sim3, "+")
model_matrix( sim3, y ~ x1 + x2)

make_matrix( sim3, "*")
model_matrix( sim3, y ~ x1 * x2)
```

### 4. For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim?

To test which model does a better job, we can look at the residuals for each model by subtracting the predicted values for each model from the true values. A measurement to compare the models would be to calculate the RMSE using these residuals, and choose the model with the lower RMSE. Doing so, we find that mod2 seems to be a better fit. The RMSE for mod2 is 2.0636 whereas the RMSE for mod1 is slightly higher (worse fit), at 2.0998.

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

sim4 <- sim4 %>%
  spread_residuals (mod1, mod2)
sim4

ggplot(sim4, aes(x1, mod1)) + 
  geom_ref_line(h = 0) +
  geom_point() 

ggplot(sim4, aes(x1, mod2)) + 
  geom_ref_line(h = 0) +
  geom_point() 

sim4 %>% summarize ( mod1_rmse = sqrt(mean(mod1^2)),
                     mod2_rmse = sqrt(mean(mod2^2)))
```



# Chapter 24

For this chapter, the following packages and modifications to datasets were used:

```{r}
options(na.action = na.warn)
library(nycflights13)
library(lubridate)

diamonds2 <- diamonds %>% 
  filter(carat <= 2.5) %>% 
  mutate(lprice = log2(price), lcarat = log2(carat))
```


## 24.2.3 Exercises

### 1. In the plot of lcarat vs. lprice, there are some bright vertical strips. What do they represent?

The color of each hexagon in geom_hex corresponds to the number of observations that lie within the hexagon, which means that the bright vertical strips represent highly concentrated areas containing large amounts of observations relative to the dim hexagons. Hadley alludes to the possible reason why these stripes exist in the previous chapters, where he mentions that humans are inclined to report "pretty" intervals such as 1.0, 1.5, 2.0, etc, resulting in more observations lying on these intervals.

```{r}
ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50)
```

### 2. If log(price) = a_0 + a_1 \* log(carat), what does that say about the relationship between price and carat?

This equation suggests that there is a linear relationsihp between log(price) and log(carat), because the equation can be interpreted as a_0 being an intercept and a_1 being a "slope". It is harder to discern the relationship between the non-logged price and carat given this equation. The relationship between the original non-transformed variables may not necessarily be linear.

### 3. Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors?

Hadley extracts the diamonds with high / low residuals (abs(lresid2) > 1) in the code below. After examining these, we find that most of the outlier diamonds that are priced much lower than predicted have a specific flaw associated with them. For example, \$1262 predicted to be \$2644 has a huge carat size but has a clarity of I1 (worst clarity), of which there are not that many observations for in the dataset (707 observations for grade I1 vs 13055 observations for grade SI1). 

The diamonds in this list usually have a combination of very good qualities as well as very bad qualities. It could be that there is some multicollinearity in the model, in which some of the predictor variables (lcarat, color, cut, and clarity) are correlated with one another. This may result in the model breaking down when something when the values of two variables which usually are correlated do not follow the trend for any specific observation. For example, the diamond priced at 2366 is predicted to only be 774, but this is likely due to the fact that the diamond has one of the best clarity values, but has the worst possible color. Looking at a density plot of color vs clarity, we find that there are very few observations which have this combination of color and clarity, which may be why the model breaks down.

```{r}
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2)

diamonds2 <- diamonds2 %>% 
  add_residuals(mod_diamond2, "lresid2")

ggplot(diamonds2, aes(lcarat, lresid2)) + 
  geom_hex(bins = 50)

diamonds2 %>% 
  filter(abs(lresid2) > 1) %>% 
  add_predictions(mod_diamond2) %>% 
  mutate(pred = round(2 ^ pred)) %>% 
  select(price, pred, carat:table, x:z, lresid2) %>% 
  arrange(price)

diamonds2 %>% group_by(clarity) %>% summarize (num = n())

diamonds2 %>% ggplot(aes(color, clarity))+
  geom_bin2d()
```


### 4. Does the final model, mod_diamond2, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond?

Judging by the plot of the residuals, the model does a decent job at removing the patterns in the data (fairly flat, only a handful of residuals > 1 st. deviation away from 0) for the log-transformed version of the data. The model could be improved to reduce the variance of the residuals (compress the points toward h=0), in order to get more accurate predictions. However, since we aren't dealing with log-transformed money when buying diamonds in the real world, we should examine how the residuals look when transformed back into their true values. To do so, we calculate subtract the non-logged prediction value from the true value to get the non-transformed residual. Plotting these non-transformed residuals shows that the variability in residuals increases as carat increases (same is true for lcarat). I would have moderate faith in the model for diamonds less than 1 carat, but for diamonds greater than one carat, I would be cautious.  The model would be useful to determine whether you were being completely scammed, but it may not be that good for determining small variations in price.

```{r}
nontransformed_residual <- diamonds2 %>%   
    filter(abs(lresid2) < 1) %>% 
    add_predictions(mod_diamond2) %>% 
    mutate(pred = round(2 ^ pred)) %>%
    mutate(resid = price - pred) 

nontransformed_residual %>% ggplot( aes (carat, resid) )+
  geom_hex()

nontransformed_residual %>% 
  summarize (resid_mean = mean(resid),
             resid_sd = sd(resid),
             resid_interval_low = mean(resid) - 1.96* sd(resid),
             resid_interval_high = mean(resid) + 1.96* sd(resid),
             limit_upper = max(resid),
             limit_lower = min(resid),
             maxprice = max(price),
             minprice = min(price))

```


## 24.3.5 Exercises

```{r}
daily <- flights %>% 
  mutate(date = make_date(year, month, day)) %>% 
  group_by(date) %>% 
  summarise(n = n())
daily

ggplot(daily, aes(date, n)) + 
  geom_line()

daily <- daily %>% 
  mutate(wday = wday(date, label = TRUE))
mod <- lm(n ~ wday, data = daily)

daily <- daily %>% 
  add_residuals(mod)
daily %>% 
  ggplot(aes(date, resid)) + 
  geom_ref_line(h = 0) + 
  geom_line()

term <- function(date) {
  cut(date, 
    breaks = ymd(20130101, 20130605, 20130825, 20140101),
    labels = c("spring", "summer", "fall") 
  )
}

daily <- daily %>% 
  mutate(term = term(date)) 

```

### 1. Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalise to another year?

These days are all the day before major US holidays (Sep 2, 2013 is Labor Day) which fall exclusively on Mondays. There may be fewer than expected flights due to the extended weekend / other holiday-associated factors. For another year, these days can be found by looking for the day before the third Monday in January, fourth Monday in May, and first Monday in September.

### 2. What do the three days with high positive residuals represent? How would these days generalise to another year?

These days look like are also associated with major US holidays, with 11/30/2013 and 12/01/2013 being the weekend after Thanksgiving, and 12/28/2013 being the weekend after Christmas. The reason flights peak on these days may be due to families who had visited their relatives leaving to go back home. These can be generalized to another year by looking for the weekends after these holidays, which typically fall after the 4th week of November and December.

```{r}
daily %>% 
  top_n(3, resid)
```

### 3. Create a new variable that splits the wday variable into terms, but only for Saturdays, i.e. it should have Thurs, Fri, but Sat-summer, Sat-spring, Sat-fall. How does this model compare with the model with every combination of wday and term?

In order to split the saturdays into terms, I wrote an annotate_sat() function that takes the wday column and term column from daily and applies the appropriate suffix to each "Sat". Fitting this model (mod3) and comparing it to the mod1 and mod2 described in the chapter shows that there is a slight improvement from mod 1 (no term variable), but that the model does slightly worse than mod2 (each day is termed). The RMSE is midway between mod1 and mod2. 

```{r}
annotate_sat <- function (wday, term) {
  index <- which (wday == "Sat")
  wday <- as.character(wday)
  wday[index] <- str_c("Sat-", as.character(term)[index])
  wday
}

daily <- daily %>% mutate (wday_sat = annotate_sat(wday,term))
daily

mod1 <- lm(n ~ wday, data = daily)
mod2 <- lm(n ~ wday * term, data = daily)
mod3 <- lm(n ~ wday_sat, data = daily)

daily %>% 
  gather_residuals(no_term = mod1, all_cominbations = mod2, only_sat_term = mod3) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

sigma(mod1)
sigma(mod2)
sigma(mod3)

```


### 4. Create a new wday variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like?

The code below adds a column to the data frame that indicates which days are holidays (I've chosen the main US corporate holidays for 2013). Fitting a model to this results in an rmse value that is lower than the model in which only the saturdays are termed (goes down from 47.36 to 42.94), suggesting that we are doing at least a slightly better job. Looking at the graph, the residuals that spike along the holidays are smaller (but still quite visible). There could be more ways to optimize the model to minimize these residuals, suchs as annotating wday with the few days before/after each holiday.

```{r}
daily
annotate_sat_holiday <- function (date, wday, term) {
  index <- which (wday == "Sat")
  wday <- as.character(wday)
  wday[index] <- str_c("Sat-", as.character(term)[index])
  holidays <- ymd(20130101, 20130527, 20130704, 20130902, 20131111, 20131128, 20131225)
  holday_index <- which (date %in% holidays)
  wday[which (date %in% holidays)] <- "holiday"
  wday
}

daily <- daily %>% mutate( wday_sat_holiday = annotate_sat_holiday(date,wday,term))
daily

mod4 <- lm(n ~ wday_sat_holiday, data = daily)

daily %>% 
  gather_residuals(only_sat_term = mod3, sat_with_holiday = mod4) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

sigma(mod3)
sigma(mod4)

```

### 5. What happens if you fit a day of week effect that varies by month (i.e. n ~ wday * month)? Why is this not very helpful?

Adding the month variable to the model reduces the values of the residuals by a small amount (and subsequently, the rmse). I would say that it is slightly helpful to include. I can see why it is not extremely helpful, because it may be over-fitting the data, as there is not necessarily a good reason to assume that each individual month is associated with their own unique day-of-the-week trends. This adds a large amount of predictor variables to the model that are not all necessarily independent from each other.

```{r}
# add month column to daily
daily <- daily %>% mutate(month = factor(month(date)))
daily

mod5 <- lm(n ~ wday*month, data = daily)

daily %>% 
  gather_residuals(original_model = mod1,  with_month = mod5) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

#summary(mod5)
sigma(mod1)
sigma(mod5)

```


### 6. What would you expect the model n ~ wday + ns(date, 5) to look like? Knowing what you know about the data, why would you expect it to be not particularly effective?

The book displays what the model n ~ wday \* ns(date, 5) looks like, which factors in the relationship between wday and the time of year into the model. The model n ~ wday + ns(date, 5) uses the "+" operator instead of "\*", which means that this model does not account for the relationship between these two variables. I would expect that n ~ wday + ns(date, 5) does a worse job as a predictive model. Testing this out below, we see that the residuals are indeed larger. The number of predictor variables are also much fewer.

```{r}
library(splines)
mod6 <- MASS::rlm(n ~ wday * ns(date, 5), data = daily)
mod7 <- MASS::rlm(n ~ wday + ns(date, 5), data = daily)

summary(mod6)
summary(mod7)

daily %>% 
  gather_residuals(multiplicative_splines = mod6,  additive_splines = mod7) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

sigma(mod6)
sigma(mod7)
```


### 7. We hypothesised that people leaving on Sundays are more likely to be business travellers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away.

To explore this hypothesis, we first generate a new data frame from flights that contains a factor splitting up each day into early morning (12am-8am, 8am-12pm, 12pm-5pm, and 5pm-12am). We can then summarize the data to obtain the mean distance of flights that occurred during each of those intervals grouped by day. Plotting this using boxplots, we observe that flights taking off between 1am and 8am on Sundays have a much higher mean distance compared to all other categories. Contrary to our hypothesis that evening flights (5pm-12am) would be the longest distance-wise, we instead see that there are more Sunday early morning flights between 12am-8am.

```{r}
# add distance and time of flight to daily matrix
timeofday <- function(date) {
  cut(date, 
    breaks = c(0,7, 12, 17, 23),
    labels = c("12am-8am", "8am-12pm", "12pm-5pm","5pm-12am") 
  )
}

flights2 <- flights %>% mutate (date = make_date (year, month, day), timeofday = timeofday(hour)) %>% 
  group_by(date, timeofday) %>% 
  summarize (n = n(),
             mean_dist = mean(distance)) %>%
  mutate(wday = wday(date, label = TRUE))

flights2

ggplot(flights2, aes (x = wday, y = mean_dist))+
  geom_boxplot(aes(color = timeofday))
# 
# flights %>% mutate (date = make_date (year, month, day), timeofday = timeofday(hour)) %>% 
#   mutate(wday = wday(date, label = TRUE))%>%
#   ggplot( aes (x = wday, y = distance))+
#   geom_boxplot(aes(color = timeofday))
```


### 8. It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday.

The levels of a factor can be re-ordered using the factor() function and redefining the order of the levels by manually passing them into the levels argument. Below is an example of a function reordering the factors so that the week starts on monday.

```{r}
# before reordering the factors
ggplot(daily, aes(wday, n)) + 
  geom_boxplot()

reorder_week <- function (mydata) {
  mydata <- factor(mydata, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))
  mydata
}

# after reordering the factors
daily %>% mutate (wday2 = reorder_week(wday)) %>% ggplot( aes(wday2, n)) + 
  geom_boxplot()
```



# Chapter 25

```{r}
library(gapminder)
by_country <- gapminder %>% 
  group_by(country, continent) %>% 
  nest()

by_country

## wrapper function to model lifeExp by year
country_model <- function(df) {
  lm(lifeExp ~ year, data = df)
}

## add the model as a list-column to the nested data
by_country <- by_country %>% 
  mutate(model = map(data, country_model))
by_country

## calculate residuals based on the nested data and the associated model
by_country <- by_country %>% 
  mutate(
    resids = map2(data, model, add_residuals)
  )
by_country
resids <- unnest(by_country, resids)
resids %>% group_by(country) %>% summarise (rsme = sqrt(mean(resid^2)))

```


## 25.2.5 Exercises

### 1. A linear trend seems to be slightly too simple for the overall trend. Can you do better with a quadratic polynomial? How can you interpret the coefficients of the quadratic? (Hint you might want to transform year so that it has mean zero.)

We can fit a quadratic polynomial either by using the equation y ~ poly(x,2) or writing it out as y ~ I(x^2) + x. When we fit this model to the data, we see that the majority of the rmse values calculated using the residuals by country are reduced compared to the original linear model used in this chapter. This suggests that we are doing better.

```{r}
country_model_poly <- function(df) {
  lm(lifeExp ~ poly(year,2), data = df)
  # alternatively can use lm(lifeExp ~ I(year^2)+year, data = df) for raw polynomial
}

country_model_poly_centered <- function(df) {
  lm(lifeExp ~ poly(year-mean(year),2), data = df)
}

by_country <- by_country %>% 
  mutate(model2 = map(data, country_model_poly),
         model3 = map(data, country_model_poly_centered)) %>%
  mutate(resids2 = map2(data, model2, add_residuals))

## residual freq-poly plot
resids2 <- unnest(by_country, resids2)
resids2 %>% 
  ggplot(aes(year, resid)) +
    geom_line(aes(group = country), alpha = 1 / 3) + 
    geom_smooth(se = FALSE)

## R-squared value
glance <- by_country %>% 
  mutate(glance = map(model2, broom::glance)) %>% 
  unnest(glance, .drop = TRUE) 

glance %>% ggplot(aes(continent, r.squared)) + 
    geom_jitter(width = 0.5)
  
# rmse using the polynomial model
resids2 %>% group_by(country) %>% summarise (rmse = sqrt(mean(resid^2)))
# rmse using the original linear model
resids %>% group_by(country) %>% summarise (rmse = sqrt(mean(resid^2)))
```

When we compare the model summaries for the centered (mean 0) model vs the non-centered model, the estimates for the coefficients are the same. Poly() creates orthogonal polynomials for the fit. The coefficients may be interpreted as the weight associated with a unit of change in year.
 
```{r}
summary(by_country$model2[[1]])
summary(by_country$model3[[1]])
```


### 2. Explore other methods for visualising the distribution of R2 per continent. You might want to try the ggbeeswarm package, which provides similar methods for avoiding overlaps as jitter, but uses deterministic methods.

To use the ggbeeswarm package, simply replace geom_jitter() with geom_beeswarm(). The output is below. We can augment the graph by adding additional graphics such as a boxplot.

```{r}
library(ggbeeswarm)
glance %>% 
  ggplot(aes(continent, r.squared)) + 
    geom_beeswarm()

glance %>% 
  ggplot(aes(continent, r.squared)) + 
    geom_boxplot( aes(color = continent))+
    geom_beeswarm()

```

### 3. To create the last plot (showing the data for the countries with the worst model fits), we needed two steps: we created a data frame with one row per country and then semi-joined it to the original dataset. It’s possible to avoid this join if we use unnest() instead of unnest(.drop = TRUE). How?

Instead of using unnest(glance,.drop = TRUE), using unnest(glance) will retain the other columns in the dataset in addition to unnesting the glance column. This means that the list-column "data" will still be associated with the glance output, which contains the `year` and `lifeExp` variables that are needed to plot the final graph. We can then perform the filtering on the r.squared value, unnest the data, and then plot the graph, as shown below, without needing to semi-join.

```{r}
glance <- by_country %>% 
  mutate(glance = map(model, broom::glance)) %>%
  unnest(glance)
glance

bad_fit <- filter(glance, r.squared < 0.25)
bad_fit

bad_fit %>% unnest (data) %>%
  ggplot(aes(year, lifeExp, colour = country)) +
    geom_line()
```


## 25.4.5 Exercises

### 1. List all the functions that you can think of that take a atomic vector and return a list.

enframe() or as_tibble() will convert an atomic vector into a list. The map() function will return a list in which it applies a function that you specify on each element of the atomic vector (examples below). stringr functions return lists of strings, such as str_split(). You may also encounter package-specific functions which may create their own types of objects. For example, in the bioinformatics world, you may have DESeq2 objects (bulk RNA-sequencing) or Seurat objects (single cell RNA-sequencing), which are S4 objects which contain several data types, including lists. Other functions that return lists are those shown in this chapter, such as broom::glance(), although in this instance the input is a model and not an atomic vector. split() also returns a list when applied to a dataframe.

```{r}
my_atomic_vector <- c("hello","world")

map(my_atomic_vector, length)
str_split(my_atomic_vector, "l")

typeof(as_tibble(my_atomic_vector))
typeof(enframe(my_atomic_vector))
summary(my_atomic_vector)

mtcars %>% 
  split(.$cyl)
```


### 2. Brainstorm useful summary functions that, like quantile(), return multiple values.

Summary functions in addition to quantile() include: summary(), range(), dim(), and coef() for linear models.

```{r}
x <- 1:10
summary(x)
range(x)
dim(mtcars)
coef(lm(mpg ~cyl, data = mtcars))


```


### 3. What’s missing in the following data frame? How does quantile() return that missing piece? Why isn’t that helpful here?

The data frame shows the output of unnesting the column `q`, which contains the quantile() values for each cylinder type in mtcars stored as a list-column. The important information that is missing is the label corresponding to each of the quantile values. For example, 21.4 (the first entry) corresponds to the 0% quantile. Without knowing how the function works, someone looking at the table would not know this important information. quantile() returns this information by naming the output vector with these labels. This is not helpful when creating list-columns, because the names are not stored automatically. 

```{r}
# what quantile should return
quantile(mtcars[which(mtcars$cyl==4),"mpg"])
names(quantile(mtcars[which(mtcars$cyl==4),"mpg"]))

# quantile is missing the labels for each of the statistics (0%, 25%, ...)
mtcars %>% 
  group_by(cyl) %>% 
  summarise(q = list(quantile(mpg))) %>% 
  unnest()

mtcars %>% 
  group_by(cyl) %>% 
  summarise(q = list(quantile(mpg))) %>% 
  unnest()
```

### 4. What does this code do? Why might might it be useful?

This code will group the dataset mtcars by cylinder type (in this case, 4, 6, or 8), then aggregate the values for each of the columns into a list. Specifically, there are 11 rows in the dataset that have a cyl value of 4, 7 rows that have a cyl value of 6, and 14 rows with a cyl value of 8. This is why the entries now look like <dbl [11]>, <dbl [7]>, and <dbl [14]>, respectively. Retrieving the value of <dbl[11]> at the first row of column `mpg`, for example, will return the 11 values associated with cylinder 4.

```{r}
mtcars2 <- mtcars %>% 
  group_by(cyl) %>% 
  summarise_each(funs(list))
mtcars2
mtcars2$mpg[1]
```

## 25.5.3 Exercises

### 1. Why might the lengths() function be useful for creating atomic vector columns from list-columns?

The lengths() function will return the length of each element in a list. This differs from the length() function which will return the number of elements contained in a list. For an example of the differences, see the code chunk below. However, for whatever reason, when lengths() is used with mutate() on a list-column, it will return the number of elements in the list rather than returning the vector with the lengths for each element in the list. In other words, when used with mutate(), lengths() performs what we normally would have thought length() would do. Even more confusing is how when length() is used with mutate, it breaks down and now returns the number of rows in the data frame regardless of what is stored in each row.

```{r}
lengths(list(a = 1:10, b = 2, z = 5))
length(mtcars)
lengths(mtcars)

df <- tribble(
  ~x,
  list(a = 1:10, b = 2, z = 5),
  list(a = 2:5, c = 4),
  list(a = 1, b = 2)
)

# lengths() returns the number of elements in the list-column
df %>% mutate(
  length_of_a = lengths(x)
)

# length() returns the number of rows regardless of the value in the list-column
df %>% mutate(
  length_of_a = length(x)
)

```

### 2. List the most common types of vector found in a data frame. What makes lists different?

In a data frame, I usually encounter numerics, characters, and factors, in which there is just a single value at any row x column designation. Lists are different because they can contain any number of data types, and multiple values for each. You can even have a list of lists! This means that a very diverse data set can be stored within lists in a single column of your data frame. That is the beauty of list-columns that this chapter tries to highlight.



# Chapter 26 - No Exercises

# Chapter 27

## 27.2.1 Exercises

### 1. Create a new notebook using File > New File > R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output.

### 2. Create a new R Markdown document with File > New File > R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update.

### 3. Compare and contrast the R notebook and R markdown files you created above. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other?

### 4. Create one new R Markdown document for each of the three built-in formats: HTML, PDF and Word. Knit each of the three documents. How does the output differ? How does the input differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.)

## 27.3.1 Exercises

### 1. Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold.

### 2. Using the R Markdown quick reference, figure out how to:

* Add a footnote.
* Add a horizontal rule.
* Add a block quote.

### 3. Copy and paste the contents of diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown in to a local R markdown document. Check that you can run it, then add text after the frequency polygon that describes its most striking features.

## 27.4.7 Exercises

### 1. Add a section that explores how diamond sizes vary by cut, colour, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo = FALSE on each chunk, set a global option.

### 2. Download diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes.

### 3. Modify diamonds-sizes.Rmd to use comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats.

### 4. Set up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache = TRUE, then verify your understanding of caching.


# Chapter 28

## 28.2.1 Exercises

### 1. Create one plot on the fuel economy data with customised title, subtitle, caption, x, y, and colour labels.

### 2. The geom_smooth() is somewhat misleading because the hwy for large engines is skewed upwards due to the inclusion of lightweight sports cars with big engines. Use your modelling tools to fit and display a better model.

### 3. Take an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand.


## 28.3.1 Exercises

### 1. Use geom_text() with infinite positions to place text at the four corners of the plot.

### 2. Read the documentation for annotate(). How can you use it to add a text label to a plot without having to create a tibble?

### 3. How do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: think about the underlying data.)

### 4. What arguments to geom_label() control the appearance of the background box?

### 5. What are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options.


## 28.4.4 Exercises

### 1. Why doesn’t the following code override the default scale?

```{r}
ggplot(df, aes(x, y)) +
  geom_hex() +
  scale_colour_gradient(low = "white", high = "red") +
  coord_fixed()
```


### 2. What is the first argument to every scale? How does it compare to labs()?

### 3. Change the display of the presidential terms by:

* Combining the two variants shown above.
* Improving the display of the y axis.
* Labelling each term with the name of the president.
* Adding informative plot labels.
* Placing breaks every 4 years (this is trickier than it seems!).

### 4. Use override.aes to make the legend on the following plot easier to see.
```{r}

ggplot(diamonds, aes(carat, price)) +
  geom_point(aes(colour = cut), alpha = 1/20)
```

# Chapters 29, 30 - No Exercises

---

Thanks for reading! This concludes my walkthrough of the book.