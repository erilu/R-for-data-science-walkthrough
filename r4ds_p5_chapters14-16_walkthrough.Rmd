---
title: "R for Data Science Walkthrough Chapters 14-16"
author: "Erick Lu"
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
    fig_width: 5
    fig_height: 3.5
---

This my walkthrough for chapters 14-16 for the book: _R for Data Science_ by Hadley Wickham and Garrett Grolemund. Here I provide solutions to their exercises and some of my own notes and explorations.

# Chapter 14 Strings

```{r}
library(tidyverse)
```

For Chapter 14, since the github_document R markdown format does not support html output, I will comment out the "str_view()" commands and replace them with str_extract() or str_subset(), as appropriate. This will still let you observe which words or strings are matched by the regex. You can also type the str_view() command into your own RStudio instance to observe the output.

## 14.2.5 Exercises

### 1. In code that doesn’t use stringr, you’ll often see paste() and paste0(). What’s the difference between the two functions? What stringr function are they equivalent to? How do the functions differ in their handling of NA?

The paste() function and paste0() functions are used to concatenate vectors after converting to character. In other words, combine converted strings together into one string. paste0() differs from paste() in that it always combines the strings without  a separating value, whereas paste() allows you to specify the ```sep``` argument. When sep = "", paste() provides the same output as paste0(). The stringr function they are equivalent to is str_c(). 

```{r}
p = NA
paste("the value of p is ", p)
str_c("the value of p is ", p)
```

### 2. In your own words, describe the difference between the sep and collapse arguments to str_c().

The sep argument allows you to specify what types of characters will lie between the items being joined. For example, you can choose to separate things with commas rather than the default no-space "". These separators will be inserted between each separate item fed into str_c, and will not be placed if a pre-vectorized argument is provided. If you have a premade vector that you want to join into one string with a specified separator, the collapse argument should be used, in which you specify the type of separator you want inserted. Below is an example illustrating these points.

```{r}
join_me <- c(88, "hello", "world")
str_c(88, "hello", "world")
str_c(88, "hello", "world", sep = ", ")
# sep does not place the commas if a premade vector is fed into str_c
str_c(join_me, sep = ", ")

# use collapse instead if you want to place separators into premade vector
str_c(join_me, collapse = ", ")

```


### 3. Use str_length() and str_sub() to extract the middle character from a string. What will you do if the string has an even number of characters?

To extract the middle character from a string, we can use str_length()/2 to find the middle index of the string, then use str_sub() to extract the character at that index. For even character strings, it would make sense to extract the two characters at the very center of the string. I wrote a function get_middle() that would test whether a string has an even number or odd number of characters using %%, then apply the appropriate str_sub() command to get the middle character or middle two characters.

```{r}
get_middle <- function (my_str) {
  if (str_length(my_str)%%2 != 0){
    return (str_sub(my_str, ceiling(str_length(my_str)/2), ceiling(str_length(my_str)/2)))
  } else {
    return (str_sub(my_str, ceiling(str_length(my_str)/2), ceiling(str_length(my_str)/2+1)))
  }
}

get_middle ("qwert")
get_middle ("qwerty")
```

### 4. What does str_wrap() do? When might you want to use it?

str_wrap(), as the name suggests, wraps a string into a paragraph. You can specify how wide to make each line of the paragraph using the width argument. It does so by inserting a newline ```'\n'``` at the appropriate positions. This would be useful for printing very long strings. Below is an example of using str_wrap().

```{r}
long_string <- 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.'

long_string
str_wrap(long_string, width = 30)

```

### 5. What does str_trim() do? What’s the opposite of str_trim()?

str_trim() will remove whitespace from the start and end of a string. The opposite of str_trim() is str_pad(), which can be used to add whitespace to either the left, right, or both sides of a string. Below is an example of using both functions.

```{r}
whitespace <- "  why is there whitespace on both sides?   "
whitespace
str_trim(whitespace)
str_pad("add some whitespace to the left!", 40, side = "left")
```

### 6. Write a function that turns (e.g.) a vector c("a", "b", "c") into the string a, b, and c. Think carefully about what it should do if given a vector of length 0, 1, or 2.

We can write a function that checks the length of the vector using length() and uses if else statements to perform the appropriate action. For vectors of length 0, we should return an empty string "". For vectors of length 1, we should just return the single item as-is. For vectors of length > 1, we should return the output specified in the question: "item-1, item-2, ... item-n-1, and item-n". To combine the items in the vector appropriately, we can first collapse the first n-1 items using str_c( collapse = ", "), then use str_c on this collapsed string with ", and ", and with the last item in the vector. This will insert the commas at the right locations, as well as the word "and" right before the last item in the vector.

```{r}
# function name is convert_str
convert_str <- function (str_vector) {
  if (length(str_vector) == 0) {
    return ("")
  } else if (length (str_vector) == 1) {
    return (str_vector[1])
  } else {
    combined_minus_last <- str_c(str_vector[1:(length(str_vector)-1)], collapse = ", ") # collapse all items except the last one
    return( str_c (combined_minus_last, ", and ", str_vector[length(str_vector)])) # concatenate the word "and" and the last item
  }
}

convert_str(c("fee", "fi", "fo", "fum"))
convert_str(c("fee"))
convert_str(c())
```



## 14.3.1.1 Exercises

### 1. Explain why each of these strings don’t match a ```\```: ```"\", "\\", "\\\"```.

To match a literal ```\```, you need to use four backslashes, ```"\\\\"```. One backslash, two backslashes, and 3 backslashes wont work! One backslash ```"\"``` won't work because it will "escape" the quotation, resulting in an error. Using 3 backslashes ```"\\\"``` will also result in an error, and using 4 backslashes ```"\\\\"``` will finally let us match the single literal ```\```.

```{r}
literal_backslash <- "hello_\\_world"
# str_view(literal_backslash, "\\\\")
str_extract(literal_backslash, "\\\\")

# errors
# x <- "\"
# x <- "\\\"
```

### 2. How would you match the sequence ```"'\```?

I would use the regex: ```"\"\'\\\\"```. This escapes the ", escapes the ', then uses the four backslashes to identify the literal ```\```.

```{r}
example <- "hello_\"\'\\_world"
writeLines(example)
# str_view(example, "\"\'\\\\" )
str_extract(example, "\"\'\\\\" )
```

### 3. What patterns will the regular expression ```\..\..\..``` match? How would you represent it as a string?

If interepreted as a regular expression and not as the string-form of a regular expression, this will match a period ```\.```, followed by any character ```.```, followed by another period, any character, another period, and then any character. Below, I've represented it as a string in R. Because of the backslash convention, it will error if you try to plug ```\..\..\..``` in directly.

```{r}
# test <- "\..\..\.." # errors

# need to double backslash within the string
test <- "\\..\\..\\.."
writeLines(test)
```

## 14.3.2.1 Exercises

### 1. How would you match the literal string ```"$^$"```?

Below, i use ```"\\$\\^\\$"``` to match the literal string ```"$^$"```. This is because you have to double backslash the ```$``` and ^ characters, which usually serve the purpose of matching the end or start of strings, respectively.

```{r}
test <- "$^$hello"
writeLines(test)
# str_view(test, "\\$\\^\\$")
str_extract(test, "\\$\\^\\$")
```


### 2. Given the corpus of common words in stringr::words, create regular expressions that find all words that:

* Start with “y”.
* End with “x”
* Are exactly three letters long. (Don’t cheat by using str_length()!)
* Have seven letters or more.
Since this list is long, you might want to use the match argument to str_view() to show only the matching or non-matching words.

To match words that start with y, use the "^" anchor before "y". To match words that end with x, use the ```"\$"``` anchor after "x". To match words that are exactly three letters long, we can use both anchors "^" and ```"\$"``` wrapped around 3 dots, which specify a series of any 3 characters. To match words that have seven letters or more, we can use the "^" anchor plus seven dots, which specify a series of any seven characters.

```{r}
# display a sample of stringr::words
head(words)

# starts with "y"
# str_view(words, "^y", match = T)
str_subset(words, "^y")
# ends with "x"
# str_view(words, "x$", match = T)
str_subset(words, "x$")
# are exactly 3 letters long
# str_view(words, "^...$")
str_subset(words, "^...$") %>% head(10)
# have seven letters or more
# str_view(words, "^.......")
str_subset(words, "^.......") %>% head(10)
```


## 14.3.3.1 Exercises
### 1. Create regular expressions to find all words that:

* Start with a vowel.
* That only contain consonants. (Hint: thinking about matching “not”-vowels.)
* End with ed, but not with eed.
* End with ing or ise.

To find words that start with vowels, we can use the "^" anchor along with a selection of vowels using the brackets. To find words that only contain consonants, we can specify that, from start to end (using ^ and ```\$```), there are no vowels (using ^ within brackets, along with the + sign, which means 1 or more of something). To find words that end in ed, but not eed, we can specify that we want words that end with ed, but have anything other than an e beforehand using [^e]. For words that end in ing or ise, we can use the "or" character "|" to combine two separate regexes, one that looks for ```ing\$``` and one that looks for ```ise\$```. Below are the regexes I have described in words above.

```{r}
# starts with a vowel
# str_view(words, "^[aeiou]")
str_subset(words, "^[aeiou]") %>% head(10)
# only contain consonants
# str_view(words, "^[^aeiou]+$", match = T)
str_subset(words, "^[^aeiou]+$")
# ends with ed, but not eed
# str_view(words, "[^e]ed$", match = T)
str_subset(words, "[^e]ed$")
# ends with ing or ise
# str_view(words, "ing$|ise$", match = T)
str_subset(words, "ing$|ise$")
```

### 2. Empirically verify the rule “i before e except after c”.

To examine whether any words in the dataset break this rule, we can search for words with the sequence of characters "cie", which contain an i before e even after a c. This yields the words "science" an "society". This tells us that the rule is not 100% correct! Searching for "cei", which will show words that adhere to the rule, yields "receive", which tells us that the rule might be correct for certain words. Is i always before e, even after something that is not c? Searching for words that do not follow this convention (i AFTER e, after anything other than a c) using the regex "[^c]ei" yields the word "weigh", which again disproves the rule. Searching for words that follow the rule (ie before e, after anything other than a c) using the regex "[^c]ie" yields many results, which shows us that the rule applies for most words, but not all of them.

```{r}
# list words that contain i before e, after a c. if this yields matches, then the rule is false!
# str_view(words, "cie", match = T)
# str_view(words, "cei", match = T)
# str_view(words, "[^c]ei", match = T)
# str_view(words, "[^c]ie", match = T)

str_subset(words, "cie")
str_subset(words, "cei")
str_subset(words, "[^c]ei")
str_subset(words, "[^c]ie")
```

### 3. Is “q” always followed by a “u”?

In the words dataset, we can identify words that contain q followed by any character using the regex "q.". All the results contain "q" followed by a "u". Alternatively, we can identify words that contain q followed by anything other than a "u", using "q[^u]". Because this does not yield any results, it does seem like "q" is followed by a "u" in this dataset.

```{r}
# list words that contain q followed by any character
# str_view(words, "q.", match = T)
str_subset(words, "q.")
# list words that contain q followed by something other than u (no matches)
# str_view(words, "q[^u]", match = T)
str_subset(words, "q[^u]")
```

### 4. Write a regular expression that matches a word if it’s probably written in British English, not American English.

Some common differences between British and American English include colour instead of color, so we could look for the sequence "lou". Other differences include "summarize" vs "summarise", so we could also include a search for the sequence "ise". Of course, this will also fetch words such as "practice" that do not have two forms.

```{r}
# str_view(words, "lou|ise", match = T)
str_subset(words, "lou|ise")
```


### 5. Create a regular expression that will match telephone numbers as commonly written in your country.

Phone numbers in the US can have the form (XXX) XXX - XXXX. Below is the regex that can be used to identify, very strictly, numbers that adhere to this exact form (including spacing and parentheses). Note the double backslash required to specify a digit. There is a more efficient way to write the regex below using brackets ```{}```, but that is covered in the next section!

```{r}
# str_view ( c("(123) 456 - 7890 is a fake number,","and (555) 555 - 5555 is also a fake number"), "\\(\\d\\d\\d\\)\\s\\d\\d\\d\\s-\\s\\d\\d\\d\\d")
str_extract ( c("(123) 456 - 7890 is a fake number,","and (555) 555 - 5555 is also a fake number"), "\\(\\d\\d\\d\\)\\s\\d\\d\\d\\s-\\s\\d\\d\\d\\d")

# same regex using brackets
# str_view ( c("(123) 456 - 7890 is a fake number,","and (555) 555 - 5555 is also a fake number"), "\\(\\d{3}\\)\\s\\d{3}\\s-\\s\\d{4}")
str_extract ( c("(123) 456 - 7890 is a fake number,","and (555) 555 - 5555 is also a fake number"), "\\(\\d{3}\\)\\s\\d{3}\\s-\\s\\d{4}")

writeLines(c("(\\d\\d\\d)\\d\\d\\d-\\d\\d\\d\\d", "\\(\\d{3}\\)\\s\\d{3}\\s-\\s\\d{4}"))
```


## 14.3.4.1 Exercises

### 1. Describe the equivalents of ```?, +, *``` in ```{m,n}``` form.

? is equivalent to ```{0,1}``` in that it matches either 0 or 1 occurances of the preceding regex or character. + is equivalent to ```{1,}``` in that it matches 1 or more occurances of the character/regex. * is equivalent to ```{0,}``` in that it matches from 0 to any number of occurances of the character/regex.

### 2. Describe in words what these regular expressions match: (read carefully to see if I’m using a regular expression or a string that defines a regular expression.)

* ```^.*$```
This matches any series of characters of 0 to any length. The anchor ^ specifies the start of the string, the . specifies any character, and the ```*``` specifies that the previous regex, (.), can be of 0 to any length, and the ```$``` specifies the end of the string.

* ```"\\{.+\\}"```
This matches a literal bracket, ```{```,  followed by 1 or more series of any characters, followed by a literal closing bracket, ```}```. This will match strings like: ```"{hello world}"```.

* ```\d{4}-\d{2}-\d{2}```
This will match 4 digits, followed by a hyphen, two digits, a hyphen, and another 2 digits. For example: "5555-55-55"

* ```"\\\\{4}"```
This will match a literal backslash repeated 4 times. For example: ```\\\\```.

Here are the above regexes and some sample input so you can see what types of character sequences they match:

```{r}
# str_view(words, "^.*$")
# str_view("hello world, {hello world} hello world", "\\{.+\\}")
# str_view("5555-55-55asdf", "\\d{4}-\\d{2}-\\d{2}")
# str_view("\\\\\\\\asdf", "\\\\{4}")

str_subset(words, "^.*$") %>% head(10)
str_extract("hello world, {hello world} hello world", "\\{.+\\}")
str_extract("5555-55-55asdf", "\\d{4}-\\d{2}-\\d{2}")
str_extract("\\\\\\\\asdf", "\\\\{4}")
```

### 3. Create regular expressions to find all words that:

* Start with three consonants.
* Have three or more vowels in a row.
* Have two or more vowel-consonant pairs in a row.

```{r}
# three consonants
# str_view("street", '[^aeiou]{3}')
str_extract("street", '[^aeiou]{3}')

#three or more vowels in a row:
# str_view("streeet", '[aeiou]{3,}')
str_extract("streeet", '[aeiou]{3,}')

# two or more vowel-consonant pairs in a row:
# str_view("streettttt", '([aeiou][^aeiou]|[^aeiou][aeiou]){2,}')
str_extract("streettttt", '([aeiou][^aeiou]|[^aeiou][aeiou]){2,}')
```

### 4. Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner.

Here are the solutions with some explanations to how I arrived at the answers:

Beginner puzzle 1, Beatles: row 1: HE|LL|O+, row2: [PLEASE]+, column 1: [^SPEAK]+, column 2: EP|IP|EF. Solution: row1: HE, row2: LP.

First examine row 1 vs column 2- column 2 can either start with the letter E or I, but the only option in row 1 that will fit with this requirement is "HE". For row 2, the only letter that can be in the bottom left is L, since P, E, A, and S are not allowed by column 1. The bottom right can only contain letters from PLEASE, which means that the option EF for column 2 is not allowed. This leaves EP as the only possibility for column 2. So row 2 must be "LP".

Beginner puzzle 2, Naughty: row 1: ```.*M?O.*```, row2: (AN|FE|BE), column 1: ```(A|B|C)\1```, column 2: (AB|OE|SK). Solution: row1: BO, row2: BE.

Row 1 specifies that there can be 0-1 of any character, followed by 0-1 of M, followed by O, followed by 0-1 of any character. The only stringent requirement is that there exist an O in row 1. Looking at column 1, this does not allow the letter O, so O must be in column 2 of row 1. The only option that allows this is the "OE" option in column 2. The two options for row 2 that end with letter E are "FE" or "BE". However, column 1 dictates that the first letter of row2 must be either A, B, or C. The only option that fits this is "BB" for column 2.

Beginner puzzle 3, Ghost: row 1: ```(.)+\1```, row2: [^ABRC]+, column 1: [COBRA]+, column 2: (AB|O|OR)+. Solution: row1: OO, row2: OO.

Row 1 specifies that 1 or more of any character should be repeated. Given the 2x2 box, this means a pair of letters. Column 1 gives COBRA as choices, but row2 specifies that ABRC cannot be present. This leaves us with O. Also, for column 2, the only option that is available that would not violate row2's rules is O+.

Beginner puzzle 4, Symbolism: row 1: ```[*]+```, row2: /+, column 1: .?.+, column 2: .+. Solution: row1: ```**```, row2:```//```.

Row1 wants 1 or more ```*```, whereas row 2 wants one or more /. Column 1 states that any character can occur 0-1 times followed by another character at least 1 or more times. Column 2 states that any character must appear 1 or more times.

Beginner puzzle 5, Airstrip One: row 1: 18|19|20, row2: ```[6789]\d```, column 1: ```\d[2480]```, column 2: 56|94|73. Solution: row1: 19, row2: 84.

The only choice for row1 that would satisfy the requirements for column2 is 19 (row1 ends with 9 and col 2 begins with 9). This means that column2 downwards is 94. The only choice for the bottom left that satisfies row2, and column 1 is 8. This leaves us with 19 in row1 and 84 in row2 as the answer.

## 14.3.5.1 Exercises

### 1. Describe, in words, what these expressions will match:

* ```(.)\1\1```

This will match any character repeated 3 times. For example, "ooo" in the string "woohooo!"
```{r}
# str_view("woohooo!", "(.)\\1\\1")
str_extract("woohooo!", "(.)\\1\\1")
```

* ```"(.)(.)\\2\\1"```

This will match a pair of characters followed by the reverse of the pair. For example, "elle" in "belle".

```{r}
# str_view("belle", "(.)(.)\\2\\1")
str_extract("belle", "(.)(.)\\2\\1")
```

* ```(..)\1```

This will match any two characters repeated twice. For example, "caca" in "cacao beans".

```{r}
# str_view("cacao beans", "(..)\\1")
str_extract("cacao beans", "(..)\\1")
```

* ```"(.).\\1.\\1"```

This matches any character repeated every other character, such as "lulul" in "lululemon".

```{r}
# str_view("lululemon", "(.).\\1.\\1")
str_extract("lululemon", "(.).\\1.\\1")
```

* ```"(.)(.)(.).*\\3\\2\\1"```

This matches 3 characters followed by any number of characters followed by the first 3 characters in reverse, such as "but the tub" in "but the tub is full".

```{r}
# str_view("but the tub is full", "(.)(.)(.).*\\3\\2\\1")
str_extract("but the tub is full", "(.)(.)(.).*\\3\\2\\1")
```

### 2. Construct regular expressions to match words that:

* Start and end with the same character.

```"^(.).*\\1$"``` performs this function. The ^ anchor specifies that any character (.) must also be present at the end of the string using the ```\$``` anchor after the backreference. Below, it will match "regular" but not "expression".

```{r}
# str_view(c("regular","expressions", "a", "aa"), "^(.).*\\1$")
str_subset(c("regular","expressions", "a", "aa"), "^(.).*\\1$")
```

* Contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice.)

```"(..).*\\1"``` will work by specifying that any two characters (..) can be separated by any amount of characters ```.*``` followed by the same two characters using a backreference.

```{r}
# str_view(c("church", "no repeats", "papaya"), "(..).*\\1")
str_subset(c("church", "no repeats", "papaya"), "(..).*\\1")
```

* Contain one letter repeated in at least three places (e.g. “eleven” contains three “e”s.)

```"(.).*\\1.*\\1"``` would work for single word strings. If there is a sentence with multiple words, this regex would match the first occurance of 3 characters, including whitespace, even if they were in different words. If we don't want this to happen, we would use a "anything except whitespace" instead of ```.*```. This would be a regexp for that purpose: ```"([^\\s])[^\\s]*\\1[^\\s]*\\1"```. We could also use the boundary regexp ```\\b```.

```{r}
# this only works for individual words
# str_view(c("eleven", "apply", "papaya", "bananas", "will this match spaces?","letters next"), "(.).*\\1.*\\1")
str_subset(c("eleven", "apply", "papaya", "bananas", "will this match spaces?","letters next"), "(.).*\\1.*\\1")

# this will exclude whitespace and only match if a word within a sentence has a letter repeated 3 times.
# str_view(c("eleven", "apply", "papaya", "bananas", "will this match spaces?", "letters next"), "([^\\s])[^\\s]*\\1[^\\s]*\\1")
str_subset(c("eleven", "apply", "papaya", "bananas", "will this match spaces?", "letters next"), "([^\\s])[^\\s]*\\1[^\\s]*\\1")
```


## 14.4.2 Exercises

### 1. For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.

* Find all words that start or end with x.

```{r}
# single expression
str_subset(words, "^x|x$")
# multiple str_detect() calls
c(words[str_detect(words, "^x")], words[str_detect(words, "x$")])
```

* Find all words that start with a vowel and end with a consonant.

```{r}
# single expression
str_subset(words, "^[aeiou].*[^aeiou]$")
# multiple str_detect() calls
words [ str_detect(words, "^[aeiou]") & str_detect(words, "[^aeiou]$")]
```

* Are there any words that contain at least one of each different vowel?

To determine whether there are any words that contain at least one of each vowel, we can look for words that have just one of each vowel to make things simpler (any words with more than one of any vowell will still be matched). This is much easier done using multiple str_detect() calls, beacuse we do not care where the vowels show up within the word. "a" can be before "i", or after "i", or between "o" and "u", which makes things complicated if we want to use a single regular expression. We would have to type out all the permutations and chain them using ```.*``` and OR (|). This would be a huge expression! Below is the simpler way of chaining multiple str_detect() calls. No words have at least one of each vowell, but some have a,e i, and o (no u).

```{r}
# multiple str_detect() calls
words[str_detect(words, "a") & str_detect(words, "e") &  str_detect(words, "i") & str_detect(words, "o") & str_detect (words, "u")]
# words with a, e, i, and o (no u)
words[str_detect(words, "a") & str_detect(words, "e") &  str_detect(words, "i") & str_detect(words, "o") ]
```

### 2. What word has the highest number of vowels? What word has the highest proportion of vowels? (Hint: what is the denominator?)

To figure this out, we need a way to count the number of vowels in each word in ```words```, which we can do with str_count() and the regexp ```[aeiou]```. This will let us know the number of vowels in each word, but we also want to keep the information about what the original word was. This means we should perform str_count() on a data frame that contains the original words as well as their index. The book provides this as a tibble. We can perform a mutate() using str_count() and also get the length of each word using str_length(), then calculate the proportion by dividing the two metrics. Then we can use arrange() to find the words with the highest vowel count and vowel proportion. The highest vowel count is 5, which includes appropriate, associate, available, colleague, encourage, experience, individual, and television. The highest vowel proportion is the word "a", which has a proportion of 1 (it itself is a vowel). Non-1 letter words with the highest vowel proportion include area and idea, at 0.75.

```{r}
# create indexed words tibble
df <- tibble(
  word = words, 
  i = seq_along(word)
)
# calculate number of vowels and proportion of vowels by word
vowels <- df %>%
  mutate ( num_vowels = str_count(words, "[aeiou]"),
           length_word = str_length(words),
           proportion_vowel = num_vowels/length_word)
# sort by words with highest number or proportion of vowels
vowels %>%
  arrange (desc(num_vowels))
vowels %>%
  arrange (desc(proportion_vowel))

```


## 14.4.3.1 Exercises

### 1. In the previous example, you might have noticed that the regular expression matched “flickered”, which is not a colour. Modify the regex to fix the problem.

We can modify the regex to detect only legitimate colors by adding a whitespace character on both sides of the name of each color. This will match sentences where a colour is only one word. However, dual-colors like "orangered" or "orange-red" will not be matched unless explicitly stated in the colours vector. To do this, we could change the whitespace to a boundary regex ```"\\b"```.

```{r}
colours <- c("red", "orange", "yellow", "green", "blue", "purple")
colours_fixed <- str_c("\\s", colours, "\\s")
colours_fixed
colour_match <- str_c(colours_fixed, collapse = "|")
has_colour <- str_subset(sentences, colour_match)
has_colour

more <- sentences[str_count(sentences, colour_match) > 1]
# str_view_all(more, colour_match)
str_extract_all (more, colour_match)
```

### 2. From the Harvard sentences data, extract:

* The first word from each sentence.

To do this, use the ^ anchor and look for all characters that lie before the first whitespace.

```{r}
first_words <- str_extract(sentences, "^.+?\\s")
head(first_words,20)
```

* All words ending in ing.

To do this, first find the subset of sentences that have "ing" before a boundary character. Then, within these sentences, extract the word containing "ing" before the boundary character.

```{r}
with_ing <- str_subset(sentences, "ing\\b")
head(with_ing)
words_ing <- str_extract(with_ing, "\\b\\w+ing\\b")
words_ing

```

* All plurals.

Below is a way to find all words that end in "s", which should capture plurals that end in "s", as well as other non-plural words that also end in "s", such as "press". To decipher which of these are plurals of singular words, we could subtract the "s" from the words and see if the resulting word matches a dictionary of specified singular words, which we don't have here. To decipher which words are plurals using only a regex doesn't seem straightforward and would require something very convoluted.

```{r}
with_plural <- str_subset(sentences, "\\b\\w+s\\s")
head(with_plural)
words_plural <- str_extract(with_plural, "\\b\\w+s\\s")
head(words_plural)
```

## 14.4.4.1 Exercises

### 1. Find all words that come after a “number” like “one”, “two”, “three” etc. Pull out both the number and the word.

I modeled the answers after the example provided in the book, and modified the code to work with the new regex. The regex I made has numbers one - ten, then followed by a suffix "teen" for cases such as "fourteen" or "ty" for cases such as "sixty". There are some other cases that I added in that do not follow this convention such as "twenty" or "thirteen". This should be followed by a whitespace then 1 or more non-whitespace character. We can then extract the results using a combination of str_subset() and str_extract(). Str_match() or tidyr::extract() can pull out both the number and the words that are encased in the parentheses.

```{r}
# regex to detect numbers with words after them
afternumber <- "\\b((one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fifteen|eighteen|twenty|thirty|fifty|eighty)(ty|teen)?) ([^ ]+)"

# find all sentences that have a match with the regex
has_number <- sentences %>%
  str_subset(afternumber) %>%
  head(10)

# display what the match for each sentence was
has_number %>% 
  str_extract(afternumber)
# display the separate parts of each match.
has_number %>% 
  str_match(afternumber)
# use tidyr::extract() to extract matches for each part of the regex and display output in a neat table
tibble(sentence = sentences) %>% 
  tidyr::extract(
    sentence, c("complete_number", "number","teen_or_ty", "word"), afternumber, 
    remove = FALSE
  ) %>%
  select(sentence, complete_number, word) %>%
  na.omit()
```


### 2. Find all contractions. Separate out the pieces before and after the apostrophe.

To find contractions, we can search for a sequence of letters using ```\w```, followed by a contraction with either "' or -", and then another sequence of words after that, followed by a boundary. Using this regex we can then use str_subset(), str_extract(), str_match(), and tidyr::extract() to obtain the pieces before and after the apostrophe. If we don't care about hyphens, we can remove it from the regex.

```{r}
contractions <- "\\b(\\w+)('|-)(\\w+)\\b"
has_contraction <- sentences %>%
  str_subset(contractions) %>%
  head(10)
has_contraction %>% 
  str_extract(contractions)

has_contraction %>% 
  str_match(contractions)

tibble(sentence = sentences) %>% 
  tidyr::extract(
    sentence, c("before_contraction", "contraction","after_contraction"), contractions, 
    remove = FALSE
  ) %>%
  mutate (
    complete = str_extract (sentence, contractions)
  )%>%
  na.omit()


```


## 14.4.5.1 Exercises

### 1. Replace all forward slashes in a string with backslashes.

```{r}
sample <- "this/has/too/many/slashes/!"
sample
str_replace_all (sample, "/", "\\\\")
```

### 2. Implement a simple version of str_to_lower() using replace_all().

It looks like stringr doesn't have a "to lowercase" regex such as ```\\L``` in perl, which can convert a backreference to lower case (```\\U``` will convert it to uppercase). This makes us write bulky and inefficient code, having to specify "A = a", "B = b", etc. Instead of using str_replace_all(), we can use the baseR gsub(), and group all caps ([A-Z]) in parentheses, and then use ```"\\L\\1"``` to turn every instance of a capital to lowercase.

```{r}
sample <- "The CaPS iS gOiNg CrAZy!"

# gsub does this more efficiently!
gsub ("([A-Z])", "\\L\\1", perl=TRUE, sample)

# this doesn't work in stringr!!
str_replace_all ( sample, "([A-Z])", "\\L\\1")

# if we must use stringr, we have to painfully write out all the replacements...
str_replace_all (sample, c("A" ="a", "C" = "c", "T" = "t", "P" = "p", "S" = "s", "O" = "o", "N" = "n", "Z" = "z"))

```

### 3. Switch the first and last letters in words. Which of those strings are still words?

To swich the first and last letters, we can use paretheses along with anchor ^ and ```$```, and then swap the backreferences. Then, we can use the function intersect() to determine which of these swapped words are still present in the original words dataset.

```{r}
# swap the first and last letters in the words dataset
swapped <- str_replace (words, "^(.)(.*)(.)$", "\\3\\2\\1")
head(swapped, 10)
# find out which words in the swapped list still match words in the original words dataset
intersect(words, swapped)
```


## 14.4.6.1 Exercises

### 1. Split up a string like "apples, pears, and bananas" into individual components.

```{r}
sample <- "apples, pears, and bananas"
# use ", " to split the string
str_split(sample, " ")[[1]]
# use boundary() to split the string
str_split(sample, boundary("word"))[[1]]
```

### 2. Why is it better to split up by boundary("word") than " "?

It is better to split up by boundary("word") than by " " because splitting by " " will cause some words to include puncutation marks, such as commmas or periods. boundary("word") takes these into consideration when splitting, and will not include them in the output. See the comparison provided in my answer to the previous exercise, which uses both ways to split the string.

### 3. What does splitting with an empty string ("") do? Experiment, and then read the documentation.

Splitting with an empty string ("") will split the string into each individual character. The empty string can be thought of as the "space" between each individual character in the string. The documentation states that "an empty pattern, "", is equivalent to boundary("character")". Below I compare the two methods, which indeed provide the same output.

```{r}
sample <- "apples, pears, and bananas"
# use ", " to split the string
str_split(sample, "")[[1]]
str_split(sample, boundary("character"))[[1]]
```


## 14.5.1 Exercises

### 1. How would you find all strings containing \ with regex() vs. with fixed()?

When using regex(), we must use four backslashes for the regex to detect literal backslashes in a string. When using fixed(), we can get away with only using two backslashes to detect literal backslashes in a string. Below is an example of using both regex() and fixed() in str_subset.

```{r}
sample <- c("this has a \\ backslash", "this doesn't have one", "this has a \\ backslash" )
sample
str_subset(sample, regex("\\\\"))
str_subset(sample, fixed("\\"))
```

### 2. What are the five most common words in sentences?

The top five most common words are "the"/"The", "of", "a", "to", and "and". To answer this question, we first use str_extract_all() with boundary("word"), to get all the words in a character matrix. Then, we convert the matrix into a vector, then in to a tibble so we can perform dplyr commands. Then, we can use group_by() to group by individual words, then count() to determine how many times each word was used, then arrange() to find the top words.

```{r}
# extract all the words in sentences
word_matrix <- str_extract_all(sentences, boundary("word"), simplify = T)
word_vector <- as.vector(word_matrix)
head(word_vector)

# use group_by, count(), and arrange() to find the top 5 words
tibble(word_vector) %>%
  group_by(word_vector) %>%
  count() %>%
  arrange(desc(n))
```


## 14.7.1 Exercises

### 1. Find the stringi functions that:

* Count the number of words.
* Find duplicated strings.
* Generate random text.

To find these functions, first load the stringi package and type "stri_". If using rstudio, you can then see all the various functions that show up and read a short description if you hover over their names. Below are the functions that perform the actions specified.

```{r}
library(stringi)
# count number of words
stri_count_words(sentences) %>% head(10)

# find duplicated strings
sample <- c("hello", "hello", "world", "this", "hello")
stri_duplicated(sample)

# generate random text
stri_rand_strings(5, 10)
```

### 2. How do you control the language that stri_sort() uses for sorting?

To determine the language that stri_sort() uses for sorting, you must specify the locale argument. For stri_sort(), you input a character vector, and then specify the decreasing argument to get a sorted version of the character vector. Decreasing = T will sort the vector such that the strings that start with a letter closer to the end of the alphabet are ranked earlier.

```{r}
# sort the sentences vector, locale = en_US
stri_sort(sentences, decreasing = TRUE, locale = "en_US") %>% head(10)

```


# Chapter 15: Factors

```{r}
library(forcats)
library(tidyverse)
```

## 15.3.1 Exercises

### 1. Explore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?

We can explore the distribution either by looking at summary(gss_cat$rincome) or by plotting the data using geom_bar(). rincome is a column of factors divided in to several categories. From the summary we can see that most people who reported their income lie in the 25,000 or more category, but a lot of people did not answer the survey or were not applicable as well. The names of each category are fairly long, and in the default plot the labels are overlapping. To improve the default plot, we can tilt the axis labels so that they are readable using the theme() option in ggplot2.

```{r barplot_rincome}
summary(gss_cat$rincome)
ggplot(gss_cat, aes(rincome)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 2. What is the most common relig in this survey? What’s the most common partyid?

Based on the bar plots, the most common relig in this survey is Protestant. The most common partyid is Independent.

```{r barplot_relig_partyid}
ggplot(gss_cat, aes(relig)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(gss_cat, aes(partyid)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 3. Which relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualisation?

You can find out with a table by using dplyr commands, first grouping by denom, then finding the proportion of each religion within a denom. To find out with a visualization, you can use an aesthetic mapping in ggplot2 to fill in a barplot with colors based on the relig. To make the proportion easier to see, you can specify position = "fill".

```{r denom_relig_visualization}

gss_cat %>%
  group_by (denom, relig) %>%
  count()

ggplot(gss_cat, aes(denom)) +
  geom_bar(aes(fill = relig)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(gss_cat, aes(denom)) +
  geom_bar(aes(fill = relig), position = "fill") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



## 15.4.1 Exercises

### 1. There are some suspiciously high numbers in tvhours. Is the mean a good summary?

If there are very high outliers in any distribution, the mean will be inflated. Since the mean is the average of the numbers, any extremely high numbers will increase the mean. Therefore, the mean is not a good summary. In this instance, the median may give a better measure of where the data is centered. It is always a good idea to be aware of what types of outliers exist in your data. Plotting a histogram of ```tvhours``` below, we can see that in some cases there are over 20 hours of tv! The distribution is skewed to the right.

```{r tvhours_distribution}
ggplot(gss_cat, aes (tvhours)) +
  geom_histogram()
```


### 2. For each factor in gss_cat identify whether the order of the levels is arbitrary or principled.

We can determine this by examining the levels of each one of the factor columns in gss_cat using levels(), and then determining whether there is a principle to the ordering listed. Here is my assessment: marital - arbitrary, race - arbitrary, rincome - principled (based on decreasing income levels), partyid - principled (going from strong republican, slowly towards strong democrat), relig - arbitrary, denom - arbitrary.

```{r}
levels(gss_cat$marital)
levels(gss_cat$race)
levels(gss_cat$rincome)
levels(gss_cat$partyid)
levels(gss_cat$relig)
levels(gss_cat$denom)
```


### 3. Why did moving “Not applicable” to the front of the levels move it to the bottom of the plot?

In the text, the "after" argument was not specified, so the default value of ```after = OL``` was used, which puts "Not applicable" to the front. This results in it being placed before "No answer". The way that geom_point() plots the categories is such that the first level is plotted at the bottom. This is why "Not applicable" appears at the bottom of the plot. 

```{r}
levels(gss_cat$rincome)
levels(fct_relevel(gss_cat$rincome, "Not applicable"))
```


## 15.5.1 Exercises

### 1. How have the proportions of people identifying as Democrat, Republican, and Independent changed over time?

To answer this, first lump together all the Democrat-associated categories into one category named "All Democrats", and likewise for Republican and Independent. Then, we can plot the change in the number of people that associate with these categories over time.

```{r proportion_by_partyid}
gss_cat %>%
  mutate(partyid = fct_recode(partyid,
    "Republican"    = "Strong republican",
    "Republican"      = "Not str republican",
    "Independent" = "Ind,near rep",
    "Independent" = "Ind,near dem",
    "Democrat"        = "Not str democrat",
    "Democrat"      = "Strong democrat",
    "Other"                 = "No answer",
    "Other"                 = "Don't know",
    "Other"                 = "Other party"
  )) %>%
  ggplot(aes(x = year))+
  geom_bar(aes(fill = partyid))
```


### 2. How could you collapse rincome into a small set of categories?

We could make the income-blocks larger, by combining the income categories into blocks of "less than 5000", "5000 to 10000", "10000 to 25000", and "25000 or more". We could also lump together "Refused", "dont know", "no answer", and "not applicable" into an "other" category, although this may be dangerous. Some questions we might ask before lumping these groups together are: do people who refused to take the survey have behavioral differences that may matter in some contexts? Why did people not answer the survey--did they not have had the means to do so? Do these categories preferentially lie in specific districts?

```{r}
gss_cat %>%
  mutate(rincome = fct_recode(rincome,
    "less than 5000"      = "Lt $1000",
    "less than 5000" = "$1000 to 2999",
    "less than 5000" = "$3000 to 3999",
    "less than 5000" = "$4000 to 4999",
    "5000 to 10000"  = "$5000 to 5999",
    "5000 to 10000"  = "$6000 to 6999",
    "5000 to 10000"  = "$7000 to 7999",
    "5000 to 10000"  = "$8000 to 9999",
    "10000 to 25000" = "$10000 - 14999",
    "10000 to 25000" = "$15000 - 19999",
    "10000 to 25000" = "$20000 - 24999",
    "Other" = "No answer",
    "Other" = "Don't know",
    "Other" = "Refused",
    "Other" = "Not applicable"
  )) %>%
  count(rincome)
```


# Chapter 16: Dates and times

```{r}
library(lubridate)
library(nycflights13)
```

## 16.2.4 Exercises

### 1. What happens if you parse a string that contains invalid dates?

You get an error message telling you the number of strings that failed to parse. In this instance, the error message is "1 failed to parse." A vector is returned containing the strings of dates that parsed correctly with the invalid dates replaced with NA.

```{r}
ymd(c("2010-10-10", "bananas"))
```


### 2. What does the tzone argument to today() do? Why is it important?

The documentation states that the ```tzone``` argument in today() accepts a character vector specifying the time zone you would like to find the curent date of. It will default to the system time zone on your computer if left unspecified. This is important because, depending on the time zone, the date could be different. For example, the US west coast is 3 hours behind the US east coast. If it is 11PM on the west coast, the east coast is one day ahead. Below is the example provided in the documentation.

```{r}
today()
today("GMT")
today() == today("GMT") # not always true
```


### 3. Use the appropriate lubridate function to parse each of the following dates:

Based on the format of the string, the order of the month, year, and day parameters will determine which appropriate lubridate function should be used. For example, the first string, d1, is in month-day-year format, so we should use the lubridate function ```mdy```. I apply the same principle to the other cases.

```{r}
d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014

mdy(d1)
ymd(d2)
dmy(d3)
mdy(d4)
mdy(d5)
```


## 16.3.4 Exercises

```{r}
make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}

flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) %>% 
  select(origin, dest, ends_with("delay"), ends_with("time"))

flights_dt
```


### 1. How does the distribution of flight times within a day change over the course of the year?

Within a day, we want to observe how the flight times differ. This means we should look at how flight times differ by the hour (ie how many flights are taking off at every hour of the day). Now, we want to see how this behavior changes over the course of the year (ie how does this graph look like when plotted monthly)? We observe that the distribution of flights within a day does not significantly change over the course of the year. The same trend is followed in which there is a peak of flights around 8am, a dip in flights from 10am-12pm, and then a slow drop off in number of flights past 7pm.

```{r dist_flight_within_day}
# flights per hour for the entire year
flights_dt %>% 
  mutate(hour = hour(dep_time)) %>%
  group_by(hour)%>%
  summarize(numflights_per_hour = n())%>%
  ggplot(aes(x = hour, y = numflights_per_hour)) +
    geom_line()
# split the above graph into months

flights_dt %>% 
  mutate(hour = hour(dep_time),
         month = as.factor(month(dep_time))) %>%
  group_by(month,hour)%>%
  summarize(numflights_per_hour = n())%>%
  ggplot(aes(x = hour, y = numflights_per_hour)) +
    geom_line(aes(color = month))

```


### 2. Compare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings.

First calculate our own version of dep_time by adding sched_dep_time and dep_delay together, then compare the result to the provided dep_time in the table. When filtering for values in dep_time that do not match the calculated version, we find 1205 discrepancies out of the 328,063 observations. For the most part, they are consistent, but we should find out why the 1205 inconsistencies exist.

```{r}

flights_dt %>%
  mutate ( calculated_dep_time = sched_dep_time + dep_delay*60) %>%
  select (calculated_dep_time, sched_dep_time, dep_delay, dep_time) %>%
  filter ( calculated_dep_time != dep_time) %>%
  count()

```


### 3. Compare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.)

Theoretically, the duration should match the difference between the arrival time and hte departure time, after accounting for time zone differences between airport locations. If the time zone difference is not accounted for, the air_time will not match the simple difference. This seems to be the case, because 327,150 of the 328,063 observations in the dataset have a reported air_time that does not match the difference between the arrival and departure times.

```{r}
flights_dt %>%
  mutate ( calculated_air_time = arr_time - dep_time) %>%
  select (calculated_air_time, air_time, arr_time, dep_time) %>%
  filter (calculated_air_time != air_time)

```


### 4. How does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why?

Use the hour() function to group observations based on hour, then group by this parameter. Use this to calculate the average dep_delay per hour over the year. Plot using geom_line(). This can also be achieved using date-time components function update(), although this would change the number of points connected on the graph. I've plotted both results using either dep_time or sched_dep_time. You should use sched_dep_time instead of dep_time since this will tell you which scheduled flights might have a higher chance of being delayed. We observe that flights scheduled later on during the day have higher chances of being delayed, with a peak around hour 20 (8pm). Organizing by dep_time will let you know what time of the day most of the flights are delayed, which will intuitively occur later than the scheduled time as the flights start backing up. We observe that this is indeed the case, in which the peak of the late flights for dep_time occurs after the peak for the sched_dep_time plot, in which the flights are now delayed past midnight.

```{r avg_delay_over_day}

flights_dt %>% 
  mutate ( dep_hour = hour(sched_dep_time) )%>%
  group_by(dep_hour) %>%
  summarize(avg_delay_hour = mean(dep_delay, na.rm = T)) %>% 
  ggplot(aes(dep_hour, avg_delay_hour)) +
    geom_line()
flights_dt %>% 
  mutate ( dep_hour = hour(dep_time) )%>%
  group_by(dep_hour) %>%
  summarize(avg_delay_hour = mean(dep_delay, na.rm = T)) %>% 
  ggplot(aes(dep_hour, avg_delay_hour)) +
    geom_line()

```


### 5. On what day of the week should you leave if you want to minimise the chance of a delay?

To find days of the week that have the lowest average delay, first assign a day to each observation using wday(). Then group by the day of the week, and use summarize() to find the average delay time on for each day of the week. We see that Saturday has the lowest average delay at 7.61, and on average the flights even arrive earlier than expected!

```{r}
flights_dt %>% 
  mutate(wday = wday(sched_dep_time, label = TRUE)) %>% 
  group_by(wday) %>%
  summarize ( avg_dep_delay_week = mean(dep_delay, na.rm = TRUE),
              avg_arr_delay_week = mean(arr_delay, na.rm = TRUE))
```

### 6. What makes the distribution of diamonds$carat and flights$sched_dep_time similar?

Let's first examine the distribution of each of these datasets, using histograms. Using just the default value for binwidth, there is no apparent similarity between the distribution of values between the two datasets upon initial observation. The carat values are skewed to the right, but the sched_dep_time is not. I suppose one could argue that there are more flights with an earlier sched_dep_time, and likewise there are more diamonds with a low carat value. However, if we bin the values using smaller binwidth, we observe "spikes" of values in both datasets. This "spike" phenomenon occurs at carat 1, 1.5, 2, etc. and around flight times near the hour. This is likely an example of human "bias" for flights leaving at "nice" departure times, as mentioned by Hadley.

```{r dist_carat_vs_sched_dep_time}
ggplot (diamonds, aes(x = carat)) +
  geom_histogram()
ggplot (flights, aes(x = sched_dep_time)) +
  geom_histogram()

ggplot (diamonds, aes(x = carat)) +
  geom_freqpoly(binwidth = 0.1)
ggplot (flights, aes(x = sched_dep_time)) +
  geom_freqpoly(binwidth = 10)

```


### 7. Confirm my hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed.

To test this hypothesis, we need to see whether the total number of flights that departed early was increased during minutes 20-30 and 50-60. If this is true, then his hypothesis is supported. If this is not true, then maybe other factors are contributing more significantly to the lower flight delays during these time slots. To figure this out, we first use mutate() and ifelse() to assign whether or not a flight left early using TRUE or FALSE. Then, grouping by minute, we can count the number of flights that left early using sum(), and plot this value using ggplot(). We observe that there are indeed more flights leaving early during the 20-30 and 50-60 time slots (the graph looks like and inverted plot of the avg_delay by minute graph).

```{r early_flights_by_min}
flights_dt %>% 
  mutate(minute = minute(dep_time),
         early = ifelse(dep_delay>=0, FALSE, TRUE)) %>% 
  group_by(minute) %>% 
  summarise(
    avg_delay = mean(arr_delay, na.rm = TRUE),
    num_flights_early = sum(early),
    n = n()) %>% 
  ggplot(aes(minute, num_flights_early)) +
    geom_line()
```



## 16.4.5 Exercises

### 1. Why is there months() but no dmonths()?

Durations must be standardized lengths of time. There is no dmonths() since months do not have a standard number of days. For example, February has a shorter number of days than January.

### 2. Explain days(overnight * 1) to someone who has just started learning R. How does it work?

The days() function converts the input into a datetime, for example, days(5) returns "5d 0H 0M 0S". In this case, the input is ```overnight * 1```. The variable ```overnight``` corresponds to the output of ```arr_time < dep_time```, which is evaluated as a boolean (TRUE or FALSE). The multiplication with 1 will cause TRUE or FALSE to be converted to 1 or 0. Thus, days(overnight *1) will give you either 0 or 1 days in datetime form, which can then be added to arr_time.


```{r}
# proof of concepts
days(5)
TRUE*1
FALSE*1
days(1) + days (TRUE *1)

# Example from text using days (overnight * 1)
flights_dt <- flights_dt %>% 
  mutate(
    overnight = arr_time < dep_time,
    arr_time = arr_time + days(overnight * 1),
    sched_arr_time = sched_arr_time + days(overnight * 1)
  )

```

### 3. Create a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year.

To do this, we can use ymd() to create a date for January 1, 2015. Then we can add this to a vector of months in order to generate a vector of 1st days for every month in that year. To do the first day of everymonth for the current year, we should write code that will work no matter what year you run it. To do this, we can use the today() function to get todays date, then extract the year from this date. This can be done two ways. You can either use floor_date(), which can round the date to the nearest year if specified. Or, since the date object can be manipulated like a string, you can use substr() to extract the first 4 characters (the year), then use str_c() to add "-01-01" which will create "january 1st"" for the current year. Then we can add the vector of months as we did previously to get the vector of dates giving the first day of every month in the current year.

```{r}
first_days_2015 <- ymd('2015-01-01') + months(seq(0,11,1)) # months(0:11) also works
first_days_2015

# do it using strings, extracting the year from today()
jan01_current <- str_c(substr(today(),1,4), "-01-01")
ymd(jan01_current) + months(0:11)

# do it using floor_date() to round today() to the current year
floor_date(today(), "year") + months(0:11)

```



### 4. Write a function that given your birthday (as a date), returns how old you are in years.

### 5. Why can’t (today() %--% (today() + years(1)) / months(1) work?




---

Thanks for reading! I hope you found my solutions to the exercises informative. A walkthrough of chapters 17 - 21 can be found at [r4ds_p6_chapters17-21_walkthrough.md](https://github.com/erilu/R-for-data-science-walkthrough/blob/master/r4ds_p6_chapters17-21_walkthrough.md).

