[
["index.html", "R for Data Science Walkthrough Introduction About me", " R for Data Science Walkthrough Erick Lu 2020-01-21 Introduction The free online version of the book R for Data Science by Hadley Wickham and Garrett Grolemund can be found at http://r4ds.had.co.nz/index.html. This is the version that I will be working with. Here, I work through each chapter of the book and provide my take on the solutions to their exercises, and also mix in some of my own notes and explorations. I highly recommend walking through their book if you are trying to learn how to use the tidyverse for data wrangling and analysis. You can navigate to a specific exercise using the table of contents on the left hand side of this webpage. My solutions may not be perfect, but hopefully they provide some insight into how each exercise can be answered. Although there are no formal solutions for this book, a more comprehensive and well-polished guide by Jeffrey Arnold can be found at https://jrnold.github.io/r4ds-exercise-solutions/. About me I used this book back when I was a PhD student in the Biomedical Sciences program at UCSF. For one of my research projects, I needed to perform RNA-sequencing experiments in order to characterize the gene expression changes that occured after immune cell activation. Typically, this type of differential gene expression analysis is performed using a suite of bioinformatics R packages available on Bioconductor (DESeq2, etc.) and requires a lot of data wrangling and processing. This book taught me useful techniques in R to help process the RNA-seq data, perform exploratory data analysis, create visualizations, and gain valuable insights. I still use many of the tools I learned from this book in my day-to-day life as a scientist, particularly when it comes to analysis of large datasets and bioinformatics. Back then, I initially walked through this book by answering each exercise in a set of R notebooks, which you can find in the directory _before_bookdown in the github repo for this book at https://github.com/erilu/R-for-data-science-walkthrough. I recently wanted to learn how to use bookdown, so I converted my set of notebooks into a bookdown website as a mini-project. I hope that whoever is reading this out there will find some utility in this. Thanks for stopping by! "],
["chapter-3-data-visualization.html", "Chapter 3 - Data visualization 3.2 Notes - Creating a ggplot 3.2.4 Exercises 3.3 Notes - Aesthetic Mappings 3.3.1 Exercises 3.5 Notes - Facets 3.5.1 Exercises 3.6 Notes - Geometric objects 3.6.1 Exercises 3.7 Notes - Statistical Transformations 3.7.1 Exercises 3.8 Notes - Position Adjustments 3.8.1 Exercises 3.9 Notes - Coordinate Systems 3.9.1 Exercises", " Chapter 3 - Data visualization library(tidyverse) 3.2 Notes - Creating a ggplot Here we learn how to use ggplot2. The dataset we will work with is the built-in dataset mpg. head(mpg) ## # A tibble: 6 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manua… f 21 29 p comp… ## 3 audi a4 2 2008 4 manua… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto(… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto(… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manua… f 18 26 p comp… Plot mileage (hwy) against engine displacement (displ): ggplot(data = mpg) + geom_point(mapping = aes (x=displ, y=hwy)) + ggtitle(&quot;Engine displacement (x-axis) vs Mileage (y-axis)&quot;) + theme(plot.title = element_text(hjust = 0.5)) I added a title to the ggplot (ggtitle) and centered the title by adding a theme parameter. I also found that it was not required to have the “data =” or “mapping =” in the ggplot() or geom_point() parameters. Since there was a inverse correlation, I was interested to see what a linear model would look like if fitted to the data. Here is how I added a trend line to the plot above. ggplot(data = mpg, aes (x = displ, y = hwy)) + geom_point() + ggtitle(&quot;Engine displacement (x-axis) vs Mileage (y-axis)&quot;) + theme(plot.title = element_text(hjust = 0.5)) + geom_smooth(method = &#39;lm&#39;, se = F) I had to specify the aes in the ggplot() parameter, so that it would apply to both the geom_point() and geom_smooth(). When I kept the aes in the geom_point() paramter as before, I received an error. 3.2.4 Exercises 1. Run ggplot(data = mpg). What do you see? ggplot(data = mpg) We see an empty plot. The ggplot() function simply creates a plotting space and specifies the data that will be plotted. Sequential parameters must be added (“+”) to the ggplot to see anything. 2. How many rows are in mpg? How many columns? dim(mpg) ## [1] 234 11 There are 234 rows and 11 columns in the data set. 3. What does the drv variable describe? Read the help for ?mpg to find out. ?mpg The drv column specifies whether the car is “f = front-wheel drive, r = rear wheel drive, 4 = 4wd”. 4. Make a scatterplot of hwy vs cyl. ggplot(data = mpg) + geom_point(mapping = aes (x=cyl, y=hwy)) + ggtitle(&quot;Number of Cylinders (x-axis) vs Mileage (y-axis)&quot;) + theme(plot.title = element_text(hjust = 0.5)) There is an inverse correlation with the number of cylinders and how much mileage the car gets on the highway. 5. What happens if you make a scatterplot of class vs drv? Why is the plot not useful? ggplot(data = mpg) + geom_point(mapping = aes (x=class, y=drv)) + ggtitle(&quot;Number of Class (x-axis) vs Type of Drive (y-axis)&quot;) + theme(plot.title = element_text(hjust = 0.5)) The data is not particularly useful since these are two categorical variables, and because the class of car does not usually dictate the type of drive. Furthermore, you do not know how many points fall under each of the dots seen at the crosshairs. This plot would suggest that, since there are many classes of cars with two or more types of drive. 3.3 Notes - Aesthetic Mappings Color-code the points in the scatterplot by another variable in the data set. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) The plot shows that SUVs have low highway mileage and high engine displacement and that compact cars have high mileage and low engine displacement, as expected. Another example, this time color coding based on the drv variable. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = drv)) We can see that the front wheel drive cars on average have low engine displacement and high highway mileage. Using size variable to further categorize in the graph (you can combine multiple parameters for the cateogrization! This is pretty cool). If you try to use a categorical/discrete variable for size, an error will be displayed. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = cyl, color = class, alpha = drv)) ## Warning: Using alpha for a discrete variable is not advised. 3.3.1 Exercises 1. What’s gone wrong with this code? Why are the points not blue? ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;blue&quot;)) The points are not blue because the “color =” parameter lies within aes(). This means the function will be looking for a column within the mpg dataset called “blue”, which does not exist. So to fix this, place the “color =” parameter outside aes(), but within geom_point(). ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = &quot;blue&quot;) 2. Which variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg? One way you can figure out which are categorical vs continuous is by using the summary() function. The continuous variables will have the quartiles specified, whereas the categorical variables will not. You might have to be wary about categorical variables in numerical form, in which you would have to read the documentation. Runing just mpg would show the type of varable under the column name (char vs int vs dbl, etc.) which would also let you know this information. summary(mpg) ## manufacturer model displ year ## Length:234 Length:234 Min. :1.600 Min. :1999 ## Class :character Class :character 1st Qu.:2.400 1st Qu.:1999 ## Mode :character Mode :character Median :3.300 Median :2004 ## Mean :3.472 Mean :2004 ## 3rd Qu.:4.600 3rd Qu.:2008 ## Max. :7.000 Max. :2008 ## cyl trans drv cty ## Min. :4.000 Length:234 Length:234 Min. : 9.00 ## 1st Qu.:4.000 Class :character Class :character 1st Qu.:14.00 ## Median :6.000 Mode :character Mode :character Median :17.00 ## Mean :5.889 Mean :16.86 ## 3rd Qu.:8.000 3rd Qu.:19.00 ## Max. :8.000 Max. :35.00 ## hwy fl class ## Min. :12.00 Length:234 Length:234 ## 1st Qu.:18.00 Class :character Class :character ## Median :24.00 Mode :character Mode :character ## Mean :23.44 ## 3rd Qu.:27.00 ## Max. :44.00 3. Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables? I mapped the continuous variable, “cty”, city miles per gallon, using color and size. By color, the points are now on a gradient. By size, the larger points have higher city miles per gallon. I couldn’t map the continuous variable to shape, since there are a set number of shapes available. This was also an issue when running it for the variable “class”, since there was one more class than there were number of shapes as well (the SUV category has no points as a result). # install gridExtra package to plot multiple graphs side by side, could also use cowplots package # install.packages(&quot;gridExtra&quot;) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine byColor &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = cty)) + ggtitle(&quot;City miles mapped by color&quot;) bySize &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = cty)) + ggtitle(&quot;City miles mapped by size&quot;) grid.arrange(byColor, bySize, ncol=2) byShape &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = class))+ ggtitle(&quot;Number of Class (x-axis) vs Type of Drive (y-axis)&quot;) byShape ## Warning: The shape palette can deal with a maximum of 6 discrete values ## because more than 6 becomes difficult to discriminate; you have 7. ## Consider specifying shapes manually if you must have them. ## Warning: Removed 62 rows containing missing values (geom_point). 4. What happens if you map the same variable to multiple aesthetics? The points will all lie on the same area of the spectrum for each aesthetic. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = cyl, color = cyl, alpha = cyl)) 5. What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point) The stroke will modify the width of the border for geom_points that have a border. Below I increase the size of the points after categorizing by the drv variable. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = drv, stroke = 3)) 6. What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? The aesthetic will be mapped to the output of the argument. Displ &lt; 5 will return TRUE for all points less than 5, and these points will be mapped to a separate color. Below is an example of displ &lt; 5 and cyl &lt; 5. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = displ &lt; 5)) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = cyl &lt; 5)) 3.5 Notes - Facets Split data up into subplots based on a discrete variable: 1 dimensional facet. This allows us to focus in on subsets of the data (say for example, you wanted to quickly compare midsize vars vs minivans) Can add aesthetic mappings as well ontop of this! ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = drv)) + facet_wrap(~ class, nrow = 2) Instead of adding the aesthetic mapping, we can also make a 2D facet. This lets us add yet another mapping on top. So useful! ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = trans)) + facet_grid(drv ~ cyl) 3.5.1 Exercises 1. What happens if you facet on a continuous variable? Let’s try faceting on city miles per gallon (cty): ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(drv ~ cty) It looks like ggplot2 will still spit out a graph, but the graph is not very interpretable. It also takes much more time to process than a discrete variable with fewer factors. 2. What do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot? The empty cells in facet_grid(drv~cyl) mean that there are no points that satisfy both of the conditions specified for drv and cyl. In the plot below, you can identify the same blank facet plots as the crosshairs that do not have points (for example, cars with 4 cylinders and rear wheel drive). ggplot(data = mpg) + geom_point(mapping = aes(x = drv, y = cyl)) 3. What plots does the following code make? What does . do? Based on the output, I assume that . means to perform a 1D facet plot using the variable supplied. Although having the . vs not having it doesn’t change the output when using the form (~ drv). Switching between . ~ drv and drv ~ . flips the orientation of the graphs. Worth to note that facet_map(~ drv, ncol = 3) provides the same output as facet_grid(. ~ drv). ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(drv ~ .) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(. ~ drv) 4. Take the first faceted plot in this section: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_wrap(~ class) What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset? Faceting will allow you to examine the overall distribution of one subset vs another. Pulling out the points and viewing the plot in isolation might make it easier to see trends in the data. Larger datasets with more variability between subsets (overlapping points) might want to use facets. However the computing power needed to facet the data might not scale well. 5. Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol argument? nrow and ncol in facet_wrap() determine how many rows and columns the output graphs will be organized into. Other options include as.table, or dir. facet_grid() does not have nrow and ncol because there are defined numbers of parameters for the two variables being compared. 6. When using facet_grid() you should usually put the variable with more unique levels in the columns. Why? Putting the variable with more unique levels in the columns will allow you to scan the facets faster. Also, monitors are widescreen. 3.6 Notes - Geometric objects Data can be visualized in different ways using different geom_ functions: # left ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) # right ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Geom functions can be combined! Also, the better coding practice is to declare global parameters in ggplot() and change as you want in the geom_() functions, so that you do not have to modify or copy/paste multiple times. You can also only choose to display a subset of the data using the filter() command. # declaring locally ggplot(data = mpg) + geom_point(mapping = aes (x = displ, y = hwy, color = drv)) + geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv, color = drv)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # declaring globally and locally ggplot(data = mpg, mapping = aes (x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth(mapping = aes(linetype = drv)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # using filter() to only display some of the data, dependent on city mileage value ggplot(data = mpg, mapping = aes (x = displ, y = hwy, color = drv)) + geom_point(data = filter(mpg, cty &lt; 20)) + geom_smooth(mapping = aes(linetype = drv)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 3.6.1 Exercises 1. What geom would you use to draw a line chart? A boxplot? A histogram? An area chart? Line chart would use geom_line(), a boxplot would use geom_boxplot(), a histogram would use geom_histogram(), and an area chart would use geom_area(). 2. Run this code in your head and predict what the output will look like. Then, run the code in R and check your predictions. I predict that hwy will be plotted against displ as a scatter plot, with the color of the dot depending on the drv variable. superimposed on these points will be a smoothened conditional mean line, also colored based on the drv variable, since these were declared globally. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 3. What does show.legend = FALSE do? What happens if you remove it? Why do you think I used it earlier in the chapter? show.legend = FALSE prevents the legend from being displayed. If you remove it, the legends will show up. I think that it was set to false just to save space! 4. What does the se argument to geom_smooth() do? Based on the ?geom_smooth documentation, the se argument tells the graph to either display or hide the confidence interval around the smooth function. This would depend on the type of smoothing performed (loess vs lm, etc.). 5. Will these two graphs look different? Why/why not? ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot() + geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; No, the graphs will not look different. One defines the parameters globally, whereas the other defines the same parameters locally in each geom_() function. 6. Recreate the R code necessary to generate the following graphs. ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point () + geom_smooth (se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point () + geom_smooth (aes(group = drv), se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point () + geom_smooth (se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point (aes(color = drv)) + geom_smooth (aes(linetype = drv), se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data = mpg, mapping = aes(x = displ, y = hwy, fill = drv)) + geom_point (size = 3, shape = 21, stroke = 3, color = &quot;white&quot;) 3.7 Notes - Statistical Transformations Make a barplot using geom_bar(): ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) We can also create a barplot if given a set of pre-defined values: demo &lt;- tribble( ~cut, ~freq, &quot;Fair&quot;, 1610, &quot;Good&quot;, 4906, &quot;Very Good&quot;, 12082, &quot;Premium&quot;, 13791, &quot;Ideal&quot;, 21551 ) ggplot(data = demo) + geom_bar(mapping = aes(x = cut, y = freq), stat = &quot;identity&quot;) Or plot the barplot as a proportion (kind of like a histogram would, except this uses discrete variables on the x axis): ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1)) This is how to provide a stat summary manually using stat_summary(). It might be better to visualize this using a boxplot. I’ll try making one here as well: # stat summary ggplot(data = diamonds) + stat_summary( mapping = aes(x = cut, y = depth), fun.ymin = min, fun.ymax = max, fun.y = median ) # boxplot ggplot(data = diamonds) + geom_boxplot( mapping = aes (x = cut, y = depth)) As you can see, the layout for the stat_summary and boxplot are identical. The type of information provided by the boxplot is also very similar, except that it also provides the 1st and 3rd quartile and individual points lying outside. I am sure we could have added this information to the stat_summary(). 3.7.1 Exercises 1. What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function? Looking at the ?stat_summary page, the default geom function associated with it is “pointrange.” Below is a replicate of the plot using this geom_function: ggplot(data = diamonds) + geom_pointrange(mapping = aes (x = cut, y = depth, ymin =depth, ymax =depth)) This plot looks similar, but its not exactly the same. We still need to find a way to convert the dots into a line and plot the median point. 2. What does geom_col() do? How is it different to geom_bar()? geom_col() creates a barplot but uses the values in the data. In other words, it is as if we used geom_bar() with stat = “identity”. 3. Most geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common? I would refer to this page on the tidyverse website to see all the pairs of stats and geoms: http://ggplot2.tidyverse.org/reference/. Most of the stats and corresponding geoms are paired and have the same suffix. 4. What variables does stat_smooth() compute? What parameters control its behaviour? stat_smooth() computes the moving average using a choice of methods. You can set the span for the smoothing to calculate from, number of points to evalate the smoother at, and other parameters. Below I use stat_smooth to replicate one of the previous graphs that used geom_smooth(). ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = drv)) + stat_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 5. In our proportion bar chart, we need to set group = 1. Why? In other words what is the problem with these two graphs? Without the group = 1, each of the proportions that are calculated for every category in cut will be equal to 1. This is because geom_bar is calculating the proportion of each category in cut within that same category (ie: what proportion of “Fair” is in “Fair”). By forcing the group to be 1, the proper proportions as part of the total number of observations will be displayed. Changing the group size to an arbitrary number doesnt seem to change the graph. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop..)) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = color, y = ..prop..)) For some reason adding “group = 1” to the second geom_bar plot (the one with the fill = color parameter) gets rid of the fill. Looking online there was no straightforward solution to this, except for using ..count.. and manually calculating the proportions (not using y = ..prop..). 3.8 Notes - Position Adjustments To apply a separate color to each bar, specify either “color” or “fill” within aes() with the same variable that was on the x axis. # border, specify color parameter ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, colour = cut)) #fill, specify fill parameter ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = cut)) If you color by a variable other than what was on the x axis, each bar will be split into colors: ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity)) Note that this is plotting the raw counts, and not the proportions. Trying to do this using y = ..prop.. and group=1 does not work. If you want to see the relative contribution in a different way (not stacked), set the position parameter to “identity”. This will overlap the bars and make them all start from 0. To visualize them, you must either make the bars transparent or have no fill. ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + geom_bar(alpha = 1/5, position = &quot;identity&quot;) ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + geom_bar(fill = NA, position = &quot;identity&quot;) Alternatively, you can have all the bars go to the same height so you can see what the differences in proportion are between the subgroups of each item on the x axis (position = fill). Or, you can have each of the subgroups plotted side by side within each bar (position = dodge). # position = fill ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;fill&quot;) # position = dodge ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;dodge&quot;) On scatterplots, overlapping points can be overlooked quite easily. One way to get around this is to jitter all the points, so that the number of overlapping points can be better visualized. The jitter parameter adds some normally distributed noise to each of the values in the dataset. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), position = &quot;jitter&quot;) 3.8.1 Exercises 1. What is the problem with this plot? How could you improve it? ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() A lot of the data points are overlapping, so we have no sense of how weighted each point is. A better version of the plot would be one that uses the jitter parameter: ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = &quot;jitter&quot;) 2. What parameters to geom_jitter() control the amount of jittering? Based on ?geom_jitter, the “width” and “height” parameters for geom_jitter will control how much noise is added to each point. 3. Compare and contrast geom_jitter() with geom_count(). # geom jitter ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter() # geom count ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_count() Geom_count does not “jitter” the points; instead, it increases the size of the point based off of how many points were in that specific x,y slot. 4. What’s the default position adjustment for geom_boxplot()? Create a visualisation of the mpg dataset that demonstrates it. ?geom_boxplot() indicates that the default position is “dodge,” which means that any further grouping by aesthetic of each category on the x-axis will have the “dodge” positioning. As shown above, “dodge” splits the category into the indicated subgroups and plots it side by side within the category. Here is a visualization of hte mpg dataset using geom_boxplot(), which shows the city miles per gallon for each class of car, further grouped by the type of drive. The type of drive is “dodged”: ggplot (data = mpg, mapping = aes (x = class, y = cty)) + geom_boxplot(aes (color = drv)) If i wanted to plot the boxplots on top of each other, I would use position = “identity”, and then make the graphs transparent by specifying an alpha value: ggplot (data = mpg, mapping = aes (x = class, y = cty)) + geom_boxplot(aes (color = drv), position = &quot;identity&quot;, alpha = 1) 3.9 Notes - Coordinate Systems Sometimes we want to swap the axes, for various reasons (one being that the x-labels are long and hard to fit on a small graph): # vertical boxplots ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() # horizontal boxplots ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() We also might want to change bar plots into pie charts: bar &lt;- ggplot(data = diamonds) + geom_bar( mapping = aes(x = cut, fill = cut), show.legend = FALSE, width = 1 ) + theme(aspect.ratio = 1) + labs(x = NULL, y = NULL) bar + coord_flip() bar + coord_polar() # 3.9.1 Exercises 1. Turn a stacked bar chart into a pie chart using coord_polar(). The previous chart is a coxcomb plot, not what you would usually expect to see when looking at pie charts. Below I’ve modified the code to produce a more conventional pie chart, starting from a stacked bar chart. # make a stacked bar chart with one bar bar &lt;- ggplot(data = diamonds) + geom_bar( mapping = aes(x = factor(1), fill = cut), # factor(1) can be &quot;&quot; or anything not specifying a variable in the dataset. width = 1 #, -&gt; if you want a transparent pie chart, uncomment these! # position = &#39;identity&#39;, # alpha = .2 ) + theme(aspect.ratio = 1) + labs(x = NULL, y = NULL) bar + coord_polar(theta = &#39;y&#39;) 2. What does labs() do? Read the documentation. Labs() allows you to specify custom labels for the ggplot graphs. 3. What’s the difference between coord_quickmap() and coord_map()? coord_map() projects a spherical map onto a 2D plane, but does not preserve straight lines. coord_quickmap() does the same thing as coord_map except that it uses a quick approximation that preserves straight lines (for the most part). 4. What does the plot below tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do? The plot below tells us that there is a positive correlation between city and highway mpg. In other words, cars with higher city mileage tend to also have higher highway mileage. Although not necessary, coord_fixed() will make the scale of each axis the same width (5 units on the y axis moves up by the same length as 5 units on the x axis). Geom_abline() adds a line of slope 1 that passes through the origin (0,0). As it is now, it is not very informative. So, I modified the parameters to turn it into a regression line by passing in the slope and intercept values from the base R “lm” function. I also plotted the ggplot geom_smooth() as a comparison, showing that they provide the same line. # provided example ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() + geom_abline() + coord_fixed() # modified the abline to plot a simple linear regression ontop of the points. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() + geom_abline(intercept = lm(hwy ~ cty, data = mpg)$coeff[1], slope = lm(hwy ~ cty, data = mpg)$coeff[2]) + coord_fixed() # used geom_smooth instead of geom_abline to get the same result ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() + geom_smooth (method = &#39;lm&#39;, se = F)+ coord_fixed() "],
["chapter-4-workflow-basics.html", "Chapter 4 - Workflow: basics 4.4 Practice", " Chapter 4 - Workflow: basics library(tidyverse) 4.4 Practice 1. Why does this code not work? my_variable &lt;- 10 my_varıable The code does not work because there is a typo in the variable name that you are calling. The letter “i” is not the same in my_var(i)able. 2. Tweak each of the following R commands so that they run correctly: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) fliter(mpg, cyl = 8) filter(diamond, carat &gt; 3) To fix the commands, change “dota” to “data”, “fliter” to “filter”, “=” to “==”, and “diamond” to “diamonds”. library(tidyverse) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) filter(mpg, cyl == 8) ## # A tibble: 70 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a6 q… 4.2 2008 8 auto… 4 16 23 p mids… ## 2 chevrolet c150… 5.3 2008 8 auto… r 14 20 r suv ## 3 chevrolet c150… 5.3 2008 8 auto… r 11 15 e suv ## 4 chevrolet c150… 5.3 2008 8 auto… r 14 20 r suv ## 5 chevrolet c150… 5.7 1999 8 auto… r 13 17 r suv ## 6 chevrolet c150… 6 2008 8 auto… r 12 17 r suv ## 7 chevrolet corv… 5.7 1999 8 manu… r 16 26 p 2sea… ## 8 chevrolet corv… 5.7 1999 8 auto… r 15 23 p 2sea… ## 9 chevrolet corv… 6.2 2008 8 manu… r 16 26 p 2sea… ## 10 chevrolet corv… 6.2 2008 8 auto… r 15 25 p 2sea… ## # … with 60 more rows filter(diamonds, carat &gt; 3) ## # A tibble: 32 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.01 Premium I I1 62.7 58 8040 9.1 8.97 5.67 ## 2 3.11 Fair J I1 65.9 57 9823 9.15 9.02 5.98 ## 3 3.01 Premium F I1 62.2 56 9925 9.24 9.13 5.73 ## 4 3.05 Premium E I1 60.9 58 10453 9.26 9.25 5.66 ## 5 3.02 Fair I I1 65.2 56 10577 9.11 9.02 5.91 ## 6 3.01 Fair H I1 56.1 62 10761 9.54 9.38 5.31 ## 7 3.65 Fair H I1 67.1 53 11668 9.53 9.48 6.38 ## 8 3.24 Premium H I1 62.1 58 12300 9.44 9.4 5.85 ## 9 3.22 Ideal I I1 62.6 55 12545 9.49 9.42 5.92 ## 10 3.5 Ideal H I1 62.8 57 12587 9.65 9.59 6.03 ## # … with 22 more rows 3. Press Alt + Shift + K. What happens? How can you get to the same place using the menus? This opens up a list of the keyboard shortcuts! Very useful. Using the menus, either type “shorcut” into the search bar under help, or find it under: tools: keyboard shortcuts help. "],
["chapter-5-data-transformation.html", "Chapter 5 - Data transformation 5.2 Notes - Filter rows with filter() 5.2.4 Exercises 5.3 Notes - Arrange rows with arrange() 5.3.1 Exercises 5.4 Notes - Select columns with select() 5.4.1 Exercises 5.5 Notes - Add new variables with mutate() 5.5.2 Exercises 5.6 Notes - Grouped summaries with summarise() 5.6.7 Exercises 5.7 Notes - Grouped mutates (and filters) 5.7.1 Exercises", " Chapter 5 - Data transformation The data that we will work with in chapter 5 is from the nycflights13 package. library(tidyverse) library(nycflights13) 5.2 Notes - Filter rows with filter() filter() will subset obervations based on their values. I think it works a lot like the which() function in base R (ie: data[which(data$variable &gt; value),] ). Below is a way to do the same thing using either filter() or base R which(). # tidyverse filter() output filter(flights, month == 1, day == 1) ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 832 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # base R way to get the same output flights[which(flights$month ==1 &amp; flights$day ==1),] ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 832 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 5.2.1 Comparisons A safer way for comparing two numeric vectors is the near() function in dplyr. For example, if running this comparison: sqrt(2) ^ 2 == 2 ## [1] FALSE 1/49 * 49 == 1 ## [1] FALSE We see that what we would normally regard as true is specified as FALSE in R, due to floating point precision issues. The near() function will allow more tolerance. near(sqrt(2) ^ 2, 2) ## [1] TRUE near(1 / 49 * 49, 1) ## [1] TRUE 5.2.2 Logical Operators There are many ways to combine “and”, &amp;, “or”, |, and “not”,! to filter out observations in a data table. #following two filter functions give same output filter(flights, month == 11 | month == 12) ## # A tibble: 55,403 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 11 1 5 2359 6 352 ## 2 2013 11 1 35 2250 105 123 ## 3 2013 11 1 455 500 -5 641 ## 4 2013 11 1 539 545 -6 856 ## 5 2013 11 1 542 545 -3 831 ## 6 2013 11 1 549 600 -11 912 ## 7 2013 11 1 550 600 -10 705 ## 8 2013 11 1 554 600 -6 659 ## 9 2013 11 1 554 600 -6 826 ## 10 2013 11 1 554 600 -6 749 ## # … with 55,393 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; filter(flights, month %in% c(11,12)) ## # A tibble: 55,403 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 11 1 5 2359 6 352 ## 2 2013 11 1 35 2250 105 123 ## 3 2013 11 1 455 500 -5 641 ## 4 2013 11 1 539 545 -6 856 ## 5 2013 11 1 542 545 -3 831 ## 6 2013 11 1 549 600 -11 912 ## 7 2013 11 1 550 600 -10 705 ## 8 2013 11 1 554 600 -6 659 ## 9 2013 11 1 554 600 -6 826 ## 10 2013 11 1 554 600 -6 749 ## # … with 55,393 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; #following two filter functions give same output filter(flights, !(arr_delay &gt; 120 | dep_delay &gt; 120)) ## # A tibble: 316,050 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 316,040 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; filter(flights, arr_delay &lt;= 120 &amp; !dep_delay &gt; 120) ## # A tibble: 316,050 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 316,040 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 5.2.3 Missing values Missing values are represented as NA. NA values are “contagious,” meaning that any operation or comparison with NA will also return NA. If a data frame contains NA values, they will not be returned by filter() unless specifically asked for, using is.na() df &lt;- tibble(x = c(1, NA, 3)) filter(df, x &gt; 1) ## # A tibble: 1 x 1 ## x ## &lt;dbl&gt; ## 1 3 #&gt; # A tibble: 1 × 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 3 filter(df, is.na(x) | x &gt; 1) ## # A tibble: 2 x 1 ## x ## &lt;dbl&gt; ## 1 NA ## 2 3 5.2.4 Exercises 1. Find all flights that Had an arrival delay of two or more hours # arr_delay is in minutes, so 120 for two hours filter(flights, arr_delay &gt;= 120) ## # A tibble: 10,200 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 811 630 101 1047 ## 2 2013 1 1 848 1835 853 1001 ## 3 2013 1 1 957 733 144 1056 ## 4 2013 1 1 1114 900 134 1447 ## 5 2013 1 1 1505 1310 115 1638 ## 6 2013 1 1 1525 1340 105 1831 ## 7 2013 1 1 1549 1445 64 1912 ## 8 2013 1 1 1558 1359 119 1718 ## 9 2013 1 1 1732 1630 62 2028 ## 10 2013 1 1 1803 1620 103 2008 ## # … with 10,190 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Flew to Houston (IAH or HOU) filter(flights, dest == &quot;IAH&quot; | dest == &quot;HOU&quot;) ## # A tibble: 9,313 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 623 627 -4 933 ## 4 2013 1 1 728 732 -4 1041 ## 5 2013 1 1 739 739 0 1104 ## 6 2013 1 1 908 908 0 1228 ## 7 2013 1 1 1028 1026 2 1350 ## 8 2013 1 1 1044 1045 -1 1352 ## 9 2013 1 1 1114 900 134 1447 ## 10 2013 1 1 1205 1200 5 1503 ## # … with 9,303 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Were operated by United, American, or Delta #find all unique carriers: unique(flights$carrier) ## [1] &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; &quot;DL&quot; &quot;EV&quot; &quot;MQ&quot; &quot;US&quot; &quot;WN&quot; &quot;VX&quot; &quot;FL&quot; &quot;AS&quot; &quot;9E&quot; &quot;F9&quot; &quot;HA&quot; ## [15] &quot;YV&quot; &quot;OO&quot; #Symbol for United = UA, American = AA, Delta = DL filter (flights, carrier %in% c(&quot;UA&quot;, &quot;AA&quot;,&quot;DL&quot;)) ## # A tibble: 139,504 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 554 600 -6 812 ## 5 2013 1 1 554 558 -4 740 ## 6 2013 1 1 558 600 -2 753 ## 7 2013 1 1 558 600 -2 924 ## 8 2013 1 1 558 600 -2 923 ## 9 2013 1 1 559 600 -1 941 ## 10 2013 1 1 559 600 -1 854 ## # … with 139,494 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Departed in summer (July, August, and September) filter (flights, month &gt;= 7 &amp; month &lt;=9) ## # A tibble: 86,326 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 7 1 1 2029 212 236 ## 2 2013 7 1 2 2359 3 344 ## 3 2013 7 1 29 2245 104 151 ## 4 2013 7 1 43 2130 193 322 ## 5 2013 7 1 44 2150 174 300 ## 6 2013 7 1 46 2051 235 304 ## 7 2013 7 1 48 2001 287 308 ## 8 2013 7 1 58 2155 183 335 ## 9 2013 7 1 100 2146 194 327 ## 10 2013 7 1 100 2245 135 337 ## # … with 86,316 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # test whether the output only has months 7 8 9 to check our work. unique(filter (flights, month &gt;= 7 &amp; month &lt;=9)$month) ## [1] 7 8 9 Arrived more than two hours late, but didn’t leave late filter (flights, arr_delay &gt; 120, dep_delay &lt;=0) ## # A tibble: 29 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 27 1419 1420 -1 1754 ## 2 2013 10 7 1350 1350 0 1736 ## 3 2013 10 7 1357 1359 -2 1858 ## 4 2013 10 16 657 700 -3 1258 ## 5 2013 11 1 658 700 -2 1329 ## 6 2013 3 18 1844 1847 -3 39 ## 7 2013 4 17 1635 1640 -5 2049 ## 8 2013 4 18 558 600 -2 1149 ## 9 2013 4 18 655 700 -5 1213 ## 10 2013 5 22 1827 1830 -3 2217 ## # … with 19 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; #looks like combining the two arguments into one does the same thing. filter(flights, arr_delay &gt; 120 &amp; dep_delay &lt;=0) ## # A tibble: 29 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 27 1419 1420 -1 1754 ## 2 2013 10 7 1350 1350 0 1736 ## 3 2013 10 7 1357 1359 -2 1858 ## 4 2013 10 16 657 700 -3 1258 ## 5 2013 11 1 658 700 -2 1329 ## 6 2013 3 18 1844 1847 -3 39 ## 7 2013 4 17 1635 1640 -5 2049 ## 8 2013 4 18 558 600 -2 1149 ## 9 2013 4 18 655 700 -5 1213 ## 10 2013 5 22 1827 1830 -3 2217 ## # … with 19 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Were delayed by at least an hour, but made up over 30 minutes in flight # if delayed 60 minutes but made up at least 30, expect arr_delay to be less than 60-30 = 30 min filter (flights, dep_delay &gt;= 60, arr_delay &lt; 30) ## # A tibble: 206 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 3 1850 1745 65 2148 ## 2 2013 1 3 1950 1845 65 2228 ## 3 2013 1 3 2015 1915 60 2135 ## 4 2013 1 6 1019 900 79 1558 ## 5 2013 1 7 1543 1430 73 1758 ## 6 2013 1 11 1020 920 60 1311 ## 7 2013 1 12 1706 1600 66 1949 ## 8 2013 1 12 1953 1845 68 2154 ## 9 2013 1 19 1456 1355 61 1636 ## 10 2013 1 21 1531 1430 61 1843 ## # … with 196 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Departed between midnight and 6am (inclusive) times &lt;- filter (flights, dep_time &gt;= 0 &amp; dep_time &lt;= 600) times ## # A tibble: 9,344 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 9,334 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; #check if it worked range(times$dep_time) ## [1] 1 600 Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges? ?between() states that this is a shortcut to perform the same function as x &gt;= left &amp; x &lt;= right, for between(x, left, right). I will use between() to produce the same result as in the previous bullet point for flights departing between midnight and 6am. filter(flights, between(dep_time, 0, 600)) ## # A tibble: 9,344 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 9,334 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2. How many flights have a missing dep_time? What other variables are missing? What might these rows represent? filter(flights, is.na(dep_time)) ## # A tibble: 8,255 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 NA 1630 NA NA ## 2 2013 1 1 NA 1935 NA NA ## 3 2013 1 1 NA 1500 NA NA ## 4 2013 1 1 NA 600 NA NA ## 5 2013 1 2 NA 1540 NA NA ## 6 2013 1 2 NA 1620 NA NA ## 7 2013 1 2 NA 1355 NA NA ## 8 2013 1 2 NA 1420 NA NA ## 9 2013 1 2 NA 1321 NA NA ## 10 2013 1 2 NA 1545 NA NA ## # … with 8,245 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The flights with missing dep_time also have missing arr_time and air_time, suggesting that these were cancelled flights. 3. Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE &amp; NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!) Since NA represents an unknown value, it still obeys certain rules as if it were a known value. Since any number to the power of 0 is 1, NA^0 returns the value 1, which will make the code filter(flights, dep_time == NA^0) return all the flights that departed at time 0001. NA | TRUE is an expression that evaluates to TRUE, because the logical operator will evaluate whether either side has a TRUE value. This would return all the data points in the data frame. FALSE &amp; NA returns FALSE based on the same premise as the previous item. Since FALSE exists on either side of the &amp; logical operator, it is evaluated as FALSE. The general (but not concrete) rule is that modifying NA with a logical operator in the form NA &lt;operator&gt; value will evaluate to either TRUE or FALSE, returning not missing values, and that NA will still abide by certain rules that any value would abide by. NA*0, however, is an exception beacuse this still evaluates to NA, whereas other values would evaluate to 0. NA^0 ## [1] 1 NA | TRUE ## [1] TRUE FALSE &amp; NA ## [1] FALSE NA*0 ## [1] NA filter(flights, dep_time == NA^0) ## # A tibble: 25 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 13 1 2249 72 108 ## 2 2013 1 31 1 2100 181 124 ## 3 2013 11 13 1 2359 2 442 ## 4 2013 12 16 1 2359 2 447 ## 5 2013 12 20 1 2359 2 430 ## 6 2013 12 26 1 2359 2 437 ## 7 2013 12 30 1 2359 2 441 ## 8 2013 2 11 1 2100 181 111 ## 9 2013 2 24 1 2245 76 121 ## 10 2013 3 8 1 2355 6 431 ## # … with 15 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; filter(flights, dep_time == NA | TRUE) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 5.3 Notes - Arrange rows with arrange() arrange() will return a data frame with the observations sorted by the variable you specify. It functions similarly to the order() function in base R. Below are two ways to get the same sorted dataframe using arrange() and order(). You can see that arrange() makes things a little simpler to read. For the base R order() function, it will only return a sorted list of values, so you have to pass them into the flights[] frame to obtain all the values for the sorted data. # using arrange() arrange(flights, desc(arr_delay)) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 9 641 900 1301 1242 ## 2 2013 6 15 1432 1935 1137 1607 ## 3 2013 1 10 1121 1635 1126 1239 ## 4 2013 9 20 1139 1845 1014 1457 ## 5 2013 7 22 845 1600 1005 1044 ## 6 2013 4 10 1100 1900 960 1342 ## 7 2013 3 17 2321 810 911 135 ## 8 2013 7 22 2257 759 898 121 ## 9 2013 12 5 756 1700 896 1058 ## 10 2013 5 3 1133 2055 878 1250 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # using base R order() flights[order(flights$arr_delay, decreasing = T),] ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 9 641 900 1301 1242 ## 2 2013 6 15 1432 1935 1137 1607 ## 3 2013 1 10 1121 1635 1126 1239 ## 4 2013 9 20 1139 1845 1014 1457 ## 5 2013 7 22 845 1600 1005 1044 ## 6 2013 4 10 1100 1900 960 1342 ## 7 2013 3 17 2321 810 911 135 ## 8 2013 7 22 2257 759 898 121 ## 9 2013 12 5 756 1700 896 1058 ## 10 2013 5 3 1133 2055 878 1250 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Missing values (NA) are placed at the end for arrange() df &lt;- tibble(x = c(5, 2, NA)) arrange(df, x) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 2 ## 2 5 ## 3 NA arrange(df, desc(x)) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 5 ## 2 2 ## 3 NA 5.3.1 Exercises 1. How could you use arrange() to sort all missing values to the start? (Hint: use is.na()). arrange(flights, desc(is.na(dep_time))) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 NA 1630 NA NA ## 2 2013 1 1 NA 1935 NA NA ## 3 2013 1 1 NA 1500 NA NA ## 4 2013 1 1 NA 600 NA NA ## 5 2013 1 2 NA 1540 NA NA ## 6 2013 1 2 NA 1620 NA NA ## 7 2013 1 2 NA 1355 NA NA ## 8 2013 1 2 NA 1420 NA NA ## 9 2013 1 2 NA 1321 NA NA ## 10 2013 1 2 NA 1545 NA NA ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2. Sort flights to find the most delayed flights. Find the flights that left earliest. # most delayed flights arrange(flights, desc(dep_delay)) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 9 641 900 1301 1242 ## 2 2013 6 15 1432 1935 1137 1607 ## 3 2013 1 10 1121 1635 1126 1239 ## 4 2013 9 20 1139 1845 1014 1457 ## 5 2013 7 22 845 1600 1005 1044 ## 6 2013 4 10 1100 1900 960 1342 ## 7 2013 3 17 2321 810 911 135 ## 8 2013 6 27 959 1900 899 1236 ## 9 2013 7 22 2257 759 898 121 ## 10 2013 12 5 756 1700 896 1058 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # flights that left earliest (least amount of delay) arrange(flights, dep_delay) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 12 7 2040 2123 -43 40 ## 2 2013 2 3 2022 2055 -33 2240 ## 3 2013 11 10 1408 1440 -32 1549 ## 4 2013 1 11 1900 1930 -30 2233 ## 5 2013 1 29 1703 1730 -27 1947 ## 6 2013 8 9 729 755 -26 1002 ## 7 2013 10 23 1907 1932 -25 2143 ## 8 2013 3 30 2030 2055 -25 2213 ## 9 2013 3 2 1431 1455 -24 1601 ## 10 2013 5 5 934 958 -24 1225 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 3. Sort flights to find the fastest flights. # fastest flights arrange (flights, arr_delay) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 5 7 1715 1729 -14 1944 ## 2 2013 5 20 719 735 -16 951 ## 3 2013 5 2 1947 1949 -2 2209 ## 4 2013 5 6 1826 1830 -4 2045 ## 5 2013 5 4 1816 1820 -4 2017 ## 6 2013 5 2 1926 1929 -3 2157 ## 7 2013 5 6 1753 1755 -2 2004 ## 8 2013 5 7 2054 2055 -1 2317 ## 9 2013 5 13 657 700 -3 908 ## 10 2013 1 4 1026 1030 -4 1305 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # find out which airlines had the top 1,000 fastest flights top1000_fastest &lt;- arrange(flights,arr_delay)[1:1000,] ggplot (top1000_fastest, aes ( x = carrier, fill = carrier))+ geom_bar() # compare total air time vs dep_delay to see if there are any trends between airlines ggplot (top1000_fastest, aes (x = arr_delay, y = air_time))+ geom_point( aes (color = carrier)) Based on the bar plot, within the top 1000 flights that landed early, AA, DL, and UA have more than other airlines. Looking at the scatterplot, it seems UA generally has medium-length flights that arrive ahead of schedule, whereas HA has very long flights that arrive ahead of schedule, and 9E has very short flights that arrive ahead of schedule. 4. Which flights travelled the longest? Which travelled the shortest? The shortest flights were from EWR to BDL, taking around 22 minutes. Of the top 100 shortest flights, flight number 4276 was the most frequent. The longest flights were from JFK to HNL or EWR to HNL, and lasted around 654 minutes. Of the top 100 longest flights, flight number 51 was the most frequent. # flights that travelled the shortest shortest &lt;- arrange(flights, air_time)[1:100,] shortest ## # A tibble: 100 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 16 1355 1315 40 1442 ## 2 2013 4 13 537 527 10 622 ## 3 2013 12 6 922 851 31 1021 ## 4 2013 2 3 2153 2129 24 2247 ## 5 2013 2 5 1303 1315 -12 1342 ## 6 2013 2 12 2123 2130 -7 2211 ## 7 2013 3 2 1450 1500 -10 1547 ## 8 2013 3 8 2026 1935 51 2131 ## 9 2013 3 18 1456 1329 87 1533 ## 10 2013 3 19 2226 2145 41 2305 ## # … with 90 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # find the most frequent flight number for top 100 shortest flights. arrange(count(shortest, flight),desc(n)) ## # A tibble: 32 x 2 ## flight n ## &lt;int&gt; &lt;int&gt; ## 1 4276 19 ## 2 4368 12 ## 3 3822 8 ## 4 4155 7 ## 5 3847 5 ## 6 4619 5 ## 7 4103 4 ## 8 3825 3 ## 9 4118 3 ## 10 5968 3 ## # … with 22 more rows # flights that travelled the longest longest &lt;- arrange(flights, desc(air_time))[1:100,] longest ## # A tibble: 100 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 3 17 1337 1335 2 1937 ## 2 2013 2 6 853 900 -7 1542 ## 3 2013 3 15 1001 1000 1 1551 ## 4 2013 3 17 1006 1000 6 1607 ## 5 2013 3 16 1001 1000 1 1544 ## 6 2013 2 5 900 900 0 1555 ## 7 2013 11 12 936 930 6 1630 ## 8 2013 3 14 958 1000 -2 1542 ## 9 2013 11 20 1006 1000 6 1639 ## 10 2013 3 15 1342 1335 7 1924 ## # … with 90 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # find the most frequent flight number for top 100 shortest flights. arrange(count(longest, flight),desc(n)) ## # A tibble: 2 x 2 ## flight n ## &lt;int&gt; &lt;int&gt; ## 1 51 61 ## 2 15 39 # get average flight time for top 100 shortest / longest flights mean(shortest$air_time) ## [1] 22.32 mean(longest$air_time) ## [1] 654.47 5.4 Notes - Select columns with select() The select() function allows you to select a subset of columns (variables) from your data frame and return a new data frame with these selected columns. This works similarly to using indexes to pull out columns from a data frame in base R. For example, here is a way to do the same thing both ways: # Select columns by name select(flights, year, month, day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # … with 336,766 more rows # use base R to do the same thing flights[,c(&quot;year&quot;,&quot;month&quot;,&quot;day&quot;)] ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # … with 336,766 more rows Select seems to be more versatile if you want to do other things quickly, like combining it with ends_with(), starts_with(), contains(), matches(), num_range(), etc. # select multiple columns using colon select(flights, year:day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # … with 336,766 more rows # select columns that end with a phrase select(flights, ends_with(&quot;time&quot;)) ## # A tibble: 336,776 x 5 ## dep_time sched_dep_time arr_time sched_arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 830 819 227 ## 2 533 529 850 830 227 ## 3 542 540 923 850 160 ## 4 544 545 1004 1022 183 ## 5 554 600 812 837 116 ## 6 554 558 740 728 150 ## 7 555 600 913 854 158 ## 8 557 600 709 723 53 ## 9 557 600 838 846 140 ## 10 558 600 753 745 138 ## # … with 336,766 more rows A variant of select(), rename(), can rename column variables. This seems very useful. rename(flights, tail_num = tailnum) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tail_num &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; To move variables to the leftmost side using select(), use the everything() function in conjunction with the variables you are pulling out. select(flights, carrier, flight, everything()) ## # A tibble: 336,776 x 19 ## carrier flight year month day dep_time sched_dep_time dep_delay ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 UA 1545 2013 1 1 517 515 2 ## 2 UA 1714 2013 1 1 533 529 4 ## 3 AA 1141 2013 1 1 542 540 2 ## 4 B6 725 2013 1 1 544 545 -1 ## 5 DL 461 2013 1 1 554 600 -6 ## 6 UA 1696 2013 1 1 554 558 -4 ## 7 B6 507 2013 1 1 555 600 -5 ## 8 EV 5708 2013 1 1 557 600 -3 ## 9 B6 79 2013 1 1 557 600 -3 ## 10 AA 301 2013 1 1 558 600 -2 ## # … with 336,766 more rows, and 11 more variables: arr_time &lt;int&gt;, ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; 5.4.1 Exercises 1. Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. # standard way to select select (flights, dep_time, dep_delay, arr_time, arr_delay) ## # A tibble: 336,776 x 4 ## dep_time dep_delay arr_time arr_delay ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 2 830 11 ## 2 533 4 850 20 ## 3 542 2 923 33 ## 4 544 -1 1004 -18 ## 5 554 -6 812 -25 ## 6 554 -4 740 12 ## 7 555 -5 913 19 ## 8 557 -3 709 -14 ## 9 557 -3 838 -8 ## 10 558 -2 753 8 ## # … with 336,766 more rows # select using starts_with() select (flights, starts_with(&quot;dep&quot;),starts_with(&quot;arr&quot;)) ## # A tibble: 336,776 x 4 ## dep_time dep_delay arr_time arr_delay ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 2 830 11 ## 2 533 4 850 20 ## 3 542 2 923 33 ## 4 544 -1 1004 -18 ## 5 554 -6 812 -25 ## 6 554 -4 740 12 ## 7 555 -5 913 19 ## 8 557 -3 709 -14 ## 9 557 -3 838 -8 ## 10 558 -2 753 8 ## # … with 336,766 more rows #can also do some less efficient combination of contains() and subtracting columns. select (flights, contains(&quot;dep_&quot;), contains(&quot;arr_&quot;),-contains(&quot;sched&quot;)) ## # A tibble: 336,776 x 4 ## dep_time dep_delay arr_time arr_delay ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 2 830 11 ## 2 533 4 850 20 ## 3 542 2 923 33 ## 4 544 -1 1004 -18 ## 5 554 -6 812 -25 ## 6 554 -4 740 12 ## 7 555 -5 913 19 ## 8 557 -3 709 -14 ## 9 557 -3 838 -8 ## 10 558 -2 753 8 ## # … with 336,766 more rows 2. What happens if you include the name of a variable multiple times in a select() call? select(flights, dep_time, dep_time) ## # A tibble: 336,776 x 1 ## dep_time ## &lt;int&gt; ## 1 517 ## 2 533 ## 3 542 ## 4 544 ## 5 554 ## 6 554 ## 7 555 ## 8 557 ## 9 557 ## 10 558 ## # … with 336,766 more rows It looks like you will only get the variable one time (it will not duplicate). 3. What does the one_of() function do? Why might it be helpful in conjunction with this vector? one_of() function takes in a vector of characters, which could be names of columns that you want to select. This way, you dont have to have so many arguments in select(). You can pre-make a vector with the columns you want, then select one_of(vars), as shown here. However, I tried just putting the vector in as a argument without one_of() and it gave the same output. vars &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;dep_delay&quot;, &quot;arr_delay&quot;) #use the one_of() function to select each of the specified columns in vars select(flights, one_of(vars)) ## # A tibble: 336,776 x 5 ## year month day dep_delay arr_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 ## 2 2013 1 1 4 20 ## 3 2013 1 1 2 33 ## 4 2013 1 1 -1 -18 ## 5 2013 1 1 -6 -25 ## 6 2013 1 1 -4 12 ## 7 2013 1 1 -5 19 ## 8 2013 1 1 -3 -14 ## 9 2013 1 1 -3 -8 ## 10 2013 1 1 -2 8 ## # … with 336,766 more rows # it seems like this also works to give the same output. select(flights, vars) ## # A tibble: 336,776 x 5 ## year month day dep_delay arr_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 ## 2 2013 1 1 4 20 ## 3 2013 1 1 2 33 ## 4 2013 1 1 -1 -18 ## 5 2013 1 1 -6 -25 ## 6 2013 1 1 -4 12 ## 7 2013 1 1 -5 19 ## 8 2013 1 1 -3 -14 ## 9 2013 1 1 -3 -8 ## 10 2013 1 1 -2 8 ## # … with 336,766 more rows 4. Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? select(flights, contains(&quot;TIME&quot;)) ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 830 819 227 ## 2 533 529 850 830 227 ## 3 542 540 923 850 160 ## 4 544 545 1004 1022 183 ## 5 554 600 812 837 116 ## 6 554 558 740 728 150 ## 7 555 600 913 854 158 ## 8 557 600 709 723 53 ## 9 557 600 838 846 140 ## 10 558 600 753 745 138 ## # … with 336,766 more rows, and 1 more variable: time_hour &lt;dttm&gt; The code returns columns that have lowercase time in them, even though we specified TIME in uppercase. This is not surprising because ?contains() specifies that “ignore.case = TRUE” by default. To get only columns with uppercase TIME, we can write: select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) ## # A tibble: 336,776 x 0 Since no columns in the flights data frame have the uppercase TIME in them, nothing is returned. 5.5 Notes - Add new variables with mutate() # view the data as a spreadsheet with View() - note capital V # View(flights) # select a subset of columns so data is easier to work with for demonstration purposes flights_sml &lt;- select(flights, year:day, ends_with(&quot;delay&quot;), distance, air_time ) flights_sml ## # A tibble: 336,776 x 7 ## year month day dep_delay arr_delay distance air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 1400 227 ## 2 2013 1 1 4 20 1416 227 ## 3 2013 1 1 2 33 1089 160 ## 4 2013 1 1 -1 -18 1576 183 ## 5 2013 1 1 -6 -25 762 116 ## 6 2013 1 1 -4 12 719 150 ## 7 2013 1 1 -5 19 1065 158 ## 8 2013 1 1 -3 -14 229 53 ## 9 2013 1 1 -3 -8 944 140 ## 10 2013 1 1 -2 8 733 138 ## # … with 336,766 more rows To add new columns to the dataset that are functions of existing columns, use the mutate() function. You can even refer to newly created columns in the same call, which seems like magic. The new columns are appended to the end of the data frame. mutate(flights_sml, gain = arr_delay - dep_delay, hours = air_time / 60, gain_per_hour = gain / hours ) ## # A tibble: 336,776 x 10 ## year month day dep_delay arr_delay distance air_time gain hours ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 1400 227 9 3.78 ## 2 2013 1 1 4 20 1416 227 16 3.78 ## 3 2013 1 1 2 33 1089 160 31 2.67 ## 4 2013 1 1 -1 -18 1576 183 -17 3.05 ## 5 2013 1 1 -6 -25 762 116 -19 1.93 ## 6 2013 1 1 -4 12 719 150 16 2.5 ## 7 2013 1 1 -5 19 1065 158 24 2.63 ## 8 2013 1 1 -3 -14 229 53 -11 0.883 ## 9 2013 1 1 -3 -8 944 140 -5 2.33 ## 10 2013 1 1 -2 8 733 138 10 2.3 ## # … with 336,766 more rows, and 1 more variable: gain_per_hour &lt;dbl&gt; To keep only the newly created columns, use transmute(): transmute(flights, gain = arr_delay - dep_delay, hours = air_time / 60, gain_per_hour = gain / hours ) ## # A tibble: 336,776 x 3 ## gain hours gain_per_hour ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 3.78 2.38 ## 2 16 3.78 4.23 ## 3 31 2.67 11.6 ## 4 -17 3.05 -5.57 ## 5 -19 1.93 -9.83 ## 6 16 2.5 6.4 ## 7 24 2.63 9.11 ## 8 -11 0.883 -12.5 ## 9 -5 2.33 -2.14 ## 10 10 2.3 4.35 ## # … with 336,766 more rows In general, all functions or operators that can be applied to vectors and return vectors with the same number of values as output can be used with mutate() or transmute(). Examples include arithmetic operators, modular arithmetic, logs, offsets (lead() and lag()), cumulative sum/averages, logical comparisons (returns boolean for each value in vector), Another provided example: # convert dep_time to hours and minutes using modulus and remainder transmute(flights, dep_time, hour = dep_time %/% 100, minute = dep_time %% 100 ) ## # A tibble: 336,776 x 3 ## dep_time hour minute ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 5 17 ## 2 533 5 33 ## 3 542 5 42 ## 4 544 5 44 ## 5 554 5 54 ## 6 554 5 54 ## 7 555 5 55 ## 8 557 5 57 ## 9 557 5 57 ## 10 558 5 58 ## # … with 336,766 more rows y &lt;- c(1, 2, NA, 2, 4, 3) min_rank(y) ## [1] 1 2 NA 2 5 4 rank(y) ## [1] 1.0 2.5 6.0 2.5 5.0 4.0 5.5.2 Exercises 1. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. To convert military hours to minutes since midnight, first find how many hours it’s been (%/% 100), then multiply that by 60 to get the minutes, then add the remainin minutes (%% 100). Below is a table of the old columns and new columns. transmute(flights, dep_time, sched_dep_time, dep_time_min = (dep_time %/% 100)*60 + (dep_time %% 100), sched_dep_time_min = (dep_time %/% 100)*60 + (dep_time %% 100) ) ## # A tibble: 336,776 x 4 ## dep_time sched_dep_time dep_time_min sched_dep_time_min ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 515 317 317 ## 2 533 529 333 333 ## 3 542 540 342 342 ## 4 544 545 344 344 ## 5 554 600 354 354 ## 6 554 558 354 354 ## 7 555 600 355 355 ## 8 557 600 357 357 ## 9 557 600 357 357 ## 10 558 600 358 358 ## # … with 336,766 more rows 2. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? I expect to see that arr_time - dep_time = air_time. However, the values do not match because arr_time - dep_time returns the amount of time in hours:minutes, whereas air_time is in total minutes. We would have to convert the output into total minutes. However, this still does not fix the problem. There is also the issue of time zones. Depending on where the plane flew, the air_time could be consistent but the arr_time could be way off. We can see from the first two rows that two different flights that had different arr_time and dep_times had the same air_time! # gives time in hours:min transmute (flights, arr_time, dep_time, air_time, my_air_time = arr_time - dep_time) ## # A tibble: 336,776 x 4 ## arr_time dep_time air_time my_air_time ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 830 517 227 313 ## 2 850 533 227 317 ## 3 923 542 160 381 ## 4 1004 544 183 460 ## 5 812 554 116 258 ## 6 740 554 150 186 ## 7 913 555 158 358 ## 8 709 557 53 152 ## 9 838 557 140 281 ## 10 753 558 138 195 ## # … with 336,766 more rows # convert to total minutes transmute (flights, arr_time, dep_time, air_time, my_air_time = ((arr_time %/% 100)*60 + arr_time %% 100) - ((dep_time %/% 100)*60 + dep_time %% 100)) ## # A tibble: 336,776 x 4 ## arr_time dep_time air_time my_air_time ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 830 517 227 193 ## 2 850 533 227 197 ## 3 923 542 160 221 ## 4 1004 544 183 260 ## 5 812 554 116 138 ## 6 740 554 150 106 ## 7 913 555 158 198 ## 8 709 557 53 72 ## 9 838 557 140 161 ## 10 753 558 138 115 ## # … with 336,766 more rows 3. Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? I would expect that dep_time - sched_dep_time, converted to minutes, would equal dep_delay. transmute (flights, dep_time, sched_dep_time, dep_delay, my_dep_delay = ((dep_time %/% 100)*60 + dep_time %% 100) - ((sched_dep_time %/% 100)*60 + sched_dep_time %% 100)) ## # A tibble: 336,776 x 4 ## dep_time sched_dep_time dep_delay my_dep_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 515 2 2 ## 2 533 529 4 4 ## 3 542 540 2 2 ## 4 544 545 -1 -1 ## 5 554 600 -6 -6 ## 6 554 558 -4 -4 ## 7 555 600 -5 -5 ## 8 557 600 -3 -3 ## 9 557 600 -3 -3 ## 10 558 600 -2 -2 ## # … with 336,766 more rows 4. Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank(). I suppose we could arrange dep_delay to find the top most delayed flights. Using min_rank() will rank the delayed flights - if we rank the delayed flights and then sort on the rank, we see that the most delayed flight is rank 328521, with a delay of 1301 minutes. The default ties.method for min_rank is “min”. transmute(flights, dep_delay, rank_delay = min_rank(dep_delay)) ## # A tibble: 336,776 x 2 ## dep_delay rank_delay ## &lt;dbl&gt; &lt;int&gt; ## 1 2 208140 ## 2 4 219823 ## 3 2 208140 ## 4 -1 164763 ## 5 -6 48888 ## 6 -4 94410 ## 7 -5 69589 ## 8 -3 119029 ## 9 -3 119029 ## 10 -2 143247 ## # … with 336,766 more rows sorted_flights &lt;- transmute(flights, dep_delay, rank_delay = min_rank(dep_delay)) %&gt;% arrange(desc(rank_delay)) sorted_flights[1:10,] ## # A tibble: 10 x 2 ## dep_delay rank_delay ## &lt;dbl&gt; &lt;int&gt; ## 1 1301 328521 ## 2 1137 328520 ## 3 1126 328519 ## 4 1014 328518 ## 5 1005 328517 ## 6 960 328516 ## 7 911 328515 ## 8 899 328514 ## 9 898 328513 ## 10 896 328512 5. What does 1:3 + 1:10 return? Why? # returns error 1:3 + 1:10 ## Warning in 1:3 + 1:10: longer object length is not a multiple of shorter ## object length ## [1] 2 4 6 5 7 9 8 10 12 11 # if adding to a multiple: 1:3 + 1:9 ## [1] 2 4 6 5 7 9 8 10 12 6. What trigonometric functions does R provide? Taken from the R documentation: “These functions give the obvious trigonometric functions. They respectively compute the cosine, sine, tangent, arc-cosine, arc-sine, arc-tangent, and the two-argument arc-tangent.” cospi(x), sinpi(x), and tanpi(x), compute cos(pi*x), sin(pi*x), and tan(pi*x). 5.6 Notes - Grouped summaries with summarise() summarise(), in its simplest usage, can perform a function on a column in the data set and return the output as a single row: summarise(flights, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 x 1 ## delay ## &lt;dbl&gt; ## 1 12.6 A more advanced usage of summarise() is when paired with group_by(). This will return the function on each of the subgroups from group_by(), and giving statistics “by group”. Looking at the group_by() output, there doesnt seem to be any striking difference between the original flights data frame and the grouped data frame. The result of using group_by() is not immediately apparent unless paired with summarise(). It would be interesting to know what other usages group_by() can have. by_day &lt;- group_by(flights, year, month, day) summarise(by_day, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # … with 355 more rows 5.6.1 Combining multiple operations with the pipe Learning how to use the pipe - the pipe, or %&gt;% can be used to more efficiently run sequential of functions on a variable and its output. This reduces the amount of naming intermediate variables we have to do. If we care about intermediate variables (ie, performing other analysis or using the vairable for other purposes) then I would not use the pipe. If i do not care about the intermeidate variables and want to quickly get output, the pipe would be useful. Here is the provided example about writing code without or with pipes: # not using pipes - note all the intermediate variables by_dest &lt;- group_by(flights, dest) delay &lt;- summarise(by_dest, count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) delay &lt;- filter(delay, count &gt; 20, dest != &quot;HNL&quot;) # using pipes delay &lt;- flights %&gt;% group_by(dest) %&gt;% summarise( count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% filter(count &gt; 20, dest != &quot;HNL&quot;) # It looks like delays increase with distance up to ~750 miles # and then decrease. Maybe as flights get longer there&#39;s more # ability to make up delays in the air? ggplot(data = delay, mapping = aes(x = dist, y = delay)) + geom_point(aes(size = count), alpha = 1/3) + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; `geom_smooth()` using method = &#39;loess&#39; 5.6.2 Missing values Without setting na.rm, the following code does not produce any means using summarse(). Instead, all the values under the mean column are NA. This is beacuse aggregating NA with any other numbers will return NA. We must call na.rm = TRUE in the summarise() function to produce meaningul values. # without na.rm flights %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay)) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day mean ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 NA ## 2 2013 1 2 NA ## 3 2013 1 3 NA ## 4 2013 1 4 NA ## 5 2013 1 5 NA ## 6 2013 1 6 NA ## 7 2013 1 7 NA ## 8 2013 1 8 NA ## 9 2013 1 9 NA ## 10 2013 1 10 NA ## # … with 355 more rows # with na.rm flights %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day mean ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # … with 355 more rows To get a data frame without any of the NA values (cancelled flights): # 2 ways to use filter() to get the non-cancelled flights (not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay))) ## # A tibble: 327,346 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 327,336 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; (not_cancelled2 &lt;- flights %&gt;% filter(!(is.na(dep_delay) | is.na(arr_delay)))) ## # A tibble: 327,346 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 327,336 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay)) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day mean ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.4 ## 2 2013 1 2 13.7 ## 3 2013 1 3 10.9 ## 4 2013 1 4 8.97 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.56 ## 9 2013 1 9 2.30 ## 10 2013 1 10 2.84 ## # … with 355 more rows 5.6.3 Counts When using summarise, its important to know how many observations each summary value was being computed on. If the counts are low, the variance of the summary value might be very high, and the results may not be as interpretable or reliable. delays &lt;- not_cancelled %&gt;% group_by(tailnum) %&gt;% summarise( delay = mean(arr_delay) ) ggplot(data = delays, mapping = aes(x = delay)) + geom_freqpoly(binwidth = 10) Here we see that some flights have very high delay values, but these flights also don’t have very many counts. To count how many observations each summary value was computed on, use the n() function in summarize(). delays &lt;- not_cancelled %&gt;% group_by(tailnum) %&gt;% summarise( delay = mean(arr_delay, na.rm = TRUE), n = n() ) # basically the previous graph flipped on its side ggplot(data = delays, mapping = aes(x = n, y = delay)) + geom_point(alpha = 1/10) We can filter out the observations based on less than 25 counts using filter(), and then pipe the result into ggplot. delays %&gt;% filter(n &gt; 25) %&gt;% ggplot(mapping = aes(x = n, y = delay)) + geom_point(alpha = 1/10) + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; 5.6.4 Useful summary functions Types of functions that you can use in summarize are: mean(), median(), sd(), IQR(), mad(), min(), quantile(), max(), first(), nth(), last(), n(), sum (!is.na()), counts of logical variables (sum(x&gt;20)), and more. And subsetting values prior to performing the function using &gt; &lt;, etc. When subsetting, it is important not to be confused between getting the mean of the subsetted values vs the proportion of the subsetted values that satisfy the condition: # get mean delay of flights delayed by more than 60 hours not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise( avg_delay_over60 = mean(arr_delay[arr_delay &gt; 60]) # the average positive delay ) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day avg_delay_over60 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 131. ## 2 2013 1 2 116. ## 3 2013 1 3 109. ## 4 2013 1 4 104. ## 5 2013 1 5 103. ## 6 2013 1 6 90.5 ## 7 2013 1 7 106. ## 8 2013 1 8 100. ## 9 2013 1 9 165. ## 10 2013 1 10 183. ## # … with 355 more rows # get proportion of flights delayed for more than 60 hours not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise(hour_perc = mean(arr_delay &gt; 60)) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day hour_perc ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 0.0722 ## 2 2013 1 2 0.0851 ## 3 2013 1 3 0.0567 ## 4 2013 1 4 0.0396 ## 5 2013 1 5 0.0349 ## 6 2013 1 6 0.0470 ## 7 2013 1 7 0.0333 ## 8 2013 1 8 0.0213 ## 9 2013 1 9 0.0202 ## 10 2013 1 10 0.0183 ## # … with 355 more rows The example in the book provides two ways to find the min &amp; max observation for each group of flights (although the output is in a different format), which I thought was interesting. # using summarise() not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise( first_dep = first(dep_time), last_dep = last(dep_time) ) ## # A tibble: 365 x 5 ## # Groups: year, month [?] ## year month day first_dep last_dep ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 2356 ## 2 2013 1 2 42 2354 ## 3 2013 1 3 32 2349 ## 4 2013 1 4 25 2358 ## 5 2013 1 5 14 2357 ## 6 2013 1 6 16 2355 ## 7 2013 1 7 49 2359 ## 8 2013 1 8 454 2351 ## 9 2013 1 9 2 2252 ## 10 2013 1 10 3 2320 ## # … with 355 more rows # using mutate() &amp; filter() not_cancelled %&gt;% group_by(year, month, day) %&gt;% mutate(r = min_rank(desc(dep_time))) %&gt;% filter(r %in% range(r)) ## # A tibble: 770 x 20 ## # Groups: year, month, day [365] ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 2356 2359 -3 425 ## 3 2013 1 2 42 2359 43 518 ## 4 2013 1 2 2354 2359 -5 413 ## 5 2013 1 3 32 2359 33 504 ## 6 2013 1 3 2349 2359 -10 434 ## 7 2013 1 4 25 2359 26 505 ## 8 2013 1 4 2358 2359 -1 429 ## 9 2013 1 4 2358 2359 -1 436 ## 10 2013 1 5 14 2359 15 503 ## # … with 760 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, r &lt;int&gt; Also, I thought it was very useful how you can weight counts based on another variable, so that you can get a sum of total values of a different variable grouped on another set of variables (basically a shorter way to use group_by and summarize-sum() together, as shown below). # sum using weighted counts() not_cancelled %&gt;% count(tailnum, wt = distance) ## # A tibble: 4,037 x 2 ## tailnum n ## &lt;chr&gt; &lt;dbl&gt; ## 1 D942DN 3418 ## 2 N0EGMQ 239143 ## 3 N10156 109664 ## 4 N102UW 25722 ## 5 N103US 24619 ## 6 N104UW 24616 ## 7 N10575 139903 ## 8 N105UW 23618 ## 9 N107US 21677 ## 10 N108UW 32070 ## # … with 4,027 more rows # sum using group_by and summarise() + sum() not_cancelled %&gt;% group_by(tailnum) %&gt;% summarise(sum_distances = sum(distance)) ## # A tibble: 4,037 x 2 ## tailnum sum_distances ## &lt;chr&gt; &lt;dbl&gt; ## 1 D942DN 3418 ## 2 N0EGMQ 239143 ## 3 N10156 109664 ## 4 N102UW 25722 ## 5 N103US 24619 ## 6 N104UW 24616 ## 7 N10575 139903 ## 8 N105UW 23618 ## 9 N107US 21677 ## 10 N108UW 32070 ## # … with 4,027 more rows To count unique values, use n_distinct(): not_cancelled %&gt;% group_by(dest) %&gt;% summarise(carriers = n_distinct(carrier)) %&gt;% arrange(desc(carriers)) ## # A tibble: 104 x 2 ## dest carriers ## &lt;chr&gt; &lt;int&gt; ## 1 ATL 7 ## 2 BOS 7 ## 3 CLT 7 ## 4 ORD 7 ## 5 TPA 7 ## 6 AUS 6 ## 7 DCA 6 ## 8 DTW 6 ## 9 IAD 6 ## 10 MSP 6 ## # … with 94 more rows 5.6.5 Grouping by multiple variables You can progressively peel off groupings by re-calling summarise() on previous summarise() tables. Must be careful to use aggregation functions that make sense, like sum(), and not rank-based statistics like median(). They initial grouped data frame can be ungrouped manually as well. # group the data daily &lt;- group_by(flights, year, month, day) # use summarise() to get metric per group (per_day &lt;- summarise(daily, flights = n())) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day flights ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 842 ## 2 2013 1 2 943 ## 3 2013 1 3 914 ## 4 2013 1 4 915 ## 5 2013 1 5 720 ## 6 2013 1 6 832 ## 7 2013 1 7 933 ## 8 2013 1 8 899 ## 9 2013 1 9 902 ## 10 2013 1 10 932 ## # … with 355 more rows # use summarise() on the previous summary to get metric one level up (per_month &lt;- summarise(per_day, flights = sum(flights))) ## # A tibble: 12 x 3 ## # Groups: year [?] ## year month flights ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 27004 ## 2 2013 2 24951 ## 3 2013 3 28834 ## 4 2013 4 28330 ## 5 2013 5 28796 ## 6 2013 6 28243 ## 7 2013 7 29425 ## 8 2013 8 29327 ## 9 2013 9 27574 ## 10 2013 10 28889 ## 11 2013 11 27268 ## 12 2013 12 28135 # use summarise() on the previous summary again to get metric another level up (per_year &lt;- summarise(per_month, flights = sum(flights))) ## # A tibble: 1 x 2 ## year flights ## &lt;int&gt; &lt;int&gt; ## 1 2013 336776 # ungroup the data daily %&gt;% ungroup() %&gt;% # no longer grouped by date summarise(flights = n()) # all flights ## # A tibble: 1 x 1 ## flights ## &lt;int&gt; ## 1 336776 5.6.7 Exercises 1. Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios: A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. To examine how flights behave, I would group by the flight number (flight), then perform analyses on the grouped flights. When looking at the data, there are some flights that are always 15 minutes late! However, these flights had less than 20 total data points. Filtering out the flights with less than 20 total points, we can see that flights with less data points tend to have a higher proportion of delays, whereas flights with many data points have an overall lower proportion of delayed flights. The proportion of flights that are early 15 minutes also follows a similar trend, with flights that have flown fewer times tending to have larger proportion of those flights 15 minutes early. by_flight &lt;- not_cancelled %&gt;% group_by(flight) %&gt;% summarise( late_15 = mean(dep_delay &gt;= 15), early_15 = mean(dep_delay &lt;= -15), n = n() ) %&gt;% arrange(desc(late_15)) by_flight ## # A tibble: 3,835 x 4 ## flight late_15 early_15 n ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 106 1 0 1 ## 2 974 1 0 1 ## 3 1084 1 0 3 ## 4 1226 1 0 1 ## 5 1320 1 0 1 ## 6 1510 1 0 1 ## 7 1514 1 0 1 ## 8 1760 1 0 2 ## 9 1859 1 0 1 ## 10 1868 1 0 2 ## # … with 3,825 more rows # filter out flights with less than 20 total flights, then plot delays vs total counts ggplot(filter(by_flight, n &gt; 20), aes (x = n, y = late_15)) + geom_point( aes(alpha = 1/5)) # filter out flights with less than 20 total flights, then plot delays vs total counts ggplot(filter(by_flight, n &gt; 20), aes (x = n, y = early_15)) + geom_point( aes(alpha = 1/5)) # find the flights that are either 15 minutes late with proportion 0.5 (no results) filter (by_flight, late_15 == 0.5, early_15 == 0.5) ## # A tibble: 0 x 4 ## # … with 4 variables: flight &lt;int&gt;, late_15 &lt;dbl&gt;, early_15 &lt;dbl&gt;, n &lt;int&gt; A flight is always 10 minutes late. The code below will give the flight numbers of all the flights that have been at least 10 minutes late 100% of the time. We can see that the number of counts is low for all of the flights returned. by_flight &lt;- not_cancelled %&gt;% group_by(flight) %&gt;% summarise( late_10 = mean(dep_delay &gt; 10), n = n() ) %&gt;% filter(late_10 == 1) by_flight ## # A tibble: 93 x 3 ## flight late_10 n ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 106 1 1 ## 2 896 1 1 ## 3 974 1 1 ## 4 1084 1 3 ## 5 1226 1 1 ## 6 1320 1 1 ## 7 1475 1 3 ## 8 1510 1 1 ## 9 1514 1 1 ## 10 1760 1 2 ## # … with 83 more rows A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. This can be solved by using the same code from the first bullet point, except changing the parameters to 30 minutes instead of 15. 99% of the time a flight is on time. 1% of the time it’s 2 hours late. Below is code that finds flights that are late at least 2 hours exactly 1% of the time. by_flight &lt;- not_cancelled %&gt;% group_by(flight) %&gt;% summarise( late_120 = mean(dep_delay &gt;= 120), n = n() ) %&gt;% filter(late_120 == 0.01) by_flight ## # A tibble: 2 x 3 ## flight late_120 n ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1153 0.01 100 ## 2 3361 0.01 200 Which is more important: arrival delay or departure delay? In my opinion, time of arrival of the flight is more important than departure delay, since you will probably have planned an itinerary that has a next step that depends on the time of arrival rather than the time of departure. 2. Come up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()). # same output as: not_cancelled %&gt;% count(dest) not_cancelled %&gt;% group_by(dest) %&gt;% summarize( n = n() ) ## # A tibble: 104 x 2 ## dest n ## &lt;chr&gt; &lt;int&gt; ## 1 ABQ 254 ## 2 ACK 264 ## 3 ALB 418 ## 4 ANC 8 ## 5 ATL 16837 ## 6 AUS 2411 ## 7 AVL 261 ## 8 BDL 412 ## 9 BGR 358 ## 10 BHM 269 ## # … with 94 more rows # same output as: not_cancelled %&gt;% count(tailnum, wt = distance) not_cancelled %&gt;% group_by(tailnum) %&gt;% summarize( n = sum(distance) ) ## # A tibble: 4,037 x 2 ## tailnum n ## &lt;chr&gt; &lt;dbl&gt; ## 1 D942DN 3418 ## 2 N0EGMQ 239143 ## 3 N10156 109664 ## 4 N102UW 25722 ## 5 N103US 24619 ## 6 N104UW 24616 ## 7 N10575 139903 ## 8 N105UW 23618 ## 9 N107US 21677 ## 10 N108UW 32070 ## # … with 4,027 more rows 3. Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column? If a flight still took place and there was an error in entering the dep_delay or arr_delay, we would have thrown out the flight. A more important column to look at may be air_time - a flight cannot have had air_time if it never flew. 4. Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay? To answer the first part of the question, this code returns a table with the number of cancelled flights per day. Plotting the data as a time series, we can see that there is some periodic trend in which spikes of large numbers of cancelled flights occur. # find number of cancelled flights per day cancelled_flights &lt;- filter (flights, is.na(air_time)) (per_day_cancelled &lt;- cancelled_flights %&gt;% group_by(year, month, day) %&gt;% summarize(n = n())) ## # A tibble: 363 x 4 ## # Groups: year, month [?] ## year month day n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 11 ## 2 2013 1 2 15 ## 3 2013 1 3 14 ## 4 2013 1 4 7 ## 5 2013 1 5 3 ## 6 2013 1 6 3 ## 7 2013 1 7 3 ## 8 2013 1 8 7 ## 9 2013 1 9 9 ## 10 2013 1 10 3 ## # … with 353 more rows ggplot (per_day_cancelled)+ geom_boxplot(aes (x = day, y = n, group = day)) ggplot (per_day_cancelled)+ geom_line(aes(x = c(1:length(per_day_cancelled$n)), y = n)) To find the proportion of cancelled flights per day, first group flights by year,month,day using group_by(), then for each day, count the total number of flights and number of cancelled flights using summarize(), and then use mutate() to calculate the proportion by dividing the number of cancelled flights by the total flights. Based on the graph of average_delay vs proportion_cancelled, we can see that there is a positive correlation in which days that have overall higher delays also have overall higher proportion of cancelled flights. #proportion of cancelled flights per day (proportion_cancelled &lt;- flights %&gt;% group_by(year,month,day) %&gt;% summarize ( average_delay = mean(dep_delay, na.rm = T), num_cancelled = sum (is.na(air_time)), total_flights = n() ) %&gt;% mutate ( prop_cancelled = num_cancelled/total_flights)) ## # A tibble: 365 x 7 ## # Groups: year, month [12] ## year month day average_delay num_cancelled total_flights ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 11.5 11 842 ## 2 2013 1 2 13.9 15 943 ## 3 2013 1 3 11.0 14 914 ## 4 2013 1 4 8.95 7 915 ## 5 2013 1 5 5.73 3 720 ## 6 2013 1 6 7.15 3 832 ## 7 2013 1 7 5.42 3 933 ## 8 2013 1 8 2.55 7 899 ## 9 2013 1 9 2.28 9 902 ## 10 2013 1 10 2.84 3 932 ## # … with 355 more rows, and 1 more variable: prop_cancelled &lt;dbl&gt; ggplot(proportion_cancelled, aes (average_delay, prop_cancelled))+ geom_point() 5. Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %&gt;% group_by(carrier, dest) %&gt;% summarise(n())) The top 5 carriers with the worst delays are: F9, EV, YV&lt; FL, and WN. If we continue to subset by airport location (see graph), we find that some carriers have higher delays on average without depending on destination, whereas some carriers which fly only to certain destinations have a large dep_delay. One such example is FL, which we see only flies to 3 destinations and has a high dep_delay on average. # find carrier with worst delay by_carrier &lt;- flights %&gt;% group_by(carrier) %&gt;% summarize ( avg_delay = mean(dep_delay, na.rm = T) ) %&gt;% arrange(desc(avg_delay)) by_carrier ## # A tibble: 16 x 2 ## carrier avg_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 F9 20.2 ## 2 EV 20.0 ## 3 YV 19.0 ## 4 FL 18.7 ## 5 WN 17.7 ## 6 9E 16.7 ## 7 B6 13.0 ## 8 VX 12.9 ## 9 OO 12.6 ## 10 UA 12.1 ## 11 MQ 10.6 ## 12 DL 9.26 ## 13 AA 8.59 ## 14 AS 5.80 ## 15 HA 4.90 ## 16 US 3.78 by_carrier_dest &lt;- flights %&gt;% group_by(carrier, dest) %&gt;% summarize ( avg_delay = mean(dep_delay, na.rm = T), n = n() ) %&gt;% arrange(desc(avg_delay)) by_carrier_dest ## # A tibble: 314 x 4 ## # Groups: carrier [16] ## carrier dest avg_delay n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 UA STL 77.5 2 ## 2 OO ORD 67 1 ## 3 OO DTW 61 2 ## 4 UA RDU 60 1 ## 5 EV PBI 48.7 6 ## 6 EV TYS 41.8 323 ## 7 EV CAE 36.7 113 ## 8 EV TUL 34.9 315 ## 9 9E BGR 34 1 ## 10 WN MSY 33.4 298 ## # … with 304 more rows # &quot;stripchart&quot; of average delay grouped by carrier, each point is a destination ggplot(by_carrier_dest) + geom_point (aes(x = carrier, y = avg_delay, color = dest, size = n, alpha = 1/5), position = &#39;jitter&#39;) ## Warning: Removed 1 rows containing missing values (geom_point). 6. What does the sort argument to count() do. When might you use it? If you set sort = TRUE in count(), it will return the output sorted in descending order of n. This might be useful when trying to find the item with the most occurances when counting, and will save you from having to pipe the data into arrange(). 5.7 Notes - Grouped mutates (and filters) You can use the group_by() function for purposes other than piping into summarize(). For example, you can use group_by() with filter() to find which items within a group satisfy a certain condition. popular_dests &lt;- flights %&gt;% group_by(dest) %&gt;% filter(n() &gt; 365) popular_dests ## # A tibble: 332,577 x 19 ## # Groups: dest [77] ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 332,567 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # out of 105 destinations, 77 have had greater than 365 flights length(unique(flights$dest)) ## [1] 105 length(unique(popular_dests$dest)) ## [1] 77 This resulting filtered data frame can be further analyzed for per-group metrics. popular_dests %&gt;% filter(arr_delay &gt; 0) %&gt;% mutate(prop_delay = arr_delay / sum(arr_delay)) %&gt;% select(year:day, dest, arr_delay, prop_delay) ## # A tibble: 131,106 x 6 ## # Groups: dest [77] ## year month day dest arr_delay prop_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 IAH 11 0.000111 ## 2 2013 1 1 IAH 20 0.000201 ## 3 2013 1 1 MIA 33 0.000235 ## 4 2013 1 1 ORD 12 0.0000424 ## 5 2013 1 1 FLL 19 0.0000938 ## 6 2013 1 1 ORD 8 0.0000283 ## 7 2013 1 1 LAX 7 0.0000344 ## 8 2013 1 1 DFW 31 0.000282 ## 9 2013 1 1 ATL 12 0.0000400 ## 10 2013 1 1 DTW 16 0.000116 ## # … with 131,096 more rows # not sure if this divides the group sum, or the total sum of all flights 5.7.1 Exercises 1. Refer back to the lists of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping. For filter(), the conditional statement in the filter() function will be applied separately to each subgroup, and all the observations corresponding to the subgroups that are TRUE will be returned. When combined with grouping, mutate seems to be more tricky. The functions do not seem to change if a grouped or ungrouped input is used. It seems that the mutate() would be useful if used on a grouped, filtered table. flights %&gt;% group_by(year,month,day) %&gt;% mutate( dep_time, hour = dep_time %/% 100, log_air_time = log(air_time) ) ## # A tibble: 336,776 x 20 ## # Groups: year, month, day [365] ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, log_air_time &lt;dbl&gt; flights %&gt;% mutate( dep_time, hour = dep_time %/% 100, log_air_time = log(air_time) ) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, log_air_time &lt;dbl&gt; 2. Which plane (tailnum) has the worst on-time record? There are over 100 planes which have a 100% record of being delayed, either on departure or arrival. # by departure delay flights %&gt;% group_by(tailnum) %&gt;% summarize( delay_count = sum (dep_delay &gt; 0), delay_proportion = mean (dep_delay&gt;0) ) %&gt;% arrange (desc(delay_proportion)) ## # A tibble: 4,044 x 3 ## tailnum delay_count delay_proportion ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 N136DL 1 1 ## 2 N206UA 1 1 ## 3 N228UA 1 1 ## 4 N245AY 2 1 ## 5 N26906 1 1 ## 6 N302AS 1 1 ## 7 N303AS 1 1 ## 8 N305AS 2 1 ## 9 N319AS 3 1 ## 10 N32626 1 1 ## # … with 4,034 more rows # by arrival delay flights %&gt;% group_by(tailnum) %&gt;% summarize( arr_count = sum (arr_delay &gt; 0), arr_proportion = mean (arr_delay&gt;0) ) %&gt;% arrange (desc(arr_proportion)) ## # A tibble: 4,044 x 3 ## tailnum arr_count arr_proportion ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 N121DE 2 1 ## 2 N136DL 1 1 ## 3 N143DA 1 1 ## 4 N17627 2 1 ## 5 N240AT 5 1 ## 6 N26906 1 1 ## 7 N295AT 4 1 ## 8 N302AS 1 1 ## 9 N303AS 1 1 ## 10 N32626 1 1 ## # … with 4,034 more rows 3. What time of day should you fly if you want to avoid delays as much as possible? To figure this out, we would want to examine the distribution of delays over time to see where the delays are minimized. Plotting time on the x-axis and the mean dep_delay per unit time on the y axis, we can see that from time 500-1000 there are on average less delays than during other times. flights %&gt;% group_by(dep_time) %&gt;% summarize( mean_delay = mean (dep_delay, na.rm = T), n = n() ) %&gt;% arrange (mean_delay) %&gt;% ggplot ( aes (dep_time, mean_delay) )+ geom_point(aes(size = n, alpha = 1/5)) ## Warning: Removed 1 rows containing missing values (geom_point). 4. For each destination, compute the total minutes of delay. For each, flight, compute the proportion of the total delay for its destination. # calculate total delay time of delayed flights using summarize() flights %&gt;% filter(dep_delay &gt;0) %&gt;% group_by(dest) %&gt;% summarize( total_delay = sum (dep_delay, na.rm = T) ) ## # A tibble: 103 x 2 ## dest total_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 ABQ 4076 ## 2 ACK 2603 ## 3 ALB 10934 ## 4 ANC 105 ## 5 ATL 254414 ## 6 AUS 36623 ## 7 AVL 3092 ## 8 BDL 8471 ## 9 BGR 8170 ## 10 BHM 8817 ## # … with 93 more rows # calculate total delay time of delayed flights using weighted counts() flights %&gt;% filter(dep_delay &gt;0) %&gt;% count(dest, wt = dep_delay) ## # A tibble: 103 x 2 ## dest n ## &lt;chr&gt; &lt;dbl&gt; ## 1 ABQ 4076 ## 2 ACK 2603 ## 3 ALB 10934 ## 4 ANC 105 ## 5 ATL 254414 ## 6 AUS 36623 ## 7 AVL 3092 ## 8 BDL 8471 ## 9 BGR 8170 ## 10 BHM 8817 ## # … with 93 more rows # first filter for all flights that are delayed, then group by destination # then calculate dep_delay for flight to destination / sum of all delayed flights within the same destination # select the rows you want, then arrange based on alphabetical order flights %&gt;% filter(dep_delay &gt;0) %&gt;% group_by(dest) %&gt;% mutate (prop_delay = dep_delay / sum (dep_delay)) %&gt;% select (dest, flight, dep_delay, prop_delay) %&gt;% arrange (dest) ## # A tibble: 128,432 x 4 ## # Groups: dest [103] ## dest flight dep_delay prop_delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ 65 9 0.00221 ## 2 ABQ 65 16 0.00393 ## 3 ABQ 65 1 0.000245 ## 4 ABQ 65 10 0.00245 ## 5 ABQ 65 53 0.0130 ## 6 ABQ 65 105 0.0258 ## 7 ABQ 65 14 0.00343 ## 8 ABQ 65 18 0.00442 ## 9 ABQ 65 3 0.000736 ## 10 ABQ 65 17 0.00417 ## # … with 128,422 more rows 5. Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() explore how the delay of a flight is related to the delay of the immediately preceding flight. I first filter for flights that are delayed. The flights are already ordered by dep_tiem. I then use mutate() and lag() to bind a new column to the data that shows what the previous delay was for that observation. Then, I plot the current delay vs. the previous delay in a scatterplot. Since there are so many points, it is hard to see the pattern. So, I added a linear regression line to the points (OLS) to visualize any correlation that exists. There is a positive correlation, suggesting that, on average, flights that come immediately after a delayed flight tend to also be delayed. The slope is &lt; 1, suggesting that the delay is less than the previous delay. flights %&gt;% filter (dep_delay &gt;0) %&gt;% mutate ( prev_delay = lag(dep_delay)) %&gt;% ggplot ( aes (x = prev_delay, y = dep_delay))+ geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). 6. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? Flights that are suspiciously fast will have an air_time value that is very small compared to the expected amount of air_time (sched_arr_time - sched_dep_time). To find these flights, first group flights by dest, use mutate() to calculate the expected air_time, and calculate the proportion of the amount of time saved during the flight, (expected_air_time - air_time)/expected_air_time. Use arrange() to sort the flights based on prop_time_saved. If we expected the flight to take two hours but the flight had an air_time of 20 minutes, these flights would show up at the top of each group. We see that there are suspiciously fast flights such as flight 4117 to ALB which took 26 minutes, but was expected to take 72 minutes, saving 63% of the expected flight time. Also, flight 4013 to PHL took 23 minutes but was expected ot take 93. This is suspicious! # calculate the expected air time by converting sched_arr_time and sched_dep_time to minutes, then subtracting. # if the sched_arr_time was past midnight, we have to add 2400 to the value before subtracting. # to fix this, I use ifelse() within the mutate function. expected_times &lt;- not_cancelled %&gt;% mutate (expected_air_time = ifelse(sched_arr_time &lt; sched_dep_time, ((((sched_arr_time+2400) %/% 100)*60 + (sched_arr_time+2400) %% 100) - ((sched_dep_time %/% 100)*60 + sched_dep_time %% 100)), (((sched_arr_time %/% 100)*60 + sched_arr_time %% 100) - ((sched_dep_time %/% 100)*60 + sched_dep_time %% 100)))) # use grouped arrange() to find suspicous flights with large prop_time_saved for each destination. expected_times %&gt;% group_by(dest) %&gt;% mutate( prop_time_saved = (expected_air_time - air_time)/expected_air_time ) %&gt;% arrange(desc(prop_time_saved)) %&gt;% slice (1:5) %&gt;% # select top 5 fastest flights select (dest, flight, sched_dep_time, sched_arr_time, air_time, expected_air_time, prop_time_saved) ## # A tibble: 516 x 7 ## # Groups: dest [104] ## dest flight sched_dep_time sched_arr_time air_time expected_air_ti… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ 1505 2001 2308 224 187 ## 2 ABQ 1505 2001 2308 230 187 ## 3 ABQ 1505 2001 2308 230 187 ## 4 ABQ 1505 2001 2308 230 187 ## 5 ABQ 1505 2001 2308 230 187 ## 6 ACK 1491 800 909 35 69 ## 7 ACK 1491 800 909 35 69 ## 8 ACK 1491 800 909 35 69 ## 9 ACK 1191 1209 1316 35 67 ## 10 ACK 1195 800 913 39 73 ## # … with 506 more rows, and 1 more variable: prop_time_saved &lt;dbl&gt; To compute air time to a flight relative to shortest flight to the destination, I would first take a ratio of the air_time to distance, find out what this value is for the shortest flight (smallest distance), then compare it with the suspicious flights. To find the flights most delayed in the air, subtract arr_delay from dep_delay and compare these values. # compare distance traveled over time not_cancelled %&gt;% group_by(dest) %&gt;% mutate ( dist_over_time = distance/air_time, shortest_dest_flight = min (air_time, na.rm = T)) %&gt;% arrange (desc(dist_over_time)) %&gt;% slice (1:5) %&gt;% select( dest, flight, distance, air_time, dist_over_time, shortest_dest_flight) ## # A tibble: 516 x 6 ## # Groups: dest [104] ## dest flight distance air_time dist_over_time shortest_dest_flight ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ 1505 1826 212 8.61 212 ## 2 ABQ 1505 1826 213 8.57 212 ## 3 ABQ 1505 1826 213 8.57 212 ## 4 ABQ 1505 1826 217 8.41 212 ## 5 ABQ 1505 1826 220 8.3 212 ## 6 ACK 1491 199 35 5.69 35 ## 7 ACK 1191 199 35 5.69 35 ## 8 ACK 1491 199 35 5.69 35 ## 9 ACK 1191 199 35 5.69 35 ## 10 ACK 1491 199 35 5.69 35 ## # … with 506 more rows # top 5 flights for each destination that are delayed the most in the air (in_air_delays &lt;- not_cancelled %&gt;% group_by(dest) %&gt;% mutate (in_air_delay = arr_delay - dep_delay) %&gt;% arrange (desc(in_air_delay)) %&gt;% slice (1:5) %&gt;% select( dest, flight, arr_delay, dep_delay, in_air_delay)) ## # A tibble: 516 x 5 ## # Groups: dest [104] ## dest flight arr_delay dep_delay in_air_delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ 1505 126 18 108 ## 2 ABQ 1505 103 0 103 ## 3 ABQ 1505 117 31 86 ## 4 ABQ 1505 72 0 72 ## 5 ABQ 1505 67 0 67 ## 6 ACK 1491 86 -5 91 ## 7 ACK 1191 150 71 79 ## 8 ACK 1191 39 -1 40 ## 9 ACK 1191 24 -9 33 ## 10 ACK 1491 52 26 26 ## # … with 506 more rows ggplot(in_air_delays) + geom_point (aes(x = dest, y = in_air_delay), position = &#39;jitter&#39;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) 7. Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. The code below will find the destinations that are flown by at least 2 carriers. The second part of the question is vague–how can carriers be ranked by whether a destination is flown to by 2 more more carriers? It would make more sense to rank the destinations. Below I rank the carriers by the number of destinations they fly to. # destinations that are flown by at least two carriers not_cancelled %&gt;% group_by(dest) %&gt;% filter( length(unique(carrier)) &gt;= 2) ## # A tibble: 315,946 x 19 ## # Groups: dest [75] ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 315,936 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # get the same output using the n_distinct() function instead of length(unique()) not_cancelled %&gt;% group_by(dest) %&gt;% filter( n_distinct(carrier) &gt;= 2) ## # A tibble: 315,946 x 19 ## # Groups: dest [75] ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 315,936 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # carriers that fly to the most destinations: not_cancelled %&gt;% group_by(carrier) %&gt;% summarize( num_dest = length (unique(dest)) ) %&gt;% arrange (desc(num_dest)) ## # A tibble: 16 x 2 ## carrier num_dest ## &lt;chr&gt; &lt;int&gt; ## 1 EV 61 ## 2 9E 48 ## 3 UA 47 ## 4 B6 42 ## 5 DL 40 ## 6 MQ 20 ## 7 AA 19 ## 8 WN 11 ## 9 OO 5 ## 10 US 5 ## 11 VX 5 ## 12 FL 3 ## 13 YV 3 ## 14 AS 1 ## 15 F9 1 ## 16 HA 1 # checking to see if length(unique(dest)) worked by selecting one of the flights and manually counting EV_flights &lt;- not_cancelled %&gt;% filter ( carrier == &#39;EV&#39;) length(unique(EV_flights$dest)) ## [1] 61 # we can also group sequentially, if that&#39;s what the question is suggesting not_cancelled %&gt;% group_by(dest) %&gt;% filter(n_distinct(carrier) &gt;= 2) %&gt;% # filter for flights to destinations that have more than 2 carriers group_by(carrier) %&gt;% # group by carriers in this filtered dataset summarize(num_dest = n_distinct(dest)) %&gt;% # find the number of distinct destinations they fly to arrange(desc(num_dest)) # sort the output ## # A tibble: 16 x 2 ## carrier num_dest ## &lt;chr&gt; &lt;int&gt; ## 1 EV 50 ## 2 9E 47 ## 3 UA 42 ## 4 DL 39 ## 5 B6 35 ## 6 AA 19 ## 7 MQ 19 ## 8 WN 10 ## 9 OO 5 ## 10 US 5 ## 11 VX 4 ## 12 YV 3 ## 13 FL 2 ## 14 AS 1 ## 15 F9 1 ## 16 HA 1 8. For each plane, count the number of flights before the first delay of greater than 1 hour. I would first group by flight, and then select the minimum sched_dep_time for flights that were delayed greater than 60 minutes. Then I would count the number of flights that are less than the minimum sched_dep_time in each group of flights. I used full_join() to map the first flight with delay greater than 1 hour to the original flights table, then used summarize() to count the number of flights that came before this for each group within tailnum. # finds the first occurance of a delay greater than 60 min min_delay &lt;- flights %&gt;% group_by(tailnum) %&gt;% filter (dep_delay &gt; 60) %&gt;% summarize( min_sched_dep = min (sched_dep_time) ) # join the dep_time of the first occurance of the &gt;60min delay to the original table (joined_delay &lt;- full_join (flights,min_delay, by = &quot;tailnum&quot;) %&gt;% arrange (tailnum)) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 2 11 1508 1400 68 1807 ## 2 2013 3 23 1340 1300 40 1638 ## 3 2013 3 24 859 835 24 1142 ## 4 2013 7 5 1253 1259 -6 1518 ## 5 2013 1 1 1604 1510 54 1817 ## 6 2013 1 1 2100 2100 0 2307 ## 7 2013 1 2 827 835 -8 1059 ## 8 2013 1 2 2014 2020 -6 2256 ## 9 2013 1 4 1621 1625 -4 1853 ## 10 2013 1 5 834 835 -1 1050 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, min_sched_dep &lt;dbl&gt; # use summarize() to figure out how many values came before the first delay &gt; 60min (see column num_before_delay60). joined_delay %&gt;% group_by(tailnum) %&gt;% summarize( num_before_delay60 = sum ( sched_dep_time &lt; min_sched_dep, na.rm = T) ) ## # A tibble: 4,044 x 2 ## tailnum num_before_delay60 ## &lt;chr&gt; &lt;int&gt; ## 1 D942DN 3 ## 2 N0EGMQ 13 ## 3 N10156 15 ## 4 N102UW 17 ## 5 N103US 0 ## 6 N104UW 28 ## 7 N10575 20 ## 8 N105UW 22 ## 9 N107US 33 ## 10 N108UW 32 ## # … with 4,034 more rows "],
["chapter-6-workflow-scripts.html", "Chapter 6 - Workflow: scripts 6.3 Exercises", " Chapter 6 - Workflow: scripts 6.3 Exercises 1. Go to the RStudio Tips twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it! RStudio Tips retweeted on Sep 21, 2018 that View(function) will show a functions source code. You can then sift through the function methods using the dropdown menu. This is a nice way to examine the inner workings of a function that is part of a package you are using. If you would like to alter the function to work differently, you can use the trace(function, edit = T). This will bring up a pop up with the code. You can then navigate to the line(s) you want to change, make your changes, and save them! This will alter the function for only your current R session. Restarting R will bring the funcion back to default. 2. What other common mistakes will RStudio diagnostics report? Read https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics to find out. Rstudio diagnostics has many helpful warnings that will: check arguments to function calls: is an argument missing, with no default? are commas missing between arguments? warn if variable used has no definition in scope warn if variable is defined but not used provide R style diagnostics provide diagnostics for other languages such as C/C++ and JavaScript "],
["chapter-7-exploratory-data-analysis.html", "Chapter 7 - Exploratory Data Analysis 7.3.1 Notes - Visualizing distributions 7.3.4 Exercises 7.4 Notes - Missing Values 7.4.1 Exercises 7.5 Notes - Covariation 7.5.1 Notes - A categorical and continuous variable 7.5.1.1 Exercises 7.5.2 Notes - Two categorical variables 7.5.2.1 Exercises 7.5.3 Notes - Two continuous variables 7.5.3.1 Exercises 7.6 Notes - Patterns and models", " Chapter 7 - Exploratory Data Analysis Exploratory data analysis (EDA) is something I was first introduced to while I was an undergraduate Statistics major at UC Berkeley. The description that this book provides is very similar to what I was taught, in that there is no “correct” or defined way to perform EDA. Exploring the data and generating questions and insights about how the data is structured, the quality of the data, what types of analysis/modeling can be performed using the data, and interpreting results of your analysis are all important things to be thinking about. While EDA can be performed in a variety of ways, this text focuses on using the tidyverse packages to do so. library(tidyverse) 7.3.1 Notes - Visualizing distributions Categorical variables can be visualized using a bar chart (how many observations have property ‘x’?): # discrete binning ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) # manual version: diamonds %&gt;% count(cut) ## # A tibble: 5 x 2 ## cut n ## &lt;ord&gt; &lt;int&gt; ## 1 Fair 1610 ## 2 Good 4906 ## 3 Very Good 12082 ## 4 Premium 13791 ## 5 Ideal 21551 Continuous variables can also be visualized using a bar chart (how many observations have values between 1.5-2?): # binning continuous variable ggplot(data = diamonds) + geom_histogram(mapping = aes(x = carat), binwidth = 0.5) # manual version: diamonds %&gt;% count(cut_width(carat, 0.5)) ## # A tibble: 11 x 2 ## `cut_width(carat, 0.5)` n ## &lt;fct&gt; &lt;int&gt; ## 1 [-0.25,0.25] 785 ## 2 (0.25,0.75] 29498 ## 3 (0.75,1.25] 15977 ## 4 (1.25,1.75] 5313 ## 5 (1.75,2.25] 2002 ## 6 (2.25,2.75] 322 ## 7 (2.75,3.25] 32 ## 8 (3.25,3.75] 5 ## 9 (3.75,4.25] 4 ## 10 (4.25,4.75] 1 ## 11 (4.75,5.25] 1 You can change the size of the bins in a histogram by modifying binwidth: # filter dataset to only keep observations with carat &lt; 3 smaller &lt;- diamonds %&gt;% filter(carat &lt; 3) # plot histogram using smaller bin width ggplot(data = smaller, mapping = aes(x = carat)) + geom_histogram(binwidth = 0.1) You can also overlay histogram-like data using geom_freqpoly(). Here, the aesthetic mapping is further subgrouped by cut, using color. Each line shows up as a different color corresponding to the type of cut. We can see that the majority of ideal cuts have lower carats. ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) + geom_freqpoly(binwidth = 0.1) Some of the common questions you can ask based on this type of data are: Which values are the most common? Why? Which values are rare? Why? Does that match your expectations? Can you see any unusual patterns? What might explain them? If there are clusters visible in your data, such as in the dataset, faithful: # example of histogram with 2 clusters ggplot(data = faithful, mapping = aes(x = eruptions)) + geom_histogram(binwidth = 0.25) You might want to ask the following questions about the clusters: How are the observations within each cluster similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading? Sometimes histograms will reveal outliers or other unusual values. For example, why are the majority of values on the lefthand side of this plot? ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5) The histogram will plot all values, so if there are 1 or 2 values ith very high ‘y’ in a dataset with thousands of observations, these will still get plotted and not be immediately visible. A way to see them is to use coor_cartesian(), as shown in the book. ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5) + coord_cartesian(ylim = c(0, 50)) We can see now that there are some outlier values with very high ‘y’ values, which are the cause of the funny-looking histogram. You can pull out unusual values or outliers using dplyr commands (from chapter 4-6), to figure out why they are deviating from the rest of the data. unusual &lt;- diamonds %&gt;% filter(y &lt; 3 | y &gt; 20) %&gt;% select(price, x, y, z) %&gt;% arrange(y) unusual ## # A tibble: 9 x 4 ## price x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5139 0 0 0 ## 2 6381 0 0 0 ## 3 12800 0 0 0 ## 4 15686 0 0 0 ## 5 18034 0 0 0 ## 6 2130 0 0 0 ## 7 2130 0 0 0 ## 8 2075 5.15 31.8 5.12 ## 9 12210 8.09 58.9 8.06 7.3.4 Exercises 1. Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. To explore the distributions of x, y, and z, we can plot a histogram for each of them. Alternatively, we can plot a boxplot, which also shows the distribution of values for each of the variables, as well as any outliers. We can see that for each of the values, there are indeed a few outliers that have very high or low values. However, for the most part, x and y are between 4-9, and z is between 2 and 6. We can also plot a scatterplot of x vs y or z and fit a linear model to the points. We can see that there is a strong positive correlation between x and y, and x and z. # plot a histogram for x, y, and z separately ggplot(diamonds, aes (x = x))+ geom_histogram(binwidth = 0.2) ggplot(diamonds, aes (x = y))+ geom_histogram(binwidth = 0.2) ggplot(diamonds, aes (x = z))+ geom_histogram(binwidth = 0.2) # or, reshape the data and plot a boxplot of the three variables side by side library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths x_y_z_melt = melt( select(diamonds, x,y,z), measure.vars = c(&#39;x&#39;,&#39;y&#39;,&#39;z&#39;)) ggplot(x_y_z_melt)+ geom_boxplot(aes(x = variable, y = value)) # get a numeric summary for all the columns in diamonds summary(diamonds) ## carat cut color clarity ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 ## Max. :5.0100 I: 5422 VVS1 : 3655 ## J: 2808 (Other): 2531 ## depth table price x ## Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 ## 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 ## Median :61.80 Median :57.00 Median : 2401 Median : 5.700 ## Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 ## 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 ## Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 ## ## y z ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 4.720 1st Qu.: 2.910 ## Median : 5.710 Median : 3.530 ## Mean : 5.735 Mean : 3.539 ## 3rd Qu.: 6.540 3rd Qu.: 4.040 ## Max. :58.900 Max. :31.800 ## # see how x correlates with y or z ggplot (diamonds, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) ggplot (diamonds, aes(x = x, y = z)) + geom_point()+ geom_smooth(method = &#39;lm&#39;, se = F) 2. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.) The distribution of price is skewed to the right (the tail is on the right side). The majority of prices are low, but there are a small amount of very high priced gems. The usual feature revealed by using smaller and smaller binwidth is that there seems to be an absence of diamonds with price 1500 +/- 50 (gap in the histogram). #examine the distribution of price using various bin widths ggplot (diamonds, aes (x = price))+ geom_histogram(binwidth = 10) ggplot (diamonds, aes (x = price))+ geom_histogram(binwidth = 50) ggplot (diamonds, aes (x = price))+ geom_histogram(binwidth = 500) # zoom in on diamonds which are less than 2500 to examine the anomaly ggplot(filter(diamonds, price &lt; 2500), aes (x = price))+ geom_histogram(binwidth = 5) 3. How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? # use dplyr to filter data by diamonds with either 0.99 or 1 carat, then count each group diamonds %&gt;% filter (carat == 0.99 | carat == 1) %&gt;% count (group_by = carat) ## # A tibble: 2 x 2 ## group_by n ## &lt;dbl&gt; &lt;int&gt; ## 1 0.99 23 ## 2 1 1558 We see that there are 23 observations with 0.99 carat, and 1558 observations with 1 carat. This is a huge difference. There could be many explanations. Maybe diamonds of at least 1 carat can be sold at a higher price range, incentivizing the production of diamonds at least 1 carat and no less. Or, it could be due to an unconcious human tendancy to round to the nearest whole number. 4. Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows? When using ylim() to zoom in with the same parameters as coord_cartesian, the values for bars that do not completely fall into view are omitted! As a result, limiting the y axis to between 0 and 50 removes almost the entire dataset! Leaving binwidth unset results in geom_histogram() choosing a default binwidth. If you try and zoom so only half a bar shows, the bar will be omitted entirely but the rest of the bins will not change (there will be a gap). # coord_cartesian() to zoom (book provided example) ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5) + coord_cartesian(ylim = c(0, 50)) # use xlim() and ylim() to zoom ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5)+ ylim(0,50) ## Warning: Removed 11 rows containing missing values (geom_bar). #Use xlim and ylim, leave binwidth unset ggplot(diamonds) + geom_histogram(mapping = aes(x = y))+ ylim(0,10000)+ xlim(0,10) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 5 rows containing non-finite values (stat_bin). ## Warning: Removed 2 rows containing missing values (geom_bar). # leave binwidth unset, cut ylim off in the middle of a bar (it dissapears!) ggplot(diamonds) + geom_histogram(mapping = aes(x = y))+ ylim(0,7500)+ xlim(0,10) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 5 rows containing non-finite values (stat_bin). ## Warning: Removed 3 rows containing missing values (geom_bar). 7.4 Notes - Missing Values If there are unusual values in your dataset, you can either filter them out using filter(), or replace the values with NA using mutate(). Below are the provided examples for both: # remove using filter() diamonds2 &lt;- diamonds %&gt;% filter(between(y, 3, 20)) # replace with NA using mutate and ifelse() diamonds2 &lt;- diamonds %&gt;% mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y)) I find that ifelse() is particularly useful, since you can use it to replace only a subset of values in any column of choice. For example, if we wanted to raise the price of all diamonds with a currrent price over 2000 by 3000, we could say: (raise_price &lt;- diamonds %&gt;% mutate(price = ifelse(price &gt; 2000, price+3000, price))) ## # A tibble: 53,940 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # … with 53,930 more rows Missing values may provide some insight into the data, even though they do not have values. For example, the presence of a missing value in a column such as dep_time in the nycflights13 dataset suggests the flight was cancelled. You can then compare how the other attributes of a cancelled flight differ from a non-cancelled flight. The example provided by the book compares the distribution of sched_dep_time for cancelled vs non-cancelled flights: nycflights13::flights %&gt;% mutate( cancelled = is.na(dep_time), sched_hour = sched_dep_time %/% 100, sched_min = sched_dep_time %% 100, sched_dep_time = sched_hour + sched_min / 60 ) %&gt;% ggplot(mapping = aes(sched_dep_time)) + geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4) 7.4.1 Exercises 1. What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference? both ?geom_bar and ?geom_histogram state that “na.rm - If FALSE, the default, missing values are removed with a warning. If TRUE, missing values are silently removed.” So, for both, missing values are omitted in either case. In terms of how this affects the plots, it looks like for histograms, there is a gap where the values used to be. For bar plots, the count is reduced for the category the NA value used to be in (geom_bar() is associated with the stat function count(), which counts the non-missing values for each category). This has significance for bar plots since if the majority of values in a certain category were missing values, the displayed counts would be low but the viewer would be unaware of the underlying reason as to why. # histogram of vector of random numbers, normally distributed x &lt;- data.frame(x = rnorm(100,5,3)) head(x) ## x ## 1 1.869367 ## 2 3.087430 ## 3 3.728269 ## 4 2.601697 ## 5 4.347069 ## 6 3.842517 ggplot(x, aes(x))+ geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # replace a chunk of values with NA and see what that does to the histogram. add_na &lt;- mutate(x, x = ifelse(x &gt; 5 &amp; x &lt;8, NA, x)) head(add_na) ## x ## 1 1.869367 ## 2 3.087430 ## 3 3.728269 ## 4 2.601697 ## 5 4.347069 ## 6 3.842517 ggplot(add_na, aes(x))+ geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 28 rows containing non-finite values (stat_bin). 2. What does na.rm = TRUE do in mean() and sum()? mean(), by default, has na.rm = FALSE, which does not remove the NA values prior to calculating the mean. It will try to calculate the mean including the NA value, which evaluates to NA. The same applies for sum(). If you set na.rm = TRUE for both functions, the NA values are removed prior to calculating, and a non-missing value is returned. # calculate the sum of vector without NA values, na.rm = FALSE by default (x &lt;- 1:10) ## [1] 1 2 3 4 5 6 7 8 9 10 mean(x) ## [1] 5.5 sum(x) ## [1] 55 # calculate the sum of vector containing NA values, na.rm = FALSE by default (x &lt;- append(x, c(NA,NA))) ## [1] 1 2 3 4 5 6 7 8 9 10 NA NA mean(x) ## [1] NA sum(x) ## [1] NA # calculate the sum of vector without NA values, setting na.rm = TRUE mean(x, na.rm = T) ## [1] 5.5 sum(x, na.rm = T) ## [1] 55 7.5 Notes - Covariation Covariation is how the values of two or more variables are related - in other words, is there a correlation between two or more variables, or columns (if the data is tidy), of your dataset? Knowing this is important, since it may affect the types of parameters you choose when building models (for example, multicollinearity issues when performing OLS). 7.5.1 Notes - A categorical and continuous variable Default for geom_freqpoly() is to plot the individual groups by count, so if any one group has a lot of observations, it might mask effects in groups that have small numbers of observations. Below, since ideal cuts have the majority of counts, it looks overrepresented in the freqpoly() graph. ggplot(data = diamonds, mapping = aes(x = price)) + geom_freqpoly(mapping = aes(colour = cut), binwidth = 500) ggplot(diamonds) + geom_bar(mapping = aes(x = cut)) To normalize all the counts and plot the distributions by density, use y= ..density.. ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + geom_freqpoly(mapping = aes(colour = cut), binwidth = 500) The book notes that there is something surprising about this plot. It shows that diamonds with an ideal cut cost on average lower than diamonds with a fair cut, which is initially counterintuitive. We would have expected that ideal cuts, because of their higher quality cut, cost more. What might be happening here is that there is a hidden 3rd variable that is related to cut and price. Perhaps ideal cuts are on average smaller in size, and prices of small diamonds tend to be small. Boxplots also give a great sense of the spread of the data. You can think of a boxplot as a histogram turned on its side and summarized by its median, and IQR. Here is the provided boxplot of the distribution of price by cut: ggplot(data = diamonds, mapping = aes(x = cut, y = price)) + geom_boxplot() We can see again that ideal diamonds have a lower median price than fair diamonds, as we learned from the individual freqpoly graphs. Another provided example is the boxplot of the mtcars dataset (mileage by car class): ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() A useful way to reorganize the categorical variables on the x-axis is to use reorder(). I think organized graphs are great! We can order the x axis based on a statistic, in this case the median: ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) We learned this in chapter 3, but a way to flip the axes is to add a coord_flip() function to your ggplot. This is useful when you have long axis labels. I prefer tilting the labels by 45 degrees by modifying the tilt using theme(axis.text.x = element_text(angle = 45, hjust = 1)). ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) + coord_flip() 7.5.1.1 Exercises 1. Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights. The provided geom_freqpoly() visualization for cancelled vs non-cancelled flights had the issue of there being many more non-cancelled flights than cancelled flights. To fix this, we will use y = ..density.. instead of the total count for the graph, so that we can better see the how the trends compare in each of the groups. nycflights13::flights %&gt;% mutate( cancelled = is.na(dep_time), sched_hour = sched_dep_time %/% 100, sched_min = sched_dep_time %% 100, sched_dep_time = sched_hour + sched_min / 60 ) %&gt;% ggplot(mapping = aes(sched_dep_time, y = ..density..)) + geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4) From the overlaid &amp; normalized graphs, we observe that more cancellations occur for flights scheduled to depart between hours 15-22. 2. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive? To look for variables that are most useful for predicting the price of the diamond, we can use the cor() function to compute the correlation matrix for all numerical columns. It will show us what is the correlation between any pairwise combination of variables. cor(diamonds[,c(1,5:10)], use = &#39;all.obs&#39;) ## carat depth table price x y ## carat 1.00000000 0.02822431 0.1816175 0.9215913 0.97509423 0.95172220 ## depth 0.02822431 1.00000000 -0.2957785 -0.0106474 -0.02528925 -0.02934067 ## table 0.18161755 -0.29577852 1.0000000 0.1271339 0.19534428 0.18376015 ## price 0.92159130 -0.01064740 0.1271339 1.0000000 0.88443516 0.86542090 ## x 0.97509423 -0.02528925 0.1953443 0.8844352 1.00000000 0.97470148 ## y 0.95172220 -0.02934067 0.1837601 0.8654209 0.97470148 1.00000000 ## z 0.95338738 0.09492388 0.1509287 0.8612494 0.97077180 0.95200572 ## z ## carat 0.95338738 ## depth 0.09492388 ## table 0.15092869 ## price 0.86124944 ## x 0.97077180 ## y 0.95200572 ## z 1.00000000 We observe that the highest correlation with price is carat. To observe how carat is correlated with cut, we can plot a boxplot of carat vs cut: ggplot(diamonds, aes(x = reorder(cut, carat, FUN = median), y = carat))+ geom_boxplot() From this graph we see that ideal cuts have the lowest carat, whereas the fair cuts have the largest carat. By pairing this information with the positive correlation between carat and price, this suggests that ideal cuts on average cost less than fair cuts, because ideal cuts have, on average, lower carat. This explains what we saw in the geom_freqpoly() plot earlier in the chapter, in which we were puzzled as to why ideal cuts had a lower median price than fair cuts. It is because the ideal cut diamonds were smaller on average! 3. Install the ggstance package, and create a horizontal boxplot. How does this compare to using coord_flip()? I plotted the graph from part 2 using either coord_flip() or geom_boxploth() from the ggstance package. For a horizontal plot using geom_boxploth(), you have to specify y = {your categorical variable} instead of x. The plots look identical. # install ggstance using the commented command below # install.packages(&quot;ggstance&quot;) library(ggstance) ## ## Attaching package: &#39;ggstance&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## geom_errorbarh, GeomErrorbarh # horizontal boxplot from question #2, using coord_flip() ggplot(diamonds, aes(x = reorder(cut, carat, FUN = median), y = carat))+ geom_boxplot()+ coord_flip() # horizontal boxplot from question #2, using ggstance geom_boxploth() ggplot(diamonds, aes(x = carat, y = reorder(cut, carat, FUN = median)))+ geom_boxploth() 4. One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots? It seems like for each category, the plot is widest where most of the points are aggregated. So for the “fair” diamonds, we can see that the widest part of the plot is higher than the widest point of the plot for the “ideal” diamonds, which matches what is represented by the traditional boxplot. # install lvplot package - if you get a ggproto error, make sure to specify type = &quot;source&quot; when installing. # install.packages(&quot;lvplot&quot;, type = &quot;source&quot;) library(lvplot) ggplot(diamonds, aes(x = cut, y = price))+ geom_lv() 5. Compare and contrast geom_violin() with a facetted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method? Below I compare the three methods specified. The violin plot gives a fast look at where the majority of the points are aggregated. The faceted geom_histogram allows us to examine the raw distribution of values for each category more closely, but the axis scales can be frustrating if the total count differs a lot between groups. It might be better to plot a facetted density histogram instead. A colored geom_freqpoly() displays the overlapping distributions in line form. If too many distributions are plotted on the same graph, it might be hard to decipher the differences between each individual plot quickly. # geom_violin() ggplot(diamonds, aes(cut, price))+ geom_violin() # geom_histogram + facet_wrap() ggplot(diamonds)+ geom_histogram(aes(x = price), binwidth = 500)+ facet_wrap(~ cut) # geom_histogram + facet_wrap() ggplot(diamonds)+ geom_histogram(aes(x = price, y = ..density..), binwidth = 500)+ facet_wrap(~ cut) # geom_freqpoly() ggplot(diamonds, aes(x = price, y = ..density.., color = cut))+ geom_freqpoly(binwidth = 500) 6. If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does. The plots generated by geom_jitter() are very similar to the stripchart() from baseR. Here, performing geom_jitter() on the entire diamonds dataset is not very informative since there are too many datapoints flooding the plot. So, I take a smaller, random sample from the data and plot it using both geom_jitter() and stripchart(). We can see that the plots are very similar! Next, I use the geom_quasirandom() plot from the ggbeeswarm package to plot the same data. It produces a more organized plot; rather than random jitter, it groups the points together in a particular fashion (see plot). This makes comparing categories which have different numbers of observations slightly easier, and might reveal some sort of underlying distribution in the data. The method described in the ggbeeswarm github repo is geom_beeswarm(). This is similar to geom_quasirandom except for large numbers of observations the points spread out more horizontally and may overlap with other columns. # sample 1000 observations randomly from diamonds (smaller_diamonds &lt;- sample_n(diamonds, 1000, replace = FALSE)) ## # A tibble: 1,000 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.8 Very Good F VS2 63.2 55 3369 5.9 5.88 3.72 ## 2 1.74 Premium G VS1 61.7 58 17904 7.78 7.69 4.77 ## 3 1.27 Premium J VVS1 60.1 58 5761 7.06 6.99 4.22 ## 4 0.9 Very Good J VS1 62.6 55 3102 6.11 6.13 3.83 ## 5 0.9 Ideal J VS1 62.1 57 3418 6.16 6.18 3.83 ## 6 0.9 Very Good E SI1 63.2 58 4560 6.12 6.1 3.86 ## 7 1.12 Premium G VS1 62.2 59 7827 6.61 6.58 4.1 ## 8 0.5 Very Good E SI1 63.3 59 1415 5.03 4.99 3.17 ## 9 0.35 Very Good F VS1 60.3 58.6 842 4.56 4.59 2.76 ## 10 0.52 Ideal G VS2 61.3 56 1737 5.18 5.21 3.18 ## # … with 990 more rows # geom_jitter() ggplot(smaller_diamonds, aes(cut,price))+ geom_jitter() # base R stripchart stripchart(price~cut, data = smaller_diamonds, method = &quot;jitter&quot;, jitter = 0.1, vertical = TRUE) #install.packages(&quot;ggbeeswarm&quot;) library(ggbeeswarm) # ggbeeswarm geom_quasirandom() ggplot(smaller_diamonds, aes(cut,price))+ geom_quasirandom() # ggbeeswarm geom_beeswarm() ggplot(smaller_diamonds, aes(cut,price))+ geom_beeswarm() 7.5.2 Notes - Two categorical variables If you are interested in comparing two categorical variables, usually you would count the pairwise frequencies of each of the factors across both categorical variables. ggplot(data = diamonds) + geom_count(mapping = aes(x = cut, y = color)) The other example in the book is to first count each of the pairs then plot using a heatmap, or geom_tile(). This shows basically the same information as the plot above, except instead of the size of the dot being bigger with larger counts, the color of the tile changes. diamonds %&gt;% count(color, cut) ## # A tibble: 35 x 3 ## color cut n ## &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 D Fair 163 ## 2 D Good 662 ## 3 D Very Good 1513 ## 4 D Premium 1603 ## 5 D Ideal 2834 ## 6 E Fair 224 ## 7 E Good 933 ## 8 E Very Good 2400 ## 9 E Premium 2337 ## 10 E Ideal 3903 ## # … with 25 more rows diamonds %&gt;% count(color, cut) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = n)) + coord_flip() 7.5.2.1 Exercises 1. How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut? Depending on the distribution you want to observe, you can: within a color, display the proportion of observations that were of a certain cut, or within a cut, display the proportion of observations that were of a certain color. Alternatively, using a stacked bar would be more straightforward to visualise this, instead of the geom_tile() heatmap. # show distribution of color within cut diamonds %&gt;% count(color, cut) %&gt;% group_by(cut) %&gt;% mutate (by_cut_prop = n/sum(n)) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = by_cut_prop)) + coord_flip() # barplot distribution of color within cut diamonds %&gt;% ggplot(aes(x = cut, fill = color))+ geom_bar(position = &#39;fill&#39;) # show distribution of cut within color diamonds %&gt;% count(color, cut) %&gt;% group_by(color) %&gt;% mutate (by_color_prop = n/sum(n)) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = by_color_prop)) + coord_flip() # barplot distribution of cut within color diamonds %&gt;% ggplot(aes(x = color, fill = cut))+ geom_bar(position = &#39;fill&#39;) 2. Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it? Using dplyr, first group by destination and month, then compute the average flight delay for each month/dest using summarize(). Then, pipe the result into a geom_tile call. The large number of destinations makes the plot difficult to read. You can flip the coords so that the destinations will be on the y axis. In general, the problem with this plot is that there are too many items to keep track of. library(nycflights13) flights %&gt;% group_by(dest, month) %&gt;% summarize ( avg_delay = mean(dep_delay, na.rm = T)) %&gt;% ggplot(mapping = aes(x = dest, y = month)) + geom_tile(mapping = aes(fill = avg_delay)) + coord_flip() 3. Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above? One reason is that the axis labels are less likely to overlap if cut is on the y axis. Otherwise, using x = cut and y = color simply flips the coordinate axis on the plot. This does not change the interpretation of the plot, so I would say that the change is insignificant when looking at the big picture. Below are the two versions of the plots. # provided example, x = color, y = cut diamonds %&gt;% count(color, cut) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = n)) # switch axes, x = cut, y = color diamonds %&gt;% count(color, cut) %&gt;% ggplot(mapping = aes(x = cut, y = color)) + geom_tile(mapping = aes(fill = n)) 7.5.3 Notes - Two continuous variables To visualize the relationship between two continuous variables, use a scatterplot, or geom_point(). If working with large numbers of points, it might be better to make the points slightly transparent using an alpha value. # standard scatterplot ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price)) # make points transparent ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100) Another way to plot the observations if there are too many points is to use geom_bin2d. As the name suggests, the plot is binned and colored based on how many observations fall into each bin. This looks similar to a density plot, in which you can see where the majority of points lie. geom_hex() looks like a fancy version of geom_bin2d(). ggplot(data = diamonds) + geom_bin2d(mapping = aes(x = carat, y = price)) # install.packages(&quot;hexbin&quot;) ggplot(data = diamonds) + geom_hex(mapping = aes(x = carat, y = price)) The book also suggests binning one of the continuous variables and plotting boxplots for the other variable for each bin created. There are two ways to bin, using either cut_width() or cut_number(). cut_width() keeps the increment the same for each bin, whereas cut_number() keeps the number of observations the same. # filter dataset to only keep observations with carat &lt; 3 smaller &lt;- diamonds %&gt;% filter(carat &lt; 3) # cut_width binning ggplot(data = smaller, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_width(carat, 0.1))) # cut_number binning ggplot(data = smaller, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_number(carat, 20))) 7.5.3.1 Exercises 1. Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price? When using cut_width(), you must consider the range of the distribution of the variable you are binning. For example, you shouldn’t bin by a number larger than the range of carat, or by a number too small (too many bins). If an inappropriate bin size is chosen, the data could be uninterpretable. This goes for cut_number() too. Selecting an appropriate number of bins to create for cut_number() is slightly different. We have to take into account the number of observations that fall into each price category. Suppose the distribution of price was skewed to the right (small number of very high priced observations). This would result in a misleading cut_number() distribution. # visualize price binning by carat, cut_width() ggplot(smaller, aes(x = price, y = ..density..,)) + geom_freqpoly(aes(color = cut_width(carat, 0.5))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # visualize price binning by carat, cut_number(), 10 bins ggplot(smaller, aes(x = price, y = ..density..,)) + geom_freqpoly(aes(color = cut_number(carat, 10))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2. Visualise the distribution of carat, partitioned by price. To get this distribution, switch carat and price in the code from the previous question. # visualize distribution of carat, binning price, cut_number() ggplot(smaller, aes(x = carat, y = ..density..)) + geom_freqpoly(aes(color = cut_number(price, 10))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # visualize distribution of carat, binning price, cut_width() ggplot(smaller, aes(x = carat, y = ..density..)) + geom_freqpoly(aes(color = cut_width(price, 5000))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3. How does the price distribution of very large diamonds compare to small diamonds. Is it as you expect, or does it surprise you? Based on the graphs in question 1 and 2, very large diamonds (high carat) are, on average, priced higher than small diamonds (low carat). This is as I would expect. Looking at the distribution of carat size partitioned by price (cut_width() of 5000), we see that the majority of highest priced diamonds have high carat sizes. 4. Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price. One way to combine the 3 parameters in one visualization is to use facetting. To visualise how the distributoin of carat and price compare within each category of cut, we can facet either a scatterplot (a binned scatterplot might be better in this case, since there are so many points), or the freqpoly() graphs that we generated previously. We observe from the binned scatterplot that there is a large concentration of points (light blue color) at a low carat and low price. For all types of cuts, price correlates positively with carat. We can also facet the distribution of price vs cut by carat using cut_number(). In this plot we observe that, again, for all cut types, price correlates with higher carat size. # visualize price vs carat, faceting by cut ggplot (diamonds, aes(carat, price))+ geom_bin2d( )+ facet_wrap(~cut) # visualize price binning by carat, cut_number(), 10 bins ggplot(diamonds, aes(x = price, y = ..density..)) + geom_freqpoly(aes(color = cut_number(carat, 10)))+ facet_wrap(~cut) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # visualize price vs cut, binning by carat, 3 bins ggplot(diamonds, aes(x = cut, y = price)) + geom_boxplot()+ facet_wrap(~cut_number(carat, 3))+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) 5. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. Why is a scatterplot a better display than a binned plot for this case? Because the correlation of x and y is so strong, it becomes easy to find the outliers in the graph below. For example, we might wonder why the outlier with a x-value of 6.7 only had a y value of 4, since the majority of other points with x-values of 6.7 had y-values of near 6.7 as well. A scatter plot allows us to see these individual points, since a scatter plot simply plots all the points in the dataset on the graph. A binned plot using geom_bin2d() or geom_hex() may mask a lot of the outliers, depending on the bin size. An outlier, if included in a bin with non-outliers, will go undetected. Binning only one of the continuous variables will have less caveats, but may still result in issues. In the example below, I’ve binned the plot using a cut_width of 0.5, and plotted boxplots for each interval. We can see that while most of the outliers are preserved, the x-values have been shifted to match the center of each bin. # the provided scatterplot ggplot(data = diamonds) + geom_point(mapping = aes(x = x, y = y)) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) # bin using geom_hex. default graph is quite ugly and bins include outliers ggplot(data = diamonds) + geom_hex(mapping = aes(x = x, y = y)) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) # a binned boxplot of the same data ggplot(data = diamonds) + geom_boxplot(mapping = aes(x = x, y = y, group = cut_width (x, 0.5))) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) 7.6 Notes - Patterns and models Some important questions to ask when discovering a pattern in the data are: Could this pattern be due to coincidence (i.e. random chance)? How can you describe the relationship implied by the pattern? How strong is the relationship implied by the pattern? What other variables might affect the relationship? Does the relationship change if you look at individual subgroups of the data? By identifying patterns between variables, we can model the pattern and even use this information to learn more about other aspects of the data. As explained earlier, it was puzzling why ideal cut diamonds cost less than lower grade, fair cut diamonds. This was due to the fact that more ideal cut diamonds were smaller in carat, which resulted in the lower price. By identifying this pattern, we can subtract the influence of carat on price, and examine how the cut quality correlates with price after doing so. The book provides the following code, which fits a linear model (lm) between price and carat (mod), adds a transformed value of the residuals (observed value of price - expected value of price based solely on carat size) to the diamonds dataset, then plots the relationship between carat and the residual. We observe that the residuals are higher with ideal cuts than with fair cuts, as expected. We can interpret this as: if carat size was kept constant, ideal cuts cost more than fair cuts on average. library(modelr) mod &lt;- lm(log(price) ~ log(carat), data = diamonds) (diamonds2 &lt;- diamonds %&gt;% add_residuals(mod) %&gt;% mutate(resid = exp(resid))) ## # A tibble: 53,940 x 11 ## carat cut color clarity depth table price x y z resid ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 0.820 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 0.955 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 0.822 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 0.569 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 0.511 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 0.787 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 0.787 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 0.690 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 0.913 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 0.850 ## # … with 53,930 more rows ggplot(data = diamonds2) + geom_boxplot(mapping = aes(x = cut, y = resid)) "],
["chapter-8-workflow-projects.html", "Chapter 8 - Workflow: projects Notes - Project Workflows", " Chapter 8 - Workflow: projects There were no exercises for this chapter. Notes - Project Workflows To ensure reproducibility, I agree with the book that it is best to set RStudio to not preserve your workspace between sessions. You can also do the following to refresh your script: Press Cmd/Ctrl + Shift + F10 to restart RStudio. Press Cmd/Ctrl + Shift + S to rerun the current script. It is also very important to keep your analysis organized. Have all your plots saved along with your analysis script, and the copies of the raw data in the same folder as your R script/R notebook. A way that RStudio can facilitate organization is through the Projects feature. By creating a RStudio project (.Rproj) in a specified directory, any scripts that you work on for the project will know to look in that directory when you read / write files, which saves time. Whether you choose to use this feature or not, it is important to stay organized and have an easy way to return to a project and know what you did and which files / code are required to reproduce the analysis. "],
["chapter-10-tibbles.html", "Chapter 10 - Tibbles Notes - Creating Tibbles 10.5 Exercises", " Chapter 10 - Tibbles Notes - Creating Tibbles You can create tibbles from existing data frames using as_tibble(), or create brand new tibbles using tibble(): tibble( x = 1:5, y = 1, z = x ^ 2 + y ) ## # A tibble: 5 x 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 ## 2 2 1 5 ## 3 3 1 10 ## 4 4 1 17 ## 5 5 1 26 A cousin of tibble(), tribble(), can also be used as a way to manually enter data into a tibble format: tribble( ~x, ~y, ~z, #--|--|---- &quot;a&quot;, 2, 3.6, &quot;b&quot;, 1, 8.5 ) ## # A tibble: 2 x 3 ## x y z ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 2 3.6 ## 2 b 1 8.5 You can also use non-syntactic names for variables in tibbles: tb &lt;- tibble( `:)` = &quot;smile&quot;, ` ` = &quot;space&quot;, `2000` = &quot;number&quot; ) tb ## # A tibble: 1 x 3 ## `:)` ` ` `2000` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 smile space number When compared to a data.frame in baseR, the tibble looks more user-friendly. Calling a tibble automatically provides only the beginning chunk of the data rather than filling up your entire console (think if it as default head(data.frame) display). Other nice features include not converting strings to factors or changing variable names. To convert tables to or from data frames, use as_tibble() and as.data.frame(): class(iris) ## [1] &quot;data.frame&quot; class(as_tibble(iris)) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(as.data.frame(as_tibble(iris))) ## [1] &quot;data.frame&quot; You can select columns in tibbles the same way you would with a data.frame: df &lt;- tibble( x = runif(5), y = rnorm(5) ) # extract column &#39;x&#39; using either $ or [[]] df$x ## [1] 0.84011082 0.09959577 0.27459996 0.87146539 0.36318207 df[[&quot;x&quot;]] ## [1] 0.84011082 0.09959577 0.27459996 0.87146539 0.36318207 df[[1]] ## [1] 0.84011082 0.09959577 0.27459996 0.87146539 0.36318207 10.5 Exercises 1. How can you tell if an object is a tibble? (Hint: try printing mtcars, which is a regular data frame). You can tell if an object is a tibble because the output you get by calling it will say “tibble”! For example, calling the diamonds tibble returns :# A tibble: 53,940 x 10 as the first line of the output. Also you can tell something is a tibble based on the class specifications underneath each variable name. A tibble will also only print out the first 10 rows by default, whereas a data.frame will print out as many as the console allows. Last, the definitive way to tell something is a tibble is to use the class() function. class(diamonds) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(mtcars) ## [1] &quot;data.frame&quot; 2. Compare and contrast the following operations on a data.frame and equivalent tibble. What is different? Why might the default data frame behaviours cause you frustration? On a data.frame, df$x will still return the values for column xyz. This behavior does not occur for a tibble, which requires the exact name of the column df$xyz. This data.frame feature might cause frustration if you have columns in your dataset with the same prefix, in which you might fetch the wrong column. The other functions between data.frame and tibble work the same way. One distinction to note is that, when creating the data.frame, “a” is considered a factor with 1 level. When creating the tibble, “a” is not converted into a factor. df &lt;- data.frame(abc = 1, xyz = &quot;a&quot;) df$x ## [1] a ## Levels: a df[, &quot;xyz&quot;] ## [1] a ## Levels: a df[, c(&quot;abc&quot;, &quot;xyz&quot;)] ## abc xyz ## 1 1 a df &lt;- tibble(abc = 1, xyz = &quot;a&quot;) df$x ## Warning: Unknown or uninitialised column: &#39;x&#39;. ## NULL df[, &quot;xyz&quot;] ## # A tibble: 1 x 1 ## xyz ## &lt;chr&gt; ## 1 a df[, c(&quot;abc&quot;, &quot;xyz&quot;)] ## # A tibble: 1 x 2 ## abc xyz ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 a 3. If you have the name of a variable stored in an object, e.g. var &lt;- “mpg”, how can you extract the reference variable from a tibble? If the name of the variable is stored in an object, you can pass the object in lieu of the variable name using [[]] or [] just as you would do so with the explicit variable name. You can even pass the object and another variable name to obtain multiple reference variables using c(). I provide an example below using the diamonds dataset. var &lt;- &quot;carat&quot; var2 &lt;- c(&quot;carat&quot;,&quot;price&quot;) # extract only carat diamonds[,var] ## # A tibble: 53,940 x 1 ## carat ## &lt;dbl&gt; ## 1 0.23 ## 2 0.21 ## 3 0.23 ## 4 0.290 ## 5 0.31 ## 6 0.24 ## 7 0.24 ## 8 0.26 ## 9 0.22 ## 10 0.23 ## # … with 53,930 more rows #extract carat and price diamonds[,c(var,&quot;price&quot;)] ## # A tibble: 53,940 x 2 ## carat price ## &lt;dbl&gt; &lt;int&gt; ## 1 0.23 326 ## 2 0.21 326 ## 3 0.23 327 ## 4 0.290 334 ## 5 0.31 335 ## 6 0.24 336 ## 7 0.24 336 ## 8 0.26 337 ## 9 0.22 337 ## 10 0.23 338 ## # … with 53,930 more rows diamonds[,var2] ## # A tibble: 53,940 x 2 ## carat price ## &lt;dbl&gt; &lt;int&gt; ## 1 0.23 326 ## 2 0.21 326 ## 3 0.23 327 ## 4 0.290 334 ## 5 0.31 335 ## 6 0.24 336 ## 7 0.24 336 ## 8 0.26 337 ## 9 0.22 337 ## 10 0.23 338 ## # … with 53,930 more rows 4. Practice referring to non-syntactic names in the following data frame by: annoying &lt;- tibble( `1` = 1:10, `2` = `1` * 2 + rnorm(length(`1`)) ) Extracting the variable called 1. annoying[,&quot;1&quot;] ## # A tibble: 10 x 1 ## `1` ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 Plotting a scatterplot of 1 vs 2. ggplot(annoying, aes(`1`,`2`))+ geom_point() Creating a new column called 3 which is 2 divided by 1. annoying %&gt;% mutate(`3` = `2`/`1`) ## # A tibble: 10 x 3 ## `1` `2` `3` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.58 1.58 ## 2 2 3.45 1.73 ## 3 3 7.28 2.43 ## 4 4 7.28 1.82 ## 5 5 9.54 1.91 ## 6 6 9.81 1.63 ## 7 7 13.6 1.94 ## 8 8 15.4 1.93 ## 9 9 19.4 2.15 ## 10 10 20.4 2.04 Renaming the columns to one, two and three. annoying %&gt;% mutate(`3` = `2`/`1`) %&gt;% rename(one = `1`, two = `2`, three = `3` ) ## # A tibble: 10 x 3 ## one two three ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.58 1.58 ## 2 2 3.45 1.73 ## 3 3 7.28 2.43 ## 4 4 7.28 1.82 ## 5 5 9.54 1.91 ## 6 6 9.81 1.63 ## 7 7 13.6 1.94 ## 8 8 15.4 1.93 ## 9 9 19.4 2.15 ## 10 10 20.4 2.04 5. What does tibble::enframe() do? When might you use it? Taken from the documentation: “enframe() converts named atomic vectors or lists to two-column data frames. For unnamed vectors, the natural sequence is used as name column.” I might use this when I have a vector that I want to turn into a data.frame for graphing using ggplot, which requires data be in data.frame or tibble. x = rnorm(100) names(x) &lt;- c(5:104) enframe(x) ## # A tibble: 100 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 5 1.21 ## 2 6 -0.197 ## 3 7 0.227 ## 4 8 -0.749 ## 5 9 -0.262 ## 6 10 -1.42 ## 7 11 0.178 ## 8 12 3.02 ## 9 13 1.07 ## 10 14 1.49 ## # … with 90 more rows class(enframe(x)) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 6. What option controls how many additional column names are printed at the footer of a tibble? The documentation for ?format.tbl (tibble formatting) says that the n_extra argument will control how many additional columns to print abbreviated information for. The example provided in the documentation is below, which only prints 2 of the additional columns (whereas the unmodified print(flights) would yield 5 additional columns in the footer). print(nycflights13::flights, n_extra = 2) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, … "],
["chapter-11-data-import.html", "Chapter 11 - Data import 11.2 Notes - Reading in files 11.2.2 Exercises 11.3 Notes - Parsing a vector 11.3.5 Exercises 11.4 Notes - Parsing a file 11.5 Notes - Writing to a file", " Chapter 11 - Data import 11.2 Notes - Reading in files To practice various utilities for reading in files, we can use inline csv designation, which requires proper newline designation. Below are some examples of reading in inline csv chunks with various arguments tailored for the type of data being read in. # basic read_csv() read_csv(&quot;a,b,c 1,2,3 4,5,6&quot;) ## # A tibble: 2 x 3 ## a b c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 ## 2 4 5 6 # read in data, ignoring metadata lines read_csv(&quot;The first line of metadata The second line of metadata x,y,z 1,2,3&quot;, skip = 2) ## # A tibble: 1 x 3 ## x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 # designate lines to skip that start with specific symbol read_csv(&quot;# A comment I want to skip x,y,z 1,2,3&quot;, comment = &quot;#&quot;) ## # A tibble: 1 x 3 ## x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 # read in file that doesnt have column names read_csv(&quot;1,2,3\\n4,5,6&quot;, col_names = FALSE) ## # A tibble: 2 x 3 ## X1 X2 X3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 ## 2 4 5 6 # read in file and specify column names read_csv(&quot;1,2,3\\n4,5,6&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;)) ## # A tibble: 2 x 3 ## x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 ## 2 4 5 6 # read in file, replacing symbol with missing values (NA) read_csv(&quot;a,b,c\\n1,2,.&quot;, na = &quot;.&quot;) ## # A tibble: 1 x 3 ## a b c ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1 2 NA 11.2.2 Exercises 1. What function would you use to read a file where fields were separated with “|”? I would use read_delim() to read in a file where fields are separated with “|”. For example: read_delim (&quot;a|b|c 1|2|3 4|5|6&quot;, &quot;|&quot;) ## # A tibble: 2 x 3 ## a b c ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot; 1&quot; 2 3 ## 2 &quot; 4&quot; 5 6 2. Apart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common? Based on the documentation, read_csv() and read_tsv() have col_names, col_types, locale, na, quoted_na, quote, trim_ws, n_max, guess_max, and progress. 3. What are the most important arguments to read_fwf()? The most important arguments are the file, and the col_positions arguments. There are many options to specify col_positions, including fwf_empty(), fwf_widths(), fwf_positions(), and fwf_cols(). Below is the example provided in the documentation: fwf_sample &lt;- readr_example(&quot;fwf-sample.txt&quot;) cat(read_lines(fwf_sample)) ## John Smith WA 418-Y11-4111 Mary Hartford CA 319-Z19-4341 Evan Nolan IL 219-532-c301 # You can specify column positions in several ways: # 1. Guess based on position of empty columns read_fwf(fwf_sample, fwf_empty(fwf_sample, col_names = c(&quot;first&quot;, &quot;last&quot;, &quot;state&quot;, &quot;ssn&quot;))) ## Parsed with column specification: ## cols( ## first = col_character(), ## last = col_character(), ## state = col_character(), ## ssn = col_character() ## ) ## # A tibble: 3 x 4 ## first last state ssn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Smith WA 418-Y11-4111 ## 2 Mary Hartford CA 319-Z19-4341 ## 3 Evan Nolan IL 219-532-c301 # 2. A vector of field widths read_fwf(fwf_sample, fwf_widths(c(20, 10, 12), c(&quot;name&quot;, &quot;state&quot;, &quot;ssn&quot;))) ## Parsed with column specification: ## cols( ## name = col_character(), ## state = col_character(), ## ssn = col_character() ## ) ## # A tibble: 3 x 3 ## name state ssn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Smith WA 418-Y11-4111 ## 2 Mary Hartford CA 319-Z19-4341 ## 3 Evan Nolan IL 219-532-c301 # 3. Paired vectors of start and end positions read_fwf(fwf_sample, fwf_positions(c(1, 30), c(10, 42), c(&quot;name&quot;, &quot;ssn&quot;))) ## Parsed with column specification: ## cols( ## name = col_character(), ## ssn = col_character() ## ) ## # A tibble: 3 x 2 ## name ssn ## &lt;chr&gt; &lt;chr&gt; ## 1 John Smith 418-Y11-4111 ## 2 Mary Hartf 319-Z19-4341 ## 3 Evan Nolan 219-532-c301 # 4. Named arguments with start and end positions read_fwf(fwf_sample, fwf_cols(name = c(1, 10), ssn = c(30, 42))) ## Parsed with column specification: ## cols( ## name = col_character(), ## ssn = col_character() ## ) ## # A tibble: 3 x 2 ## name ssn ## &lt;chr&gt; &lt;chr&gt; ## 1 John Smith 418-Y11-4111 ## 2 Mary Hartf 319-Z19-4341 ## 3 Evan Nolan 219-532-c301 # 5. Named arguments with column widths read_fwf(fwf_sample, fwf_cols(name = 20, state = 10, ssn = 12)) ## Parsed with column specification: ## cols( ## name = col_character(), ## state = col_character(), ## ssn = col_character() ## ) ## # A tibble: 3 x 3 ## name state ssn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Smith WA 418-Y11-4111 ## 2 Mary Hartford CA 319-Z19-4341 ## 3 Evan Nolan IL 219-532-c301 0.0.1 4. Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like &quot; or ’. By convention, read_csv() assumes that the quoting character will be “, and if you want to change it you’ll need to use read_delim() instead. What arguments do you need to specify to read the following text into a data frame? {-} &quot;x,y\\n1,'a,b'&quot; In this example, the string is surrounded by the quoting character ’. This is not what read_csv() default assumes. In the documentation it looks like you can specify the quote argument for both read_csv() and read_delim(). Below I read in the text using both methods, specifying quote = &quot;\\'&quot;. read_csv (&quot;x,y\\n1,&#39;a,b&#39;&quot;, quote = &quot;\\&#39;&quot;) ## # A tibble: 1 x 2 ## x y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 a,b read_delim(&quot;x,y\\n1,&#39;a,b&#39;&quot;, delim = &quot;,&quot;, quote = &quot;\\&#39;&quot;) ## # A tibble: 1 x 2 ## x y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 a,b 5. Identify what is wrong with each of the following inline CSV files. What happens when you run the code? I have annotated the code below with the problems for each of the inline CSV files. The output should be displayed if you are viewing the rendered .md file (you won’t see the output if this is a .Rmd file). # There are not enough column names to go with the amount of columns in the data. read_csv(&quot;a,b\\n1,2,3\\n4,5,6&quot;) ## Warning: 2 parsing failures. ## row col expected actual file ## 1 -- 2 columns 3 columns literal data ## 2 -- 2 columns 3 columns literal data ## # A tibble: 2 x 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 ## 2 4 5 # Mismatched numbers of columns again. The first row only has 2, whereas the 2nd row has 4, and the header only has 3. read_csv(&quot;a,b,c\\n1,2\\n1,2,3,4&quot;) ## Warning: 2 parsing failures. ## row col expected actual file ## 1 -- 3 columns 2 columns literal data ## 2 -- 3 columns 4 columns literal data ## # A tibble: 2 x 3 ## a b c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 NA ## 2 1 2 3 # Two header columns, and one column of data. Also the &quot;1&quot; is still being read in as an integer. read_csv(&quot;a,b\\n\\&quot;1&quot;) ## Warning: 2 parsing failures. ## row col expected actual file ## 1 a closing quote at end of file literal data ## 1 -- 2 columns 1 columns literal data ## # A tibble: 1 x 2 ## a b ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 &lt;NA&gt; # Since there are both integer values and character values in the same column, both columns are defined as character. read_csv(&quot;a,b\\n1,2\\na,b&quot;) ## # A tibble: 2 x 2 ## a b ## &lt;chr&gt; &lt;chr&gt; ## 1 1 2 ## 2 a b # I assume &quot;;&quot; was meant to be the delimiter. The csv only has one observation. Use read_delim(&quot;a;b\\n1;3&quot;, &quot;;&quot;) instead. read_csv(&quot;a;b\\n1;3&quot;) ## # A tibble: 1 x 1 ## `a;b` ## &lt;chr&gt; ## 1 1;3 11.3 Notes - Parsing a vector Parsing vectors or files can be useful to convert variables to their appropriate classes. For example, if a column of integer values was read in as characters, we can convert the data back into integers using parse_integer(). Below are the provided examples of use: str(parse_logical(c(&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;NA&quot;))) ## logi [1:3] TRUE FALSE NA str(parse_integer(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) ## int [1:3] 1 2 3 str(parse_date(c(&quot;2010-01-01&quot;, &quot;1979-10-14&quot;))) ## Date[1:2], format: &quot;2010-01-01&quot; &quot;1979-10-14&quot; Below are the other examples using parsing functions. Using the problems() function on a parsed vector seems especially useful! # can specify na values if present in data parse_integer(c(&quot;1&quot;, &quot;231&quot;, &quot;.&quot;, &quot;456&quot;), na = &quot;.&quot;) ## [1] 1 231 NA 456 x &lt;- parse_integer(c(&quot;123&quot;, &quot;345&quot;, &quot;abc&quot;, &quot;123.45&quot;)) ## Warning: 2 parsing failures. ## row col expected actual ## 3 -- an integer abc ## 4 -- no trailing characters .45 x ## [1] 123 345 NA NA ## attr(,&quot;problems&quot;) ## # A tibble: 2 x 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3 NA an integer abc ## 2 4 NA no trailing characters .45 problems(x) ## # A tibble: 2 x 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3 NA an integer abc ## 2 4 NA no trailing characters .45 11.3.1 Parsing Numbers Reading in data obtained from outside the US is also tricky since there are different conventions used to display numerical data. For example, using “.” instead of “,” to mark decimal places or groupings. The functions have arguments that allow you to specify these marks. The examples provided in the book are below. parse_number() also ignores non-numerical symbols such as $ or %. However, parse_double does not seem to have this feature. # you can specify the decimal mark symbol if needed parse_double(&quot;1.23&quot;) ## [1] 1.23 parse_double(&quot;1,23&quot;, locale = locale(decimal_mark = &quot;,&quot;)) ## [1] 1.23 # parse_number() ignores non-numerical symbols parse_number(&quot;$100&quot;) ## [1] 100 parse_number(&quot;20%&quot;) ## [1] 20 parse_number(&quot;It cost $123.45&quot;) ## [1] 123.45 # You can also specify grouping marks if needed. # Used in America parse_number(&quot;$123,456,789&quot;) ## [1] 123456789 # Used in many parts of Europe parse_number(&quot;123.456.789&quot;, locale = locale(grouping_mark = &quot;.&quot;)) ## [1] 123456789 # Used in Switzerland parse_number(&quot;123&#39;456&#39;789&quot;, locale = locale(grouping_mark = &quot;&#39;&quot;)) ## [1] 123456789 11.3.2 Parsing Strings You can use parse_character() to parse strings. Each character in a string is encoded, and you can specify the encoding as an argument in parse_character(). To guess the encoding for a particular string you are parsing, you can use guess_encoding(). x1 &lt;- &quot;El Ni\\xf1o was particularly bad this year&quot; x2 &lt;- &quot;\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd&quot; parse_character(x1, locale = locale(encoding = &quot;Latin1&quot;)) ## [1] &quot;El Niño was particularly bad this year&quot; parse_character(x2, locale = locale(encoding = &quot;Shift-JIS&quot;)) ## [1] &quot;こんにちは&quot; guess_encoding(charToRaw(x1)) ## # A tibble: 2 x 2 ## encoding confidence ## &lt;chr&gt; &lt;dbl&gt; ## 1 ISO-8859-1 0.46 ## 2 ISO-8859-9 0.23 guess_encoding(charToRaw(x2)) ## # A tibble: 1 x 2 ## encoding confidence ## &lt;chr&gt; &lt;dbl&gt; ## 1 KOI8-R 0.42 11.3.3 Parsing Factors To parse a vector of factors, you can use parse_factor(), specifying the levels that you are expecting to see. If a value in the vector does not exist in the levels argument, an error is returned. fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;) parse_factor(c(&quot;apple&quot;, &quot;banana&quot;, &quot;banana&quot;), levels = fruit) ## [1] apple banana banana ## Levels: apple banana parse_factor(c(&quot;apple&quot;, &quot;banana&quot;, &quot;bananana&quot;), levels = fruit) ## Warning: 1 parsing failure. ## row col expected actual ## 3 -- value in level set bananana ## [1] apple banana &lt;NA&gt; ## attr(,&quot;problems&quot;) ## # A tibble: 1 x 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3 NA value in level set bananana ## Levels: apple banana 11.3.4 Parsing Dates, Date-times, and Times There are three types of parsers for these purposes which spit out a combination of date, time, or date-time. Below are the provided examples from the book for each of the parsers. # date-time # requires input as year, month, day (mandatory), time-(optional)-hour, minute, second, parse_datetime(&quot;2010-10-01T2010&quot;) ## [1] &quot;2010-10-01 20:10:00 UTC&quot; parse_datetime(&quot;20101010&quot;) ## [1] &quot;2010-10-10 UTC&quot; # date - year, month, day # expects a four digit year, a - or /, the month, a - or /, then the day parse_date(&quot;2010-10-01&quot;) ## [1] &quot;2010-10-01&quot; # expects the hour, :, minutes, optionally : and seconds, and an optional am/pm specifier: library(hms) parse_time(&quot;01:10 am&quot;) ## 01:10:00 parse_time(&quot;01:10 pm&quot;) ## 13:10:00 parse_time(&quot;20:10:01&quot;) ## 20:10:01 You can also create your own date-time format. There are many parameters you can specify for your date-time “key”. See ?parse_date for the options. Depending on how you set up the “key”, you may parse different dates from one set of numbers (book example below). # different dates are parsed depending on the key that you provide parse_date(&quot;01/02/15&quot;, &quot;%m/%d/%y&quot;) ## [1] &quot;2015-01-02&quot; parse_date(&quot;01/02/15&quot;, &quot;%d/%m/%y&quot;) ## [1] &quot;2015-02-01&quot; parse_date(&quot;01/02/15&quot;, &quot;%y/%m/%d&quot;) ## [1] &quot;2001-02-15&quot; Last, as with parsing numbers, different countries may have different date formats. You can solve this by specifying the local argument, as we did with parse_integer(). parse_date(&quot;1 janvier 2015&quot;, &quot;%d %B %Y&quot;, locale = locale(&quot;fr&quot;)) ## [1] &quot;2015-01-01&quot; 11.3.5 Exercises 1. What are the most important arguments to locale()? If you are using locale() for parse_number(), then the most important arguments are decimal_mark and grouping_mark. For parse_character(), you should specify encoding. For parse_date(), you should specify the region using the appropriate characters. 2. What happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to “,”? What happens to the default value of decimal_mark when you set the grouping_mark to “.”? When you try to set them to the same character, you get an error: Error: `decimal_mark` and `grouping_mark` must be different The default grouping_mark becomes ‘.’ if decimal_mark is set to ‘,’. # parse_number(&quot;1,234.567&quot;, locale = locale(grouping_mark = &#39;.&#39;, decimal_mark = &#39;.&#39;)) # This Errors! parse_number(&quot;1.234,567&quot;, locale = locale(decimal_mark = &#39;,&#39;)) ## [1] 1234.567 parse_number(&quot;1.234,567&quot;, locale = locale(grouping_mark = &#39;.&#39;)) ## [1] 1234.567 3. I didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful. The date_format and time_format specify the date and time formats for the parse function, which are by default date_format = &quot;%AD&quot; and time_format = &quot;%AT&quot;. From the readr vignette, for date_format, “The default value is %AD which uses an automatic date parser that recognises dates of the format Y-m-d or Y/m/d.” For time_format, “The default value is %At which uses an automatic time parser that recognises times of the form H:M optionally followed by seconds and am/pm.” I could see this useful to specify a custom date_format for american dates, which are often entered as m-d-Y instead of the default Y-m-d. The full four year date is also often truncated to the last 2 digits, which might result in an error without specifying it in date_format. Below is an example. # today&#39;s american date parsed incorrectly using default date_format (throws error) parse_date(&quot;05/24/18&quot;) ## Warning: 1 parsing failure. ## row col expected actual ## 1 -- date like 05/24/18 ## [1] NA # today&#39;s american date parsed correctly by specifying date_format parse_date(&quot;05/24/18&quot;, locale = locale(date_format = &quot;%m/%d/%y&quot;)) ## [1] &quot;2018-05-24&quot; 4. If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly. I live in the US, but for practice purposes lets say I move to Colombia. I might have to commonly read in old files that are not UTF-8 encoded, but latin1 encoded. The decimal marks are also “,” instead of “.”. Below is an example locale object for these requirements. locale(date_names = &quot;es&quot;, decimal_mark = &quot;,&quot;, encoding = &quot;latin1&quot;) ## &lt;locale&gt; ## Numbers: 123.456,78 ## Formats: %AD / %AT ## Timezone: UTC ## Encoding: latin1 ## &lt;date_names&gt; ## Days: domingo (dom.), lunes (lun.), martes (mar.), miércoles (mié.), ## jueves (jue.), viernes (vie.), sábado (sáb.) ## Months: enero (ene.), febrero (feb.), marzo (mar.), abril (abr.), mayo ## (may.), junio (jun.), julio (jul.), agosto (ago.), ## septiembre (sept.), octubre (oct.), noviembre (nov.), ## diciembre (dic.) ## AM/PM: a. m./p. m. 5. What’s the difference between read_csv() and read_csv2()? Base on the documentation, read_csv2() uses semicolons “;” as separators, instead of “,”. read_csv2() would ideally be used if the comma “,” is used as a decimal point within the file, which would mess up read_csv(). Below is an example: # messed up because &quot;,&quot; is used as a decimal point read_csv(&quot;a;b\\n1,0;2,0&quot;) ## Warning: 1 parsing failure. ## row col expected actual file ## 1 -- 1 columns 3 columns literal data ## # A tibble: 1 x 1 ## `a;b` ## &lt;dbl&gt; ## 1 1 # using read_csv2 fixes the problem read_csv2(&quot;a;b\\n1,0;2,0&quot;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## # A tibble: 1 x 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 6. What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out. I got the info from the ?stringi::stri_enc_detect documentation. Common encodings used in Europe are: ISO-8859-1, ISO-8859-2, windows-1252, ISO-8859-7 Common encodings used in Asia are: Shift_JIS, ISO-2022-JP, ISO-2022-CN, ISO-2022-KR, GB18030, EUC-JP, EUC-KR UTF-8 is widely popular now, and you can also use guess_encoding() if you are unsure what encoding to use. There is also a lot of info about encoding on Wikipedia. 7. Generate the correct format string to parse each of the following dates and times: My answers are in the R code below. Helpful descriptions for the format string paramters are found at ?parse_datetime. d1 &lt;- &quot;January 1, 2010&quot; parse_date (d1, &quot;%B%e%*%Y&quot;) ## [1] &quot;2010-01-01&quot; d2 &lt;- &quot;2015-Mar-07&quot; parse_date(d2, &quot;%Y-%b-%d&quot;) ## [1] &quot;2015-03-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; parse_date(d3, &quot;%d-%b-%Y&quot;) ## [1] &quot;2017-06-06&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) parse_date(d4, &quot;%B %d (%Y)&quot;) ## [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 parse_date(d5, &quot;%m/%d/%y&quot;) ## [1] &quot;2014-12-30&quot; t1 &lt;- &quot;1705&quot; parse_time(t1, &quot;%H%M&quot;) ## 17:05:00 t2 &lt;- &quot;11:15:10.12 PM&quot; parse_time(t2, &quot;%I:%M:%OS %p&quot;) ## 23:15:10.12 11.4 Notes - Parsing a file The parsers that we learned about in the previous section are automatically applied by readr when reading in a file using read_csv() or other reading functions. These guess the type of data in each column being read in, using a combination of guess_parser() and parse_guess() on the first 1000 rows of observations. guess_parser(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) ## [1] &quot;logical&quot; parse_guess(&quot;2010-10-10&quot;) ## [1] &quot;2010-10-10&quot; There are usually a lot of issues when parsing a large, unorganized file. readr has a “challenge” example that displays some of the issues that arise: challenge &lt;- read_csv(readr_example(&quot;challenge.csv&quot;)) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_logical() ## ) ## Warning: 1000 parsing failures. ## row col expected actual file ## 1001 y 1/0/T/F/TRUE/FALSE 2015-01-16 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1002 y 1/0/T/F/TRUE/FALSE 2018-05-18 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1003 y 1/0/T/F/TRUE/FALSE 2015-09-05 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1004 y 1/0/T/F/TRUE/FALSE 2012-11-28 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1005 y 1/0/T/F/TRUE/FALSE 2020-01-13 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## .... ... .................. .......... ............................................................................................ ## See problems(...) for more details. Since the default only looks at the first 1000 rows, we can run into issues if the first 1000 rows or more have troubling characteristics. Here there are many issues displayed in the output after attempting to read the file in. It is very helpful that the error output displays what the function attempted to do. We see that it attempted to parse column x using col_integer(), and column y using col_character(). We can see more details by using problems(): problems(challenge) ## # A tibble: 1,000 x 5 ## row col expected actual file ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1001 y 1/0/T/F/TRUE… 2015-01… &#39;/Library/Frameworks/R.framework/Ver… ## 2 1002 y 1/0/T/F/TRUE… 2018-05… &#39;/Library/Frameworks/R.framework/Ver… ## 3 1003 y 1/0/T/F/TRUE… 2015-09… &#39;/Library/Frameworks/R.framework/Ver… ## 4 1004 y 1/0/T/F/TRUE… 2012-11… &#39;/Library/Frameworks/R.framework/Ver… ## 5 1005 y 1/0/T/F/TRUE… 2020-01… &#39;/Library/Frameworks/R.framework/Ver… ## 6 1006 y 1/0/T/F/TRUE… 2016-04… &#39;/Library/Frameworks/R.framework/Ver… ## 7 1007 y 1/0/T/F/TRUE… 2011-05… &#39;/Library/Frameworks/R.framework/Ver… ## 8 1008 y 1/0/T/F/TRUE… 2020-07… &#39;/Library/Frameworks/R.framework/Ver… ## 9 1009 y 1/0/T/F/TRUE… 2011-04… &#39;/Library/Frameworks/R.framework/Ver… ## 10 1010 y 1/0/T/F/TRUE… 2010-05… &#39;/Library/Frameworks/R.framework/Ver… ## # … with 990 more rows tail(challenge) ## # A tibble: 6 x 2 ## x y ## &lt;dbl&gt; &lt;lgl&gt; ## 1 0.805 NA ## 2 0.164 NA ## 3 0.472 NA ## 4 0.718 NA ## 5 0.270 NA ## 6 0.608 NA typeof(challenge$x) ## [1] &quot;double&quot; typeof(challenge$y) ## [1] &quot;logical&quot; unique(challenge$y[1:1000]) ## [1] NA The values for column x after row 1000 seem to be doubles, rather than integers. We can fix this by changing the default parsing function from col_integer() to col_double(). We also observe that column y contains date values, but the default type was character, since the first 1000 values were NA. We can fix this by changing col_character() to col_date(). challenge &lt;- read_csv( readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_double(), y = col_date() ) ) tail(challenge) ## # A tibble: 6 x 2 ## x y ## &lt;dbl&gt; &lt;date&gt; ## 1 0.805 2019-11-21 ## 2 0.164 2018-03-29 ## 3 0.472 2014-08-04 ## 4 0.718 2015-08-16 ## 5 0.270 2020-02-04 ## 6 0.608 2019-01-06 typeof(challenge$x) ## [1] &quot;double&quot; typeof(challenge$y) ## [1] &quot;double&quot; Hadley recommends that we should always examine the output of the read_() function and re-specify the col_parsers to match what is appropriate for the data. One strategy around this that he describes, which I think would probably be more straightforward if you have many, many columns of data, is to read everything in as a character, then use type_convert() on the table to convert to the appropriate types. We can see in the example below that type_convert() properly converts column x to double and column y to date formats. challenge2 &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), col_types = cols(.default = col_character()) ) challenge2 ## # A tibble: 2,000 x 2 ## x y ## &lt;chr&gt; &lt;chr&gt; ## 1 404 &lt;NA&gt; ## 2 4172 &lt;NA&gt; ## 3 3004 &lt;NA&gt; ## 4 787 &lt;NA&gt; ## 5 37 &lt;NA&gt; ## 6 2332 &lt;NA&gt; ## 7 2489 &lt;NA&gt; ## 8 1449 &lt;NA&gt; ## 9 3665 &lt;NA&gt; ## 10 3863 &lt;NA&gt; ## # … with 1,990 more rows type_convert(challenge2) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_date(format = &quot;&quot;) ## ) ## # A tibble: 2,000 x 2 ## x y ## &lt;dbl&gt; &lt;date&gt; ## 1 404 NA ## 2 4172 NA ## 3 3004 NA ## 4 787 NA ## 5 37 NA ## 6 2332 NA ## 7 2489 NA ## 8 1449 NA ## 9 3665 NA ## 10 3863 NA ## # … with 1,990 more rows The functions read_lines() and read_file() also seem useful to read in the raw lines or unstructured text of a file, in order to better understand the type of data contained. Unless you want to manipulate the strings or extract data using regexes, it might be more efficient to use less in your terminal to view the data file, rather than read_file(). # use read_lines to read individual lines of the file head(read_lines(readr_example(&quot;challenge.csv&quot;))) ## [1] &quot;x,y&quot; &quot;404,NA&quot; &quot;4172,NA&quot; &quot;3004,NA&quot; &quot;787,NA&quot; &quot;37,NA&quot; #use read_file to read the entire file in as one string substr(read_file(readr_example(&quot;challenge.csv&quot;)), 1, 100) ## [1] &quot;x,y\\n404,NA\\n4172,NA\\n3004,NA\\n787,NA\\n37,NA\\n2332,NA\\n2489,NA\\n1449,NA\\n3665,NA\\n3863,NA\\n4374,NA\\n875,NA\\n172,N&quot; 11.5 Notes - Writing to a file The functions write_csv() and write_tsv() are useful functions to write data.frames or tibbles in R to files. When writing files, it is important to use UTF-8 encoding for strings and save dates/date-times in ISO8601 format. There is also a special function for writing to excel: write_excel_csv(). write_rds() will save the actual R object containing the data frame, so that if you load the .rds using read_rds() you can access the data as it was at the time of save. Think of this as using save() in base R to save a .Robj for a variable you want to keep track of. You could also use write_feather() from the feather package to save the data in a format accessible by other programming languages, or read it back into R using read_feather(). # write_csv(challenge, &quot;challenge.csv&quot;) # write_rds(challenge, &quot;challenge.rds&quot;) # library(feather) # write_feather(challenge, &quot;challenge.feather&quot;) When executing the above write_csv() or write_rds(), the files will appear in your working directory, which if you are using an R notebook, conveniently is where your .Rmd file is being kept! There are also other types of files that might be read in. You can use haven for reading in SPSS, Stata, and SAS files, readxl for excel files, DBI to run SQL queries against databases (returns data frame), jsonlite for json, and xml2 for XML. "],
["chapter-12-tidy-data.html", "Chapter 12 - Tidy data 12.2.1 Exercises 12.3.1 Notes - Spreading and Gathering 12.3.3 Exercises 12.4 Notes - Separating and uniting 12.4.3 Exercises 12.5 Notes - Missing Values 12.5.1 Exercises 12.6 Notes - Case Study 12.6.1 Exercises", " Chapter 12 - Tidy data Below is the example provided by the book of the same data presented in 4 different ways. One way is tidy, the others are not! table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 # data spread across 2 tables table4a # cases ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4b # population ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 The three rules that must be satisfied for data to be “tidy” are: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Using these rules, the a tidy dataset from above is table1. The packages in the tidyverse such as ggplot2, dplyr, etc. are designed to work with tidy data, so we must learn how to reorganize data and clean it up in the event that we receive a dataset that is not tidy (which is very often). 12.2.1 Exercises 1. Using prose, describe how the variables and observations are organised in each of the sample tables. In table1, each of the variables are organized into columns, and each observation is in a separate row. Each row presents a unique combination of variables and respective observations, and each value has its own cell. This is considered a tidy dataset. In table2, each observation is in a separate row, but each variable does not have its own column. This is not a tidy dataset. To “tidy” this data, population and cases should be separated into individual rows, based on the count column. Count is not considered a variable. Rather, it is considered a value. In table 3, population and cases have been combined into a single column, “rate”. Note that this is not the actual value of the cases divided by the population (as you might obtain using a mutate()), rather it is a character listing the two separate items! If we had not known there were two variables combined into one column, we might have called this a tidy dataset. I would find a way to split the rate into cases and population, then use mutate() to calculate the rate. In table 4a and 4b, the data for population and cases have been split into two separate tables. The date variable is spread across the columns, and each row of each table represents an individual observation. The two tables have the same country column, and look like something you would see when working with relational data. 2. Compute the rate for table2, and table4a + table4b. You will need to perform four operations: Extract the number of TB cases per country per year. Extract the matching population per country per year. Divide cases by population, and multiply by 10000. Store back in the appropriate place. Which representation is easiest to work with? Which is hardest? Why? I will try doing the above operations without using tidyr functions. For table2, we can first filter for the TB cases using filter(), then filter for the population per country using filter() as well. Then we can divide the two, multiply by 10000, and bind the results back to the original table2 by using cbind(). Since there are two rows in table2 corresponding to the same rate value, we must duplicate each rate twice using rep( each = 2 ) before binding to table2. This preserves the original structure of table2, and has the rate stored in the appropriate places. Using the spread() function in tidyr to generate tidy data and then using mutate() is much easier! For table 4a and 4b, we can extract the 1999 values from each table and perform the rate calculation, and do the same thing for the 2000 values.Then, we can bind these rates back to both table4a and 4b using cbind. This preserves the original structure for both tables. # perform the operations above for table 2a, without using spread() rate_table2 &lt;- ((filter (table2, type == &#39;cases&#39;)$count / # select cases filter (table2, type == &#39;population&#39;)$count) * 10000) %&gt;% # divide by populatoin rep(each = 2) # repeat each element twice, maintaining the order of the vector cbind(table2,rate_table2) ## country year type count rate_table2 ## 1 Afghanistan 1999 cases 745 0.372741 ## 2 Afghanistan 1999 population 19987071 0.372741 ## 3 Afghanistan 2000 cases 2666 1.294466 ## 4 Afghanistan 2000 population 20595360 1.294466 ## 5 Brazil 1999 cases 37737 2.193930 ## 6 Brazil 1999 population 172006362 2.193930 ## 7 Brazil 2000 cases 80488 4.612363 ## 8 Brazil 2000 population 174504898 4.612363 ## 9 China 1999 cases 212258 1.667495 ## 10 China 1999 population 1272915272 1.667495 ## 11 China 2000 cases 213766 1.669488 ## 12 China 2000 population 1280428583 1.669488 # using spread() and dplyr is more straightforward and results in tidy data. table2 %&gt;% spread(key = type, value = count) %&gt;% mutate(rate = (cases / population) * 10000) ## # A tibble: 6 x 5 ## country year cases population rate ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 0.373 ## 2 Afghanistan 2000 2666 20595360 1.29 ## 3 Brazil 1999 37737 172006362 2.19 ## 4 Brazil 2000 80488 174504898 4.61 ## 5 China 1999 212258 1272915272 1.67 ## 6 China 2000 213766 1280428583 1.67 # perform the operations above for table 4a and 4b rate_1999 &lt;- (table4a$`1999` / table4b$`1999`)*10000 rate_2000 &lt;- (table4a$`2000` / table4b$`2000`)*10000 cbind (table4a, rate_1999, rate_2000) ## country 1999 2000 rate_1999 rate_2000 ## 1 Afghanistan 745 2666 0.372741 1.294466 ## 2 Brazil 37737 80488 2.193930 4.612363 ## 3 China 212258 213766 1.667495 1.669488 cbind (table4b, rate_1999, rate_2000) ## country 1999 2000 rate_1999 rate_2000 ## 1 Afghanistan 19987071 20595360 0.372741 1.294466 ## 2 Brazil 172006362 174504898 2.193930 4.612363 ## 3 China 1272915272 1280428583 1.667495 1.669488 3. Recreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first? First, we need to tidy table2 so that we can use it with ggplot2, using the spread() function. Then, we can use the ggplot() function in the same manner as in the book example. table2 %&gt;% spread(key = type, value = count) %&gt;% ggplot(aes(year, cases)) + geom_line(aes(group = country), colour = &quot;grey50&quot;) + geom_point(aes(colour = country)) 12.3.1 Notes - Spreading and Gathering If some column names are not names of variables, but rather values of variables, it will be helful to “gather” these columns into a single column and map the values accordingly. For example, table4a and table4b has values for 1999 and 2000 split across multiple columns. To gather the values, use gather() by specifying the names of the columns to gather, and the names of the variable that is being gathered (key) and the value (value) that is being redistributed. table4a %&gt;% gather(`1999`, `2000`, key = &quot;year&quot;, value = &quot;cases&quot;) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Brazil 1999 37737 ## 3 China 1999 212258 ## 4 Afghanistan 2000 2666 ## 5 Brazil 2000 80488 ## 6 China 2000 213766 table4b %&gt;% gather(`1999`, `2000`, key = &quot;year&quot;, value = &quot;population&quot;) ## # A tibble: 6 x 3 ## country year population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 19987071 ## 2 Brazil 1999 172006362 ## 3 China 1999 1272915272 ## 4 Afghanistan 2000 20595360 ## 5 Brazil 2000 174504898 ## 6 China 2000 1280428583 When an observation is spread across multiple rows, as in table2, you can spread the data into multiple columns such that each row contains data from a single observation. In table2, each observation is considered a country in a year. Cases and populations for each country/year observation can be split from the type column into their own columns. spread(table2, key = type, value = count) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 12.3.3 Exercises 1. Why are gather() and spread() not perfectly symmetrical? Carefully consider the following example: stocks &lt;- tibble( year = c(2015, 2015, 2016, 2016), half = c( 1, 2, 1, 2), return = c(1.88, 0.59, 0.92, 0.17) ) stocks ## # A tibble: 4 x 3 ## year half return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2016 1 0.92 ## 4 2016 2 0.17 stocks %&gt;% spread(year, return) ## # A tibble: 2 x 3 ## half `2015` `2016` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.88 0.92 ## 2 2 0.59 0.17 stocks %&gt;% spread(year, return) %&gt;% gather(&quot;year&quot;, &quot;return&quot;, `2015`:`2016`) ## # A tibble: 4 x 3 ## half year return ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2015 1.88 ## 2 2 2015 0.59 ## 3 1 2016 0.92 ## 4 2 2016 0.17 (Hint: look at the variable types and think about column names.) There are a couple differences between the unmodified stocks tibble and the spread and gathered tibble. The spread and gathered stocks tibble has the columns ordered differently. Also, the the types of each column have changed. The original tibble had 3 columns of type whereas the spread and gathered tibble has changed the year column to . The column names can also be changed by spreading and gathering, since they have to be specified as arguments in gather(). Both spread() and gather() have a convert argument. What does it do? If set to TRUE (the default is FALSE), then the function type.convert() with asis = TRUE will be run on each of the new columns in the spread() or gather() output. This will try to convert the columns to variables of the appropriate type. For example, using convert = TRUE on the example above causes the year column to become an rather than . stocks %&gt;% spread(year, return) %&gt;% gather(&quot;year&quot;, &quot;return&quot;, `2015`:`2016`, convert = TRUE) ## # A tibble: 4 x 3 ## half year return ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 2015 1.88 ## 2 2 2015 0.59 ## 3 1 2016 0.92 ## 4 2 2016 0.17 2. Why does this code fail? # table4a %&gt;% # gather(1999, 2000, key = &quot;year&quot;, value = &quot;cases&quot;) #&gt; Error in combine_vars(vars, ind_list): Position must be between 0 and n The code fails because the variables passed into gather(), 1999 and 2000, are not written correctly. They should be encased in backticks, like this: 1999 and 2000. The code below works. table4a %&gt;% gather(`1999`, `2000`, key = &quot;year&quot;, value = &quot;cases&quot;) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Brazil 1999 37737 ## 3 China 1999 212258 ## 4 Afghanistan 2000 2666 ## 5 Brazil 2000 80488 ## 6 China 2000 213766 3. Why does spreading this tibble fail? How could you add a new column to fix the problem? people &lt;- tribble( ~name, ~key, ~value, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) # spreading fails #spread(people, key, value) people2 &lt;- tribble( ~name, ~key, ~value, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods2&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) spread(people2, key, value) ## # A tibble: 3 x 3 ## name age height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jessica Cordero 37 156 ## 2 Phillip Woods 45 186 ## 3 Phillip Woods2 50 NA people3 &lt;- tribble( ~name, ~key, ~value, ~index, #-----------------|--------|------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, 1, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, 1, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, 2, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, 3, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156, 3 ) # works now spread(people3, key, value) ## # A tibble: 3 x 4 ## name index age height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jessica Cordero 3 37 156 ## 2 Phillip Woods 1 45 186 ## 3 Phillip Woods 2 50 NA Spreading the provided tribble fails because there is a duplicate value for the variables name and key. If I modify the second “Phillip Woods” value to “Phillip Woods2”, the spread() function works. If I add another column named “index” that identifies the unique sets of age and height, spread() also works. In any case, spread() requires a unique combination of variables to spread against. 4. Tidy the simple tibble below. Do you need to spread or gather it? What are the variables? preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) I would need to gather the data. We can combine the male and female designation into a column called “gender”, and gather the 4 numerical observations into a column called “cases”. Below is the code required to do this. preg %&gt;% gather(male, female, key = &quot;gender&quot;, value = &quot;cases&quot;) ## # A tibble: 4 x 3 ## pregnant gender cases ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 yes male NA ## 2 no male 20 ## 3 yes female 10 ## 4 no female 12 12.4 Notes - Separating and uniting In table3, the data for population and cases are combined into one column, rate. To separate them, we can use the separate() function in tidyr. In the opposite case, the function unite() can be used to combine an observation split across multiple columns. Do not confuse unite() with gather()! Separate() separates one column into multiple columns, by splitting based on a separator character. If the separator is the only non-numerical character for each observation, it does not have to be specified. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;)) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Similar to spread() and gather(), separate() has a convert argument that can try to convert the columns to the appropriate types. You can also specify the index to separate the values on, using a number instead of string value for the sep argument. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;), convert = TRUE) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table3 %&gt;% separate(year, into = c(&quot;century&quot;, &quot;year&quot;), sep = 2) ## # A tibble: 6 x 4 ## country century year rate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 19 99 745/19987071 ## 2 Afghanistan 20 00 2666/20595360 ## 3 Brazil 19 99 37737/172006362 ## 4 Brazil 20 00 80488/174504898 ## 5 China 19 99 212258/1272915272 ## 6 China 20 00 213766/1280428583 Unite() is the opposite of separating, and will combine two columns together using a default underscore “_“. You can change the underscore to your choice of separator, using and empty”&quot; if you do not wish to separate the values. The convert argument does not seem to work with this function in the example below. # use default separator for unite() table5 %&gt;% unite(new, century,year) ## # A tibble: 6 x 3 ## country new rate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 19_99 745/19987071 ## 2 Afghanistan 20_00 2666/20595360 ## 3 Brazil 19_99 37737/172006362 ## 4 Brazil 20_00 80488/174504898 ## 5 China 19_99 212258/1272915272 ## 6 China 20_00 213766/1280428583 # specify sep = &quot;&quot; to combine the two columns with no separation table5 %&gt;% unite(new, century, year, sep = &quot;&quot;) ## # A tibble: 6 x 3 ## country new rate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 12.4.3 Exercises 1. What do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) ## Warning: Expected 3 pieces. Additional pieces discarded in 1 rows [2]. ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 d e f ## 3 h i j tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 1 rows [2]. ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 d e &lt;NA&gt; ## 3 f g i In the examples above, the tibbles created are 1 column, 3 rows, with strings as observations. Within each string, commas separate the letters. The separate(x, c(“one”, “two”, “three”)) will try to split each string using the comma into three columns. However, in the first tibble, there is a row with four values separated by commas (an extra value). This is where the extra argument comes into play. The documentation for extra states that: * “warn” (the default): emit a warning and drop extra values. * “drop”: drop any extra values without a warning. * “merge”: only splits at most length(into) times For the second tibble, there are only two values separated by a comma in the second row (a missing value). This is where the fill argument comes into play. The documentation for fill states that: “warn” (the default): emit a warning and fill from the right “right”: fill with missing values on the right “left”: fill with missing values on the left The default for both extra and fill is “warn”, which just provides a warning about the missing or extra value. Below I will try the other two options on the appropriate tibles. # tibble with extra value tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), extra = &quot;drop&quot;) ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 d e f ## 3 h i j tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), extra = &quot;merge&quot;) ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 d e f,g ## 3 h i j # tible with missing value tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), fill = &quot;right&quot;) ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 d e &lt;NA&gt; ## 3 f g i tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), fill = &quot;left&quot;) ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 &lt;NA&gt; d e ## 3 f g i 2. Both unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE? The documentation states that “If TRUE, remove input column from output data frame.” The column with the values that were split is by default removed from the resulting table, but if this parameter is set to FALSE, the column remains. You might want to set it to FALSE if you want to have a record of what the original column looked like or want to separate the column a different way. Below is an exmaple of setting remove = FALSE on table3. We can see that the rate column still exists in the output. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;), convert = TRUE, remove = FALSE) ## # A tibble: 6 x 5 ## country year rate cases population ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745/19987071 745 19987071 ## 2 Afghanistan 2000 2666/20595360 2666 20595360 ## 3 Brazil 1999 37737/172006362 37737 172006362 ## 4 Brazil 2000 80488/174504898 80488 174504898 ## 5 China 1999 212258/1272915272 212258 1272915272 ## 6 China 2000 213766/1280428583 213766 1280428583 3. Compare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite? extract() is a version of separate() except you can specify which groups to include in the final output using regexes. For example, if you run the default extract() on a data frame, it will first separate based on a non-numeric delimiter, then only place the number of groups you specify in the into argument, starting from the left. separate() will throw an error if you specify less than the number of groups for the into argument, and by default want to return all the groups created. There are three variations of separation because the output can be flexible based on the type of separation used (there are fewer constraints on how a value can be split up). There is only one unite because combining values together has more constraints. Unless you can take bits and pieces of different variables to unite (and define how that occurs), the only thing you can define is the delimiter that is used when uniting the data. # separate errors if you try to separate into less columns than groups, but can still function. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;)) ## Warning: Expected 1 pieces. Additional pieces discarded in 6 rows [1, 2, 3, ## 4, 5, 6]. ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 # extract does not error table3 %&gt;% extract(rate, into = c(&quot;cases&quot;)) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 12.5 Notes - Missing Values Values in your dataset can be missing explicitly, in which they are specified as NA, or implicitly, in which they are not specified at all. The example provided is: stocks &lt;- tibble( year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), qtr = c( 1, 2, 3, 4, 2, 3, 4), return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) stocks ## # A tibble: 7 x 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 NA ## 5 2016 2 0.92 ## 6 2016 3 0.17 ## 7 2016 4 2.66 In which there is an explicitly missing value for year 2015 quarter 4, and an implicitly missing value for year 2016 quarter 1 (which is just not present in the data). We can expose the implicitly missing value by spreading the data: stocks %&gt;% spread(year, return) ## # A tibble: 4 x 3 ## qtr `2015` `2016` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.88 NA ## 2 2 0.59 0.92 ## 3 3 0.35 0.17 ## 4 4 NA 2.66 Now we see two explicitly missing values. We can also omit NA values entirely by using na.omit() or by specifying na.rm = T in a spread and gather combination. # get rid of NA values using na.omit() na.omit(stocks) ## # A tibble: 6 x 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2016 2 0.92 ## 5 2016 3 0.17 ## 6 2016 4 2.66 # get rid of NA values using spread and gather( na.rm = T ) stocks %&gt;% spread(year, return) %&gt;% gather(year, return, `2015`:`2016`, na.rm = TRUE) ## # A tibble: 6 x 3 ## qtr year return ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2015 1.88 ## 2 2 2015 0.59 ## 3 3 2015 0.35 ## 4 2 2016 0.92 ## 5 3 2016 0.17 ## 6 4 2016 2.66 A very useful tool is the complete() function in tidyr! It will look for all combinations of the variables specified and find implicitly missing values in the data, and turn them explicit. This is much cleaner-looking than spreading and gathering. stocks %&gt;% complete(year, qtr) ## # A tibble: 8 x 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 NA ## 5 2016 1 NA ## 6 2016 2 0.92 ## 7 2016 3 0.17 ## 8 2016 4 2.66 Another reason there might be missing values in your dataset is because data was only entered for the first occurance of an observation, and implied that the sequential observations would be for the same identifying entry (also called “carrying forward” values). Below is the example provided in the book, in which the treatment and response variables are associated with a person variable. However, patient names are only identified for their first occurance in the table. To make the table easier to work with, we can “carry forward” the names using fill(). treatment &lt;- tribble( ~ person, ~ treatment, ~response, &quot;Derrick Whitmore&quot;, 1, 7, NA, 2, 10, NA, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) treatment ## # A tibble: 4 x 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 &lt;NA&gt; 2 10 ## 3 &lt;NA&gt; 3 9 ## 4 Katherine Burke 1 4 treatment %&gt;% fill(person) ## # A tibble: 4 x 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 Derrick Whitmore 2 10 ## 3 Derrick Whitmore 3 9 ## 4 Katherine Burke 1 4 12.5.1 Exercises 1. Compare and contrast the fill arguments to spread() and complete(). For the spread() function, the documentation for the fill argument states that: “If set, missing values will be replaced with this value. Note that there are two types of missingness in the input: explicit missing values (i.e. NA), and implicit missings, rows that simply aren’t present. Both types of missing value will be replaced by fill.” In other words, you can replace the missing values with a value of your choice, specified by the fill argument. For example: stocks %&gt;% spread(year, return, fill = &quot;MISSING&quot;) ## # A tibble: 4 x 3 ## qtr `2015` `2016` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1.88 MISSING ## 2 2 0.59 0.92 ## 3 3 0.35 0.17 ## 4 4 MISSING 2.66 # turns out you can supply more than one value, but this doesnt seem to work that great. use complete() instead for multiple values. stocks %&gt;% spread(year, return, fill = c(&quot;NO_2015&quot;,&quot;NO_2016&quot;)) ## Warning in if (!is.na(fill)) {: the condition has length &gt; 1 and only the ## first element will be used ## # A tibble: 4 x 3 ## qtr `2015` `2016` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1.88 NO_2016 ## 2 2 0.59 0.92 ## 3 3 0.35 0.17 ## 4 4 NO_2015 2.66 For the complete() function, the documentation for the fill argument states that: “A named list that for each variable supplies a single value to use instead of NA for missing combinations.” In other words, you can supply the fill argument with a list that has one entry for each of the columns in your data frame. Depending on where the missing value is located, it will be replaced by the appropriate NA value. In the example below, I replace any NA values in the year column with “NO_YEAR”, in the qtr column with “NO_QTR”, and in the return column with “NO_RETURN”. stocks %&gt;% complete(year, qtr, fill = list(year = &quot;NO_YEAR&quot;, qtr = &quot;NO_QTR&quot;, return = &quot;NO_RETURN&quot;)) ## # A tibble: 8 x 3 ## year qtr return ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 NO_RETURN ## 5 2016 1 NO_RETURN ## 6 2016 2 0.92 ## 7 2016 3 0.17 ## 8 2016 4 2.66 2. What does the direction argument to fill() do? The documentation for the direction argument in fill() states: “Direction in which to fill missing values. Currently either”down&quot; (the default) or “up”.&quot; So if we specify direction = “up” for the example provided earlier, the NA values will be filled with the 2nd patient (Katherine Burke), not the 1st patient. treatment %&gt;% fill(person, .direction = &quot;up&quot;) ## # A tibble: 4 x 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 Katherine Burke 2 10 ## 3 Katherine Burke 3 9 ## 4 Katherine Burke 1 4 12.6 Notes - Case Study The case study presented uses the tidyr::who dataset, which is depicted below: who ## # A tibble: 7,240 x 60 ## country iso2 iso3 year new_sp_m014 new_sp_m1524 new_sp_m2534 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghan… AF AFG 1980 NA NA NA ## 2 Afghan… AF AFG 1981 NA NA NA ## 3 Afghan… AF AFG 1982 NA NA NA ## 4 Afghan… AF AFG 1983 NA NA NA ## 5 Afghan… AF AFG 1984 NA NA NA ## 6 Afghan… AF AFG 1985 NA NA NA ## 7 Afghan… AF AFG 1986 NA NA NA ## 8 Afghan… AF AFG 1987 NA NA NA ## 9 Afghan… AF AFG 1988 NA NA NA ## 10 Afghan… AF AFG 1989 NA NA NA ## # … with 7,230 more rows, and 53 more variables: new_sp_m3544 &lt;int&gt;, ## # new_sp_m4554 &lt;int&gt;, new_sp_m5564 &lt;int&gt;, new_sp_m65 &lt;int&gt;, ## # new_sp_f014 &lt;int&gt;, new_sp_f1524 &lt;int&gt;, new_sp_f2534 &lt;int&gt;, ## # new_sp_f3544 &lt;int&gt;, new_sp_f4554 &lt;int&gt;, new_sp_f5564 &lt;int&gt;, ## # new_sp_f65 &lt;int&gt;, new_sn_m014 &lt;int&gt;, new_sn_m1524 &lt;int&gt;, ## # new_sn_m2534 &lt;int&gt;, new_sn_m3544 &lt;int&gt;, new_sn_m4554 &lt;int&gt;, ## # new_sn_m5564 &lt;int&gt;, new_sn_m65 &lt;int&gt;, new_sn_f014 &lt;int&gt;, ## # new_sn_f1524 &lt;int&gt;, new_sn_f2534 &lt;int&gt;, new_sn_f3544 &lt;int&gt;, ## # new_sn_f4554 &lt;int&gt;, new_sn_f5564 &lt;int&gt;, new_sn_f65 &lt;int&gt;, ## # new_ep_m014 &lt;int&gt;, new_ep_m1524 &lt;int&gt;, new_ep_m2534 &lt;int&gt;, ## # new_ep_m3544 &lt;int&gt;, new_ep_m4554 &lt;int&gt;, new_ep_m5564 &lt;int&gt;, ## # new_ep_m65 &lt;int&gt;, new_ep_f014 &lt;int&gt;, new_ep_f1524 &lt;int&gt;, ## # new_ep_f2534 &lt;int&gt;, new_ep_f3544 &lt;int&gt;, new_ep_f4554 &lt;int&gt;, ## # new_ep_f5564 &lt;int&gt;, new_ep_f65 &lt;int&gt;, newrel_m014 &lt;int&gt;, ## # newrel_m1524 &lt;int&gt;, newrel_m2534 &lt;int&gt;, newrel_m3544 &lt;int&gt;, ## # newrel_m4554 &lt;int&gt;, newrel_m5564 &lt;int&gt;, newrel_m65 &lt;int&gt;, ## # newrel_f014 &lt;int&gt;, newrel_f1524 &lt;int&gt;, newrel_f2534 &lt;int&gt;, ## # newrel_f3544 &lt;int&gt;, newrel_f4554 &lt;int&gt;, newrel_f5564 &lt;int&gt;, ## # newrel_f65 &lt;int&gt; Below are the steps taken to “tidy” this dataset. The columns to the right of year are recorded cases of TB for each of the specified cohorts, which can be gathered. who1 &lt;- who %&gt;% gather(new_sp_m014:newrel_f65, key = &quot;key&quot;, value = &quot;cases&quot;, na.rm = TRUE) who1 ## # A tibble: 76,046 x 6 ## country iso2 iso3 year key cases ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan AF AFG 1997 new_sp_m014 0 ## 2 Afghanistan AF AFG 1998 new_sp_m014 30 ## 3 Afghanistan AF AFG 1999 new_sp_m014 8 ## 4 Afghanistan AF AFG 2000 new_sp_m014 52 ## 5 Afghanistan AF AFG 2001 new_sp_m014 129 ## 6 Afghanistan AF AFG 2002 new_sp_m014 90 ## 7 Afghanistan AF AFG 2003 new_sp_m014 127 ## 8 Afghanistan AF AFG 2004 new_sp_m014 139 ## 9 Afghanistan AF AFG 2005 new_sp_m014 151 ## 10 Afghanistan AF AFG 2006 new_sp_m014 193 ## # … with 76,036 more rows In order to prepare for using separate on “key”, we need to standardize the format of the values. Most of the values have two underscores: one after “new” and one before the age range. However, one cohort does not have the first underscore after “new”. The underscore can be added using str_replace(). who2 &lt;- who1 %&gt;% mutate(key = stringr::str_replace(key, &quot;newrel&quot;, &quot;new_rel&quot;)) who2 ## # A tibble: 76,046 x 6 ## country iso2 iso3 year key cases ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan AF AFG 1997 new_sp_m014 0 ## 2 Afghanistan AF AFG 1998 new_sp_m014 30 ## 3 Afghanistan AF AFG 1999 new_sp_m014 8 ## 4 Afghanistan AF AFG 2000 new_sp_m014 52 ## 5 Afghanistan AF AFG 2001 new_sp_m014 129 ## 6 Afghanistan AF AFG 2002 new_sp_m014 90 ## 7 Afghanistan AF AFG 2003 new_sp_m014 127 ## 8 Afghanistan AF AFG 2004 new_sp_m014 139 ## 9 Afghanistan AF AFG 2005 new_sp_m014 151 ## 10 Afghanistan AF AFG 2006 new_sp_m014 193 ## # … with 76,036 more rows Now that the format is standardized, the new, type, and sexage can be separated: who3 &lt;- who2 %&gt;% separate(key, c(&quot;new&quot;, &quot;type&quot;, &quot;sexage&quot;), sep = &quot;_&quot;) who3 ## # A tibble: 76,046 x 8 ## country iso2 iso3 year new type sexage cases ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan AF AFG 1997 new sp m014 0 ## 2 Afghanistan AF AFG 1998 new sp m014 30 ## 3 Afghanistan AF AFG 1999 new sp m014 8 ## 4 Afghanistan AF AFG 2000 new sp m014 52 ## 5 Afghanistan AF AFG 2001 new sp m014 129 ## 6 Afghanistan AF AFG 2002 new sp m014 90 ## 7 Afghanistan AF AFG 2003 new sp m014 127 ## 8 Afghanistan AF AFG 2004 new sp m014 139 ## 9 Afghanistan AF AFG 2005 new sp m014 151 ## 10 Afghanistan AF AFG 2006 new sp m014 193 ## # … with 76,036 more rows The iso2 and iso3 columns look like different identifiers for country, so these can be dropped. who3 %&gt;% count(new) ## # A tibble: 1 x 2 ## new n ## &lt;chr&gt; &lt;int&gt; ## 1 new 76046 who4 &lt;- who3 %&gt;% select(-new, -iso2, -iso3) The sexage column can be futher separated into sex and age: who5 &lt;- who4 %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) who5 ## # A tibble: 76,046 x 6 ## country year type sex age cases ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1997 sp m 014 0 ## 2 Afghanistan 1998 sp m 014 30 ## 3 Afghanistan 1999 sp m 014 8 ## 4 Afghanistan 2000 sp m 014 52 ## 5 Afghanistan 2001 sp m 014 129 ## 6 Afghanistan 2002 sp m 014 90 ## 7 Afghanistan 2003 sp m 014 127 ## 8 Afghanistan 2004 sp m 014 139 ## 9 Afghanistan 2005 sp m 014 151 ## 10 Afghanistan 2006 sp m 014 193 ## # … with 76,036 more rows We can combine all the steps above using the pipe: who %&gt;% gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %&gt;% mutate(code = stringr::str_replace(code, &quot;newrel&quot;, &quot;new_rel&quot;)) %&gt;% separate(code, c(&quot;new&quot;, &quot;var&quot;, &quot;sexage&quot;)) %&gt;% select(-new, -iso2, -iso3) %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) ## # A tibble: 76,046 x 6 ## country year var sex age value ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1997 sp m 014 0 ## 2 Afghanistan 1998 sp m 014 30 ## 3 Afghanistan 1999 sp m 014 8 ## 4 Afghanistan 2000 sp m 014 52 ## 5 Afghanistan 2001 sp m 014 129 ## 6 Afghanistan 2002 sp m 014 90 ## 7 Afghanistan 2003 sp m 014 127 ## 8 Afghanistan 2004 sp m 014 139 ## 9 Afghanistan 2005 sp m 014 151 ## 10 Afghanistan 2006 sp m 014 193 ## # … with 76,036 more rows 12.6.1 Exercises 1. In this case study I set na.rm = TRUE just to make it easier to check that we had the correct values. Is this reasonable? Think about how missing values are represented in this dataset. Are there implicit missing values? What’s the difference between an NA and zero? In some instances, the presence of an explicitly missing value might have meaning behind it. For example, maybe more missing values are present in data collected from districts in poor neighborhoods, because of staffing issues. In cases where knowing why missing values are present, it might not be good to use na.rm = TRUE. In this context, the presence of missing values might have meant that no cases were observed, or that no people were recorded in the sex/age group specified. However we do not know this for certain. For the purposes of teaching us how to tidy data, I would say that this was reasonable. To find the number of implicit missing values, first make version of the tidy who without removing NA values, then use complete() to convert implicitly missing values to explicitly missing values. We can then look at how many new rows with NA values were added by subtracting the # of rows (using nrow()). The difference between NA and zero is that the value of NA is unknown (it could have been anything, but the data wasn’t entered), and the value of 0 is that there were zero TB cases (this is a defined value that holds meaning). clean_who &lt;- who %&gt;% gather(code, value, new_sp_m014:newrel_f65) %&gt;% mutate(code = stringr::str_replace(code, &quot;newrel&quot;, &quot;new_rel&quot;)) %&gt;% separate(code, c(&quot;new&quot;, &quot;var&quot;, &quot;sexage&quot;)) %&gt;% select(-new, -iso2, -iso3) %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) # Use complete to convert implicity missing values into explicitly missing values complete_who &lt;- complete(clean_who, country, year) # Calculate how many implicitly missing values there are in the dataset by subtracting rows nrow(complete_who) - nrow(clean_who)[1] ## [1] 206 # count how many values are NA in the data set, how many are 0, and how many are neither NA or 0. clean_who %&gt;% count(is.na(value), value == 0) ## # A tibble: 3 x 3 ## `is.na(value)` `value == 0` n ## &lt;lgl&gt; &lt;lgl&gt; &lt;int&gt; ## 1 FALSE FALSE 64966 ## 2 FALSE TRUE 11080 ## 3 TRUE NA 329394 2. What happens if you neglect the mutate() step? (mutate(key = stringr::str_replace(key, “newrel”, “new_rel”))) The mutate() step is required to allow the separate() function to be applied in the same manner for all the values in the code column, because it changes instances of “newrel” to “new_rel”, which has an underscore after “new” like all the other entries in the column. Without the mutate() step, the rows with “newrel” in them will not be separated correctly by separate(). Below is the output after removing the mutate() step, which shows an error “Expected 3 pieces. Missing pieces filled with NA in 2580 rows…” who %&gt;% gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %&gt;% separate(code, c(&quot;new&quot;, &quot;var&quot;, &quot;sexage&quot;)) %&gt;% select(-new, -iso2, -iso3) %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 2580 rows ## [73467, 73468, 73469, 73470, 73471, 73472, 73473, 73474, 73475, 73476, ## 73477, 73478, 73479, 73480, 73481, 73482, 73483, 73484, 73485, 73486, ...]. ## # A tibble: 76,046 x 6 ## country year var sex age value ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1997 sp m 014 0 ## 2 Afghanistan 1998 sp m 014 30 ## 3 Afghanistan 1999 sp m 014 8 ## 4 Afghanistan 2000 sp m 014 52 ## 5 Afghanistan 2001 sp m 014 129 ## 6 Afghanistan 2002 sp m 014 90 ## 7 Afghanistan 2003 sp m 014 127 ## 8 Afghanistan 2004 sp m 014 139 ## 9 Afghanistan 2005 sp m 014 151 ## 10 Afghanistan 2006 sp m 014 193 ## # … with 76,036 more rows 3. I claimed that iso2 and iso3 were redundant with country. Confirm this claim. To confirm this claim, we can first identify the number of unique values in each of the three columns. Then, we can compare the number of unique combinations of country, iso2, and iso3 that are present in the data using group_by(). When we compare these metrics, they are all equal to 219, proving that iso2 and iso3 are reduntant with country. # see how many unique values for country, iso2, and iso3 there are length(unique(who$country)) ## [1] 219 length(unique(who$iso2)) ## [1] 219 length(unique(who$iso3)) ## [1] 219 # find out how many unique groupings of country, iso2, and iso3 there are who %&gt;% group_by(country, iso2, iso3) %&gt;% count() %&gt;% nrow() ## [1] 219 4. For each country, year, and sex compute the total number of cases of TB. Make an informative visualisation of the data. Below I use group_by() and summarize() to compute the total number of cases of TB by country, year, and sex. We can feed this into ggplot() to make a visualization. However, it is difficult to make something informative that combines all of these variables, due to the large number of countries and years to display. I tried to do this in the plot below, by depicting the number of TB cases over time, grouped by country, and faceted by sex. I had to remove the legend for the countries because it was occupying too much space. Looking at the plot, we can see two countries with high number of TB cases rising from 2000 and onwards, with a higher occurance in males compared to females. # store the tidy version of who in an object tidy_who &lt;- who %&gt;% gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %&gt;% mutate(code = stringr::str_replace(code, &quot;newrel&quot;, &quot;new_rel&quot;)) %&gt;% separate(code, c(&quot;new&quot;, &quot;var&quot;, &quot;sexage&quot;)) %&gt;% select(-new, -iso2, -iso3) %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) # count the total number of cases for each country, year and sex total_TB &lt;- tidy_who %&gt;% group_by(country, year, sex) %&gt;% summarize( total_TB_cases = sum(value, na.rm = T) ) # plot the data total_TB %&gt;% ggplot(aes (x = year, y = total_TB_cases))+ geom_line(aes(color = country))+ theme(legend.position=&quot;none&quot;)+ facet_wrap(~sex) # remove values before 1995 and then plot the data total_TB %&gt;% filter( year &gt; 1995 )%&gt;% ggplot(aes (x = year, y = total_TB_cases))+ geom_line(aes(color = country))+ theme(legend.position=&quot;none&quot;)+ facet_wrap(~sex) "],
["chapter-13-relational-data.html", "Chapter 13 - Relational data 13.2.1 Exercises 13.3 Notes - Keys 13.3.1 Exercises 13.4 Notes - Mutating Joins 13.4.6 Exercises 13.5 Notes - Filtering Joins 13.5.1 Exercises 13.6 &amp; 13.7 Notes - Join problems and set operations", " Chapter 13 - Relational data If you’ve ever worked with SQL before, this chapter should be relatively familiar to you. Here we learn about the different ways to work with relational data in R, using dplyr functions. Quoted from the book, the different types of thins you can perform are: “Mutating joins, which add new variables to one data frame from matching observations in another.” “Filtering joins, which filter observations from one data frame based on whether or not they match an observation in the other table.” “Set operations, which treat observations as if they were set elements.” library(&quot;tidyverse&quot;) library(&quot;nycflights13&quot;) The flights dataset in nycflights13 has tibbles that can be used to practice relational data management: # the complete flights table flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # info on airlines airlines ## # A tibble: 16 x 2 ## carrier name ## &lt;chr&gt; &lt;chr&gt; ## 1 9E Endeavor Air Inc. ## 2 AA American Airlines Inc. ## 3 AS Alaska Airlines Inc. ## 4 B6 JetBlue Airways ## 5 DL Delta Air Lines Inc. ## 6 EV ExpressJet Airlines Inc. ## 7 F9 Frontier Airlines Inc. ## 8 FL AirTran Airways Corporation ## 9 HA Hawaiian Airlines Inc. ## 10 MQ Envoy Air ## 11 OO SkyWest Airlines Inc. ## 12 UA United Air Lines Inc. ## 13 US US Airways Inc. ## 14 VX Virgin America ## 15 WN Southwest Airlines Co. ## 16 YV Mesa Airlines Inc. # info on airports airports ## # A tibble: 1,458 x 8 ## faa name lat lon alt tz dst tzone ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 04G Lansdowne Airport 41.1 -80.6 1044 -5 A America/New_… ## 2 06A Moton Field Municipa… 32.5 -85.7 264 -6 A America/Chic… ## 3 06C Schaumburg Regional 42.0 -88.1 801 -6 A America/Chic… ## 4 06N Randall Airport 41.4 -74.4 523 -5 A America/New_… ## 5 09J Jekyll Island Airport 31.1 -81.4 11 -5 A America/New_… ## 6 0A9 Elizabethton Municip… 36.4 -82.2 1593 -5 A America/New_… ## 7 0G6 Williams County Airp… 41.5 -84.5 730 -5 A America/New_… ## 8 0G7 Finger Lakes Regiona… 42.9 -76.8 492 -5 A America/New_… ## 9 0P2 Shoestring Aviation … 39.8 -76.6 1000 -5 U America/New_… ## 10 0S9 Jefferson County Intl 48.1 -123. 108 -8 A America/Los_… ## # … with 1,448 more rows # info on individual planes planes ## # A tibble: 3,322 x 9 ## tailnum year type manufacturer model engines seats speed engine ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 N10156 2004 Fixed win… EMBRAER EMB-1… 2 55 NA Turbo… ## 2 N102UW 1998 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## 3 N103US 1999 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## 4 N104UW 1999 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## 5 N10575 2002 Fixed win… EMBRAER EMB-1… 2 55 NA Turbo… ## 6 N105UW 1999 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## 7 N107US 1999 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## 8 N108UW 1999 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## 9 N109UW 1999 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## 10 N110UW 1999 Fixed win… AIRBUS INDUS… A320-… 2 182 NA Turbo… ## # … with 3,312 more rows # weater information weather ## # A tibble: 26,115 x 15 ## origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 EWR 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 EWR 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 EWR 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 EWR 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with 26,105 more rows, and 5 more variables: wind_gust &lt;dbl&gt;, ## # precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; flights connects to planes via a single variable, tailnum. flights connects to airlines through the carrier variable. flights connects to airports in two ways: via the origin and dest variables. flights connects to weather via origin (the location), and year, month, day and hour (the time). 13.2.1 Exercises 1. Imagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine? You would need the variables tailnum, which defines individual planes, and the latitude and longitude values of the origin and destination variables. To do this, you can combine the flights table, which connects tailnum to origin and destination, with the airports table, which contains the latitude and longitude of each airport. Here is the join that can be performed: flights_1 &lt;- left_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) flights_2 &lt;- left_join(flights_1, airports, by = c(&quot;origin&quot; = &quot;faa&quot;)) flights_2 ## # A tibble: 336,776 x 33 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 26 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name.x &lt;chr&gt;, lat.x &lt;dbl&gt;, ## # lon.x &lt;dbl&gt;, alt.x &lt;int&gt;, tz.x &lt;dbl&gt;, dst.x &lt;chr&gt;, tzone.x &lt;chr&gt;, ## # name.y &lt;chr&gt;, lat.y &lt;dbl&gt;, lon.y &lt;dbl&gt;, alt.y &lt;int&gt;, tz.y &lt;dbl&gt;, ## # dst.y &lt;chr&gt;, tzone.y &lt;chr&gt; 2. I forgot to draw the relationship between weather and airports. What is the relationship and how should it appear in the diagram? The airports table can be joined to the weather table, in which the primary key is origin in the weather table and the foreign key is the faa column in the airports table. For each observation in weather, information about the airport can be appended as a new column. For example: weather_airports &lt;- left_join(weather, airports, by = c(&quot;origin&quot; = &quot;faa&quot;)) weather_airports ## # A tibble: 26,115 x 22 ## origin year month day hour temp dewp humid wind_dir wind_speed ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 ## 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 ## 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 ## 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 ## 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 ## 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 ## 7 EWR 2013 1 1 7 39.0 28.0 64.4 240 15.0 ## 8 EWR 2013 1 1 8 39.9 28.0 62.2 250 10.4 ## 9 EWR 2013 1 1 9 39.9 28.0 62.2 260 15.0 ## 10 EWR 2013 1 1 10 41 28.0 59.6 260 13.8 ## # … with 26,105 more rows, and 12 more variables: wind_gust &lt;dbl&gt;, ## # precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # name &lt;chr&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;int&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, ## # tzone &lt;chr&gt; 3. weather only contains information for the origin (NYC) airports. If it contained weather records for all airports in the USA, what additional relation would it define with flights? If it contained weather records for all airports, it could be joined with flights using the primary key consisting of the columns year, month, day, hour and origin in weather, which could be connected with the foreign key year, month, day, hour, and either origin or dest in flights. 4. We know that some days of the year are “special”, and fewer people than usual fly on them. How might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables? You could have a separate data frame containing the variables month, day, and “special”, in which the “special” days would be designated TRUE. This could be connected to the existing tables using the primary key consisting of month, and day (should probably include the year as well, depending on the holiday). The value for the “special” column would be appended to the corresponding foriegn key values as TRUE or FALSE. For example, below is a table containing some special days of the year (Christmas, New Years), that is joined to the flights table. special &lt;- tribble ( ~month, ~day, ~event, 12, 25, TRUE, 1, 1, TRUE ) left_join(flights, special) %&gt;% select (year, month, day, flight, event) %&gt;% mutate ( event = (!is.na(event))) ## Joining, by = c(&quot;month&quot;, &quot;day&quot;) ## # A tibble: 336,776 x 5 ## year month day flight event ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; ## 1 2013 1 1 1545 TRUE ## 2 2013 1 1 1714 TRUE ## 3 2013 1 1 1141 TRUE ## 4 2013 1 1 725 TRUE ## 5 2013 1 1 461 TRUE ## 6 2013 1 1 1696 TRUE ## 7 2013 1 1 507 TRUE ## 8 2013 1 1 5708 TRUE ## 9 2013 1 1 79 TRUE ## 10 2013 1 1 301 TRUE ## # … with 336,766 more rows 13.3 Notes - Keys A key is a value or combination of values that uniquely identifies an observation. From the book, “A primary key uniquely identifies an observation in its own table. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table.” “A foreign key uniquely identifies an observation in another table. For example, the flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane.” You can verify that a variable can serve as a primary key by verifying that they do uniquely identify each observation. The book uses a combination of count() and filter(). If there are any values with count &gt; 1, the variable is not a primary key. planes %&gt;% count(tailnum) %&gt;% filter(n &gt; 1) ## # A tibble: 0 x 2 ## # … with 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt; weather %&gt;% count(year, month, day, hour, origin) %&gt;% filter(n &gt; 1) ## # A tibble: 3 x 6 ## year month day hour origin n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2013 11 3 1 EWR 2 ## 2 2013 11 3 1 JFK 2 ## 3 2013 11 3 1 LGA 2 If there are no variables in the dataset that can serve as primary keys, we can create a primary key with mutate() and row_number(). This is called a surrogate key. # combination of year, month, day, flight is not adequate as a primary key. # A surrogate key must be added. See 13.3.1 exercises for an example of adding a surrogate key. flights %&gt;% count(year, month, day, flight) %&gt;% filter(n &gt; 1) ## # A tibble: 29,768 x 5 ## year month day flight n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 1 2 ## 2 2013 1 1 3 2 ## 3 2013 1 1 4 2 ## 4 2013 1 1 11 3 ## 5 2013 1 1 15 2 ## 6 2013 1 1 21 2 ## 7 2013 1 1 27 4 ## 8 2013 1 1 31 2 ## 9 2013 1 1 32 2 ## 10 2013 1 1 35 2 ## # … with 29,758 more rows How a primary key from one table matches a foreign key in another table is what is called the relation. Depending on how they match, we can perform joins or other set operations, based on the values present in both keys. 13.3.1 Exercises 1. Add a surrogate key to flights. # add a surrogate key to flights, display vs flight flights %&gt;% transmute (flight, surrogate_key = row_number() ) ## # A tibble: 336,776 x 2 ## flight surrogate_key ## &lt;int&gt; &lt;int&gt; ## 1 1545 1 ## 2 1714 2 ## 3 1141 3 ## 4 725 4 ## 5 461 5 ## 6 1696 6 ## 7 507 7 ## 8 5708 8 ## 9 79 9 ## 10 301 10 ## # … with 336,766 more rows 2. Identify the keys in the following datasets: We can determine the primary keys by looking for unique groupings of variables that match the total number of observations in each table using group_by() and count(), and piping the ouput either to nrow() or filtering for groups that have n&gt;1. If nrow() is equal to the total observations, or if filter(n&gt;1) yields zero observations, then we have found the primary key. The primary key for Lahman::Batting is the combination of playerID, yearID, and stint. The primary key for babynames::babynames is year, sex, and name. The primary key for nasaweather::atmos is lat, long, year, and month. The primary key for fueleconomy::vehicles is id. For ggplot2::diamonds, there is no primary key. We can generate a surrogate key using mutate() and row_number(). head(Lahman::Batting) ## playerID yearID stint teamID lgID G AB R H X2B X3B HR RBI SB CS BB ## 1 abercda01 1871 1 TRO NA 1 4 0 0 0 0 0 0 0 0 0 ## 2 addybo01 1871 1 RC1 NA 25 118 30 32 6 0 0 13 8 1 4 ## 3 allisar01 1871 1 CL1 NA 29 137 28 40 4 5 0 19 3 1 2 ## 4 allisdo01 1871 1 WS3 NA 27 133 28 44 10 2 2 27 1 1 0 ## 5 ansonca01 1871 1 RC1 NA 25 120 29 39 11 3 0 16 6 2 2 ## 6 armstbo01 1871 1 FW1 NA 12 49 9 11 2 1 0 5 0 1 0 ## SO IBB HBP SH SF GIDP ## 1 0 NA NA NA NA NA ## 2 0 NA NA NA NA NA ## 3 5 NA NA NA NA NA ## 4 2 NA NA NA NA NA ## 5 1 NA NA NA NA NA ## 6 1 NA NA NA NA NA # method 1 dim(Lahman::Batting) ## [1] 102816 22 group_by(Lahman::Batting, playerID, yearID,stint) %&gt;% count() %&gt;% nrow() ## [1] 102816 # method 2 group_by(Lahman::Batting, playerID, yearID,stint) %&gt;% count() %&gt;% nrow() ## [1] 102816 babynames::babynames ## # A tibble: 1,924,665 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 ## 7 1880 F Ida 1472 0.0151 ## 8 1880 F Alice 1414 0.0145 ## 9 1880 F Bertha 1320 0.0135 ## 10 1880 F Sarah 1288 0.0132 ## # … with 1,924,655 more rows dim(babynames::babynames) ## [1] 1924665 5 group_by(babynames::babynames, year, sex,name) %&gt;% count() %&gt;% nrow() ## [1] 1924665 nasaweather::atmos ## # A tibble: 41,472 x 11 ## lat long year month surftemp temp pressure ozone cloudlow cloudmid ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 36.2 -114. 1995 1 273. 272. 835 304 7.5 34.5 ## 2 33.7 -114. 1995 1 280. 282. 940 304 11.5 32.5 ## 3 31.2 -114. 1995 1 285. 285. 960 298 16.5 26 ## 4 28.7 -114. 1995 1 289. 291. 990 276 20.5 14.5 ## 5 26.2 -114. 1995 1 292. 293. 1000 274 26 10.5 ## 6 23.7 -114. 1995 1 294. 294. 1000 264 30 9.5 ## 7 21.2 -114. 1995 1 295 295. 1000 258 29.5 11 ## 8 18.7 -114. 1995 1 298. 297. 1000 252 26.5 17.5 ## 9 16.2 -114. 1995 1 300. 298. 1000 250 27.5 18.5 ## 10 13.7 -114. 1995 1 300. 299. 1000 250 26 16.5 ## # … with 41,462 more rows, and 1 more variable: cloudhigh &lt;dbl&gt; dim(nasaweather::atmos) ## [1] 41472 11 group_by(nasaweather::atmos, lat, long, year, month) %&gt;% count() %&gt;% nrow() ## [1] 41472 fueleconomy::vehicles ## # A tibble: 33,442 x 12 ## id make model year class trans drive cyl displ fuel hwy cty ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 27550 AM Ge… DJ P… 1984 Spec… Auto… 2-Wh… 4 2.5 Regu… 17 18 ## 2 28426 AM Ge… DJ P… 1984 Spec… Auto… 2-Wh… 4 2.5 Regu… 17 18 ## 3 27549 AM Ge… FJ8c… 1984 Spec… Auto… 2-Wh… 6 4.2 Regu… 13 13 ## 4 28425 AM Ge… FJ8c… 1984 Spec… Auto… 2-Wh… 6 4.2 Regu… 13 13 ## 5 1032 AM Ge… Post… 1985 Spec… Auto… Rear… 4 2.5 Regu… 17 16 ## 6 1033 AM Ge… Post… 1985 Spec… Auto… Rear… 6 4.2 Regu… 13 13 ## 7 3347 ASC I… GNX 1987 Mids… Auto… Rear… 6 3.8 Prem… 21 14 ## 8 13309 Acura 2.2C… 1997 Subc… Auto… Fron… 4 2.2 Regu… 26 20 ## 9 13310 Acura 2.2C… 1997 Subc… Manu… Fron… 4 2.2 Regu… 28 22 ## 10 13311 Acura 2.2C… 1997 Subc… Auto… Fron… 6 3 Regu… 26 18 ## # … with 33,432 more rows dim(fueleconomy::vehicles) ## [1] 33442 12 group_by(fueleconomy::vehicles, id) %&gt;% count() %&gt;% nrow() ## [1] 33442 ggplot2::diamonds ## # A tibble: 53,940 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # … with 53,930 more rows diamonds_withkey &lt;- ggplot2::diamonds %&gt;% mutate (surrogate_key = row_number()) diamonds_withkey ## # A tibble: 53,940 x 11 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Prem… E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Prem… I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very… J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very… I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very… H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very… H VS1 59.4 61 338 4 4.05 2.39 ## # … with 53,930 more rows, and 1 more variable: surrogate_key &lt;int&gt; 3. Draw a diagram illustrating the connections between the Batting, Master, and Salaries tables in the Lahman package. Draw another diagram that shows the relationship between Master, Managers, AwardsManagers. How would you characterise the relationship between the Batting, Pitching, and Fielding tables? Batting connects to Master using playerID, and with Salaries through playerID, yearID, teamID, and lgID. Master connects with Salaries through playerID. Master connects with Managers and with AwardsManagers through playerID. Managers connects with AwardsManagers using playerID, year, and lgID. Batting, Pitching, and Fielding all connect with each other through playerID, yearID, stint, teamID, and lgID. They are just variables for this key categorized into different tables based on the type of play. You could also just use playerID, yearID, and stint to get a unique key for these tables. library(Lahman) as_tibble(Batting) ## # A tibble: 102,816 x 22 ## playerID yearID stint teamID lgID G AB R H X2B X3B ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 abercda… 1871 1 TRO NA 1 4 0 0 0 0 ## 2 addybo01 1871 1 RC1 NA 25 118 30 32 6 0 ## 3 allisar… 1871 1 CL1 NA 29 137 28 40 4 5 ## 4 allisdo… 1871 1 WS3 NA 27 133 28 44 10 2 ## 5 ansonca… 1871 1 RC1 NA 25 120 29 39 11 3 ## 6 armstbo… 1871 1 FW1 NA 12 49 9 11 2 1 ## 7 barkeal… 1871 1 RC1 NA 1 4 0 1 0 0 ## 8 barnero… 1871 1 BS1 NA 31 157 66 63 10 9 ## 9 barrebi… 1871 1 FW1 NA 1 5 1 1 1 0 ## 10 barrofr… 1871 1 BS1 NA 18 86 13 13 2 1 ## # … with 102,806 more rows, and 11 more variables: HR &lt;int&gt;, RBI &lt;int&gt;, ## # SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;, IBB &lt;int&gt;, HBP &lt;int&gt;, ## # SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt; as_tibble(Master) ## # A tibble: 19,105 x 26 ## playerID birthYear birthMonth birthDay birthCountry birthState birthCity ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 aardsda… 1981 12 27 USA CO Denver ## 2 aaronha… 1934 2 5 USA AL Mobile ## 3 aaronto… 1939 8 5 USA AL Mobile ## 4 aasedo01 1954 9 8 USA CA Orange ## 5 abadan01 1972 8 25 USA FL Palm Bea… ## 6 abadfe01 1985 12 17 D.R. La Romana La Romana ## 7 abadijo… 1850 11 4 USA PA Philadel… ## 8 abbated… 1877 4 15 USA PA Latrobe ## 9 abbeybe… 1869 11 11 USA VT Essex ## 10 abbeych… 1866 10 14 USA NE Falls Ci… ## # … with 19,095 more rows, and 19 more variables: deathYear &lt;int&gt;, ## # deathMonth &lt;int&gt;, deathDay &lt;int&gt;, deathCountry &lt;chr&gt;, ## # deathState &lt;chr&gt;, deathCity &lt;chr&gt;, nameFirst &lt;chr&gt;, nameLast &lt;chr&gt;, ## # nameGiven &lt;chr&gt;, weight &lt;int&gt;, height &lt;int&gt;, bats &lt;fct&gt;, throws &lt;fct&gt;, ## # debut &lt;chr&gt;, finalGame &lt;chr&gt;, retroID &lt;chr&gt;, bbrefID &lt;chr&gt;, ## # deathDate &lt;date&gt;, birthDate &lt;date&gt; as_tibble(Salaries) ## # A tibble: 26,428 x 5 ## yearID teamID lgID playerID salary ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 1985 ATL NL barkele01 870000 ## 2 1985 ATL NL bedrost01 550000 ## 3 1985 ATL NL benedbr01 545000 ## 4 1985 ATL NL campri01 633333 ## 5 1985 ATL NL ceronri01 625000 ## 6 1985 ATL NL chambch01 800000 ## 7 1985 ATL NL dedmoje01 150000 ## 8 1985 ATL NL forstte01 483333 ## 9 1985 ATL NL garbege01 772000 ## 10 1985 ATL NL harpete01 250000 ## # … with 26,418 more rows as_tibble(Managers) ## # A tibble: 3,436 x 10 ## playerID yearID teamID lgID inseason G W L rank plyrMgr ## &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; ## 1 wrighha01 1871 BS1 NA 1 31 20 10 3 Y ## 2 woodji01 1871 CH1 NA 1 28 19 9 2 Y ## 3 paborch01 1871 CL1 NA 1 29 10 19 8 Y ## 4 lennobi01 1871 FW1 NA 1 14 5 9 8 Y ## 5 deaneha01 1871 FW1 NA 2 5 2 3 8 Y ## 6 fergubo01 1871 NY2 NA 1 33 16 17 5 Y ## 7 mcbridi01 1871 PH1 NA 1 28 21 7 1 Y ## 8 hastisc01 1871 RC1 NA 1 25 4 21 9 Y ## 9 pikeli01 1871 TRO NA 1 4 1 3 6 Y ## 10 cravebi01 1871 TRO NA 2 25 12 12 6 Y ## # … with 3,426 more rows as_tibble(AwardsManagers) ## # A tibble: 179 x 6 ## playerID awardID yearID lgID tie notes ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 larusto01 BBWAA Manager of the Year 1983 AL &lt;NA&gt; NA ## 2 lasorto01 BBWAA Manager of the Year 1983 NL &lt;NA&gt; NA ## 3 andersp01 BBWAA Manager of the Year 1984 AL &lt;NA&gt; NA ## 4 freyji99 BBWAA Manager of the Year 1984 NL &lt;NA&gt; NA ## 5 coxbo01 BBWAA Manager of the Year 1985 AL &lt;NA&gt; NA ## 6 herzowh01 BBWAA Manager of the Year 1985 NL &lt;NA&gt; NA ## 7 mcnamjo99 BBWAA Manager of the Year 1986 AL &lt;NA&gt; NA ## 8 lanieha01 BBWAA Manager of the Year 1986 NL &lt;NA&gt; NA ## 9 andersp01 BBWAA Manager of the Year 1987 AL &lt;NA&gt; NA ## 10 rodgebu01 BBWAA Manager of the Year 1987 NL &lt;NA&gt; NA ## # … with 169 more rows as_tibble(Pitching) ## # A tibble: 44,963 x 30 ## playerID yearID stint teamID lgID W L G GS CG SHO ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 bechtge… 1871 1 PH1 NA 1 2 3 3 2 0 ## 2 brainas… 1871 1 WS3 NA 12 15 30 30 30 0 ## 3 fergubo… 1871 1 NY2 NA 0 0 1 0 0 0 ## 4 fishech… 1871 1 RC1 NA 4 16 24 24 22 1 ## 5 fleetfr… 1871 1 NY2 NA 0 1 1 1 1 0 ## 6 flowedi… 1871 1 TRO NA 0 0 1 0 0 0 ## 7 mackde01 1871 1 RC1 NA 0 1 3 1 1 0 ## 8 mathebo… 1871 1 FW1 NA 6 11 19 19 19 1 ## 9 mcbridi… 1871 1 PH1 NA 18 5 25 25 25 0 ## 10 mcmuljo… 1871 1 TRO NA 12 15 29 29 28 0 ## # … with 44,953 more rows, and 19 more variables: SV &lt;int&gt;, IPouts &lt;int&gt;, ## # H &lt;int&gt;, ER &lt;int&gt;, HR &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;, BAOpp &lt;dbl&gt;, ## # ERA &lt;dbl&gt;, IBB &lt;int&gt;, WP &lt;int&gt;, HBP &lt;int&gt;, BK &lt;int&gt;, BFP &lt;int&gt;, ## # GF &lt;int&gt;, R &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt; as_tibble(Fielding) ## # A tibble: 136,815 x 18 ## playerID yearID stint teamID lgID POS G GS InnOuts PO A ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 abercda… 1871 1 TRO NA SS 1 NA NA 1 3 ## 2 addybo01 1871 1 RC1 NA 2B 22 NA NA 67 72 ## 3 addybo01 1871 1 RC1 NA SS 3 NA NA 8 14 ## 4 allisar… 1871 1 CL1 NA 2B 2 NA NA 1 4 ## 5 allisar… 1871 1 CL1 NA OF 29 NA NA 51 3 ## 6 allisdo… 1871 1 WS3 NA C 27 NA NA 68 15 ## 7 ansonca… 1871 1 RC1 NA 1B 1 NA NA 7 0 ## 8 ansonca… 1871 1 RC1 NA 2B 2 NA NA 3 4 ## 9 ansonca… 1871 1 RC1 NA 3B 20 NA NA 38 52 ## 10 ansonca… 1871 1 RC1 NA C 5 NA NA 10 0 ## # … with 136,805 more rows, and 7 more variables: E &lt;int&gt;, DP &lt;int&gt;, ## # PB &lt;int&gt;, WP &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, ZR &lt;int&gt; Batting %&gt;% group_by(playerID, yearID, stint) %&gt;% count() %&gt;% filter(n&gt;1) ## # A tibble: 0 x 4 ## # Groups: playerID, yearID, stint [0] ## # … with 4 variables: playerID &lt;chr&gt;, yearID &lt;int&gt;, stint &lt;int&gt;, n &lt;int&gt; 13.4 Notes - Mutating Joins This section on mutating joins introduces the usage of the join functions and provides examples of how they could be used. Below are the provided examples. # make a truncated dataset to work with flights2 &lt;- flights %&gt;% select(year:day, hour, origin, dest, tailnum, carrier) flights2 ## # A tibble: 336,776 x 8 ## year month day hour origin dest tailnum carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 1 5 EWR IAH N14228 UA ## 2 2013 1 1 5 LGA IAH N24211 UA ## 3 2013 1 1 5 JFK MIA N619AA AA ## 4 2013 1 1 5 JFK BQN N804JB B6 ## 5 2013 1 1 6 LGA ATL N668DN DL ## 6 2013 1 1 5 EWR ORD N39463 UA ## 7 2013 1 1 6 EWR FLL N516JB B6 ## 8 2013 1 1 6 LGA IAD N829AS EV ## 9 2013 1 1 6 JFK MCO N593JB B6 ## 10 2013 1 1 6 LGA ORD N3ALAA AA ## # … with 336,766 more rows Comparing how to perform joins using dplyr or baseR: # left join using left_join() from dplyr flights2 %&gt;% select(-origin, -dest) %&gt;% left_join(airlines, by = &quot;carrier&quot;) ## # A tibble: 336,776 x 7 ## year month day hour tailnum carrier name ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 1 5 N14228 UA United Air Lines Inc. ## 2 2013 1 1 5 N24211 UA United Air Lines Inc. ## 3 2013 1 1 5 N619AA AA American Airlines Inc. ## 4 2013 1 1 5 N804JB B6 JetBlue Airways ## 5 2013 1 1 6 N668DN DL Delta Air Lines Inc. ## 6 2013 1 1 5 N39463 UA United Air Lines Inc. ## 7 2013 1 1 6 N516JB B6 JetBlue Airways ## 8 2013 1 1 6 N829AS EV ExpressJet Airlines Inc. ## 9 2013 1 1 6 N593JB B6 JetBlue Airways ## 10 2013 1 1 6 N3ALAA AA American Airlines Inc. ## # … with 336,766 more rows # left join using base R commands and mutate() flights2 %&gt;% select(-origin, -dest) %&gt;% mutate(name = airlines$name[match(carrier, airlines$carrier)]) ## # A tibble: 336,776 x 7 ## year month day hour tailnum carrier name ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 1 5 N14228 UA United Air Lines Inc. ## 2 2013 1 1 5 N24211 UA United Air Lines Inc. ## 3 2013 1 1 5 N619AA AA American Airlines Inc. ## 4 2013 1 1 5 N804JB B6 JetBlue Airways ## 5 2013 1 1 6 N668DN DL Delta Air Lines Inc. ## 6 2013 1 1 5 N39463 UA United Air Lines Inc. ## 7 2013 1 1 6 N516JB B6 JetBlue Airways ## 8 2013 1 1 6 N829AS EV ExpressJet Airlines Inc. ## 9 2013 1 1 6 N593JB B6 JetBlue Airways ## 10 2013 1 1 6 N3ALAA AA American Airlines Inc. ## # … with 336,766 more rows There are two classes of joins: inner joins, in which only observations with common key values are provided in the joined output, or outer joins, in which all observations from one or more of the tables are kept, in the joined output, and the rest of the missing values are filled in with NA. From the text, the types of outer joins are defined as: “A left join keeps all observations in x.” “A right join keeps all observations in y.” “A full join keeps all observations in x and y.” These joins can be performed using dplyr commands (left_join(), right_join(), full_join(), inner_join(), semi_join(), anti_join()) or using the base R command merge(). The “by” argument is important to modify depending on the type of join you want to perform and the keys you want to use between the two tables. The book provides the examples below on the different ways the “by” argument can be used. # left join, if &quot;by&quot; argument is unspecified, will perform natural join flights2 %&gt;% left_join(weather) ## Joining, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;origin&quot;) ## # A tibble: 336,776 x 18 ## year month day hour origin dest tailnum carrier temp dewp humid ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 5 EWR IAH N14228 UA 39.0 28.0 64.4 ## 2 2013 1 1 5 LGA IAH N24211 UA 39.9 25.0 54.8 ## 3 2013 1 1 5 JFK MIA N619AA AA 39.0 27.0 61.6 ## 4 2013 1 1 5 JFK BQN N804JB B6 39.0 27.0 61.6 ## 5 2013 1 1 6 LGA ATL N668DN DL 39.9 25.0 54.8 ## 6 2013 1 1 5 EWR ORD N39463 UA 39.0 28.0 64.4 ## 7 2013 1 1 6 EWR FLL N516JB B6 37.9 28.0 67.2 ## 8 2013 1 1 6 LGA IAD N829AS EV 39.9 25.0 54.8 ## 9 2013 1 1 6 JFK MCO N593JB B6 37.9 27.0 64.3 ## 10 2013 1 1 6 LGA ORD N3ALAA AA 39.9 25.0 54.8 ## # … with 336,766 more rows, and 7 more variables: wind_dir &lt;dbl&gt;, ## # wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, ## # visib &lt;dbl&gt;, time_hour &lt;dttm&gt; # left join specifying which column to use as key flights2 %&gt;% left_join(planes, by = &quot;tailnum&quot;) ## # A tibble: 336,776 x 16 ## year.x month day hour origin dest tailnum carrier year.y type ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 2013 1 1 5 EWR IAH N14228 UA 1999 Fixe… ## 2 2013 1 1 5 LGA IAH N24211 UA 1998 Fixe… ## 3 2013 1 1 5 JFK MIA N619AA AA 1990 Fixe… ## 4 2013 1 1 5 JFK BQN N804JB B6 2012 Fixe… ## 5 2013 1 1 6 LGA ATL N668DN DL 1991 Fixe… ## 6 2013 1 1 5 EWR ORD N39463 UA 2012 Fixe… ## 7 2013 1 1 6 EWR FLL N516JB B6 2000 Fixe… ## 8 2013 1 1 6 LGA IAD N829AS EV 1998 Fixe… ## 9 2013 1 1 6 JFK MCO N593JB B6 2004 Fixe… ## 10 2013 1 1 6 LGA ORD N3ALAA AA NA &lt;NA&gt; ## # … with 336,766 more rows, and 6 more variables: manufacturer &lt;chr&gt;, ## # model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt; # left join specifying column to use as key, if named differently in the two tables flights2 %&gt;% left_join(airports, c(&quot;dest&quot; = &quot;faa&quot;)) ## # A tibble: 336,776 x 15 ## year month day hour origin dest tailnum carrier name lat lon ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 5 EWR IAH N14228 UA Geor… 30.0 -95.3 ## 2 2013 1 1 5 LGA IAH N24211 UA Geor… 30.0 -95.3 ## 3 2013 1 1 5 JFK MIA N619AA AA Miam… 25.8 -80.3 ## 4 2013 1 1 5 JFK BQN N804JB B6 &lt;NA&gt; NA NA ## 5 2013 1 1 6 LGA ATL N668DN DL Hart… 33.6 -84.4 ## 6 2013 1 1 5 EWR ORD N39463 UA Chic… 42.0 -87.9 ## 7 2013 1 1 6 EWR FLL N516JB B6 Fort… 26.1 -80.2 ## 8 2013 1 1 6 LGA IAD N829AS EV Wash… 38.9 -77.5 ## 9 2013 1 1 6 JFK MCO N593JB B6 Orla… 28.4 -81.3 ## 10 2013 1 1 6 LGA ORD N3ALAA AA Chic… 42.0 -87.9 ## # … with 336,766 more rows, and 4 more variables: alt &lt;int&gt;, tz &lt;dbl&gt;, ## # dst &lt;chr&gt;, tzone &lt;chr&gt; 13.4.6 Exercises 1. Compute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States: library(nycflights13) airports %&gt;% semi_join(flights, c(&quot;faa&quot; = &quot;dest&quot;)) %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point() + coord_quickmap() ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map (Don’t worry if you don’t understand what semi_join() does — you’ll learn about it next.) You might want to use the size or colour of the points to display the average delay for each airport. I first grouped by destination and computed the average delay per destination, and then joined the airports data to this metric using left_join(). Then, we can use this table to create a plot displaying the relative average delays at airports across the US by modifying the code provided by the book above. # caluclate average delay by destination and join airports data avg_delay_byDest &lt;- flights %&gt;% group_by(dest) %&gt;% summarize ( avg_delay = mean(arr_delay, na.rm=T) ) %&gt;% left_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) avg_delay_byDest ## # A tibble: 105 x 9 ## dest avg_delay name lat lon alt tz dst tzone ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ABQ 4.38 Albuquerque In… 35.0 -107. 5355 -7 A America/… ## 2 ACK 4.85 Nantucket Mem 41.3 -70.1 48 -5 A America/… ## 3 ALB 14.4 Albany Intl 42.7 -73.8 285 -5 A America/… ## 4 ANC -2.5 Ted Stevens An… 61.2 -150. 152 -9 A America/… ## 5 ATL 11.3 Hartsfield Jac… 33.6 -84.4 1026 -5 A America/… ## 6 AUS 6.02 Austin Bergstr… 30.2 -97.7 542 -6 A America/… ## 7 AVL 8.00 Asheville Regi… 35.4 -82.5 2165 -5 A America/… ## 8 BDL 7.05 Bradley Intl 41.9 -72.7 173 -5 A America/… ## 9 BGR 8.03 Bangor Intl 44.8 -68.8 192 -5 A America/… ## 10 BHM 16.9 Birmingham Intl 33.6 -86.8 644 -6 A America/… ## # … with 95 more rows # plot the results on top of the map of the US avg_delay_byDest %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point(aes(size = avg_delay, alpha = 1/5, color = avg_delay)) + coord_quickmap() ## Warning: Removed 5 rows containing missing values (geom_point). 2. Add the location of the origin and destination (i.e. the lat and lon) to flights. To do this, I first make a smaller table from airports containing just the variables that we need: faa, lat, and lon. Then, I join this table to flights using origin to add the lat + lon values for origin. I join again using dest this time, to add the lat + lon values for dest. In order to label the lat and lon values according to origin or dest airport, I specify suffix = c(&quot;.origin&quot;, &quot;.dest&quot;). lat_lon_airports &lt;- airports %&gt;% select(faa, lat, lon) with_origin &lt;- left_join(flights, lat_lon_airports, by = c(&quot;origin&quot; = &quot;faa&quot;)) with_origin ## # A tibble: 336,776 x 21 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 14 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt; with_origin_and_dest &lt;- left_join(with_origin, lat_lon_airports, by = c(&quot;dest&quot; = &quot;faa&quot;), suffix = c(&quot;.origin&quot;, &quot;.dest&quot;)) with_origin_and_dest ## # A tibble: 336,776 x 23 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 16 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, lat.origin &lt;dbl&gt;, lon.origin &lt;dbl&gt;, ## # lat.dest &lt;dbl&gt;, lon.dest &lt;dbl&gt; 3. Is there a relationship between the age of a plane and its delays? First group by tailnum, then calculate avg delays for arr_delay and dep_delay, then join the info from planes. Then we can visualize any relationships between the year variable (age) of the plane and the delays associated with it. # first group by tailnum, calculate avg delays, then join with planes data avg_delay_plane &lt;- flights %&gt;% group_by(tailnum) %&gt;% summarize( avg_delay_dep = mean (dep_delay, na.rm = T), avg_delay_arr = mean (arr_delay, na.rm = T) ) %&gt;% left_join(planes, by = &quot;tailnum&quot;) avg_delay_plane ## # A tibble: 4,044 x 11 ## tailnum avg_delay_dep avg_delay_arr year type manufacturer model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 D942DN 31.5 31.5 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 N0EGMQ 8.49 9.98 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 N10156 17.8 12.7 2004 Fixe… EMBRAER EMB-… ## 4 N102UW 8 2.94 1998 Fixe… AIRBUS INDU… A320… ## 5 N103US -3.20 -6.93 1999 Fixe… AIRBUS INDU… A320… ## 6 N104UW 9.94 1.80 1999 Fixe… AIRBUS INDU… A320… ## 7 N10575 22.7 20.7 2002 Fixe… EMBRAER EMB-… ## 8 N105UW 2.58 -0.267 1999 Fixe… AIRBUS INDU… A320… ## 9 N107US -0.463 -5.73 1999 Fixe… AIRBUS INDU… A320… ## 10 N108UW 4.22 -1.25 1999 Fixe… AIRBUS INDU… A320… ## # … with 4,034 more rows, and 4 more variables: engines &lt;int&gt;, ## # seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt; # vizualize dep_delay vs year of plane avg_delay_plane %&gt;% ggplot( aes ( year, avg_delay_dep )) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## Warning: Removed 798 rows containing non-finite values (stat_smooth). ## Warning: Removed 798 rows containing missing values (geom_point). # vizualize arr_delay vs year of plane avg_delay_plane %&gt;% ggplot( aes ( year, avg_delay_arr )) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## Warning: Removed 798 rows containing non-finite values (stat_smooth). ## Warning: Removed 798 rows containing missing values (geom_point). 4. What weather conditions make it more likely to see a delay? Join the weather table with flights, and then we can visualize the relationship with delay and weather conditions. One way to do so is with a correlogram, which shows the correlation between each of the variables in a matrix. After joining weather with flights, we can remove the time variables (year, month, day, hour, etc) and use as.matrix() to turn the tibble into a matrix. Then, we can use the ggcorrplot package to plot a nice looking correllogram. From this plot, we can observe that variable in weather with the highest positive correlation with dep_delay is precip, at roughly 0.05. While this is a very small number (between 0-1), over a large number of observations it is still meaningful. This also makes sense since high precipitation values often make visibility an issue, which may lead to delayed flights. It might make more sense to group dep_delay by day, and then look at the correlations in the manner I have done so below. I will revisit this in the 13.5.1 exercise 4, in which I find that lower visibility, higher humidity, and lower pressure on average correlate with higher cumulative delays. # install.packages(&quot;ggcorrplot&quot;) library(ggcorrplot) # join weather table with flights, using natural join (no &quot;by&quot; argument specified) weatherdelays &lt;- flights %&gt;% select(year, month, day, hour, time_hour, origin, dep_delay) %&gt;% left_join(weather)%&gt;% select(-year,-month,-day,-hour,-time_hour,-origin) ## Joining, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;time_hour&quot;, &quot;origin&quot;) weatherdelays ## # A tibble: 336,776 x 10 ## dep_delay temp dewp humid wind_dir wind_speed wind_gust precip ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 39.0 28.0 64.4 260 12.7 NA 0 ## 2 4 39.9 25.0 54.8 250 15.0 21.9 0 ## 3 2 39.0 27.0 61.6 260 15.0 NA 0 ## 4 -1 39.0 27.0 61.6 260 15.0 NA 0 ## 5 -6 39.9 25.0 54.8 260 16.1 23.0 0 ## 6 -4 39.0 28.0 64.4 260 12.7 NA 0 ## 7 -5 37.9 28.0 67.2 240 11.5 NA 0 ## 8 -3 39.9 25.0 54.8 260 16.1 23.0 0 ## 9 -3 37.9 27.0 64.3 260 13.8 NA 0 ## 10 -2 39.9 25.0 54.8 260 16.1 23.0 0 ## # … with 336,766 more rows, and 2 more variables: pressure &lt;dbl&gt;, ## # visib &lt;dbl&gt; rm_na_weatherdelays &lt;- as.matrix (na.omit(weatherdelays)) head(rm_na_weatherdelays) ## dep_delay temp dewp humid wind_dir wind_speed wind_gust precip ## [1,] 4 39.92 24.98 54.81 250 14.96014 21.86482 0 ## [2,] -6 39.92 24.98 54.81 260 16.11092 23.01560 0 ## [3,] -3 39.92 24.98 54.81 260 16.11092 23.01560 0 ## [4,] -2 39.92 24.98 54.81 260 16.11092 23.01560 0 ## [5,] -1 39.92 24.98 54.81 260 16.11092 23.01560 0 ## [6,] 0 39.92 24.98 54.81 260 16.11092 23.01560 0 ## pressure visib ## [1,] 1011.4 10 ## [2,] 1011.7 10 ## [3,] 1011.7 10 ## [4,] 1011.7 10 ## [5,] 1011.7 10 ## [6,] 1011.7 10 corr_weather &lt;- cor(rm_na_weatherdelays) ggcorrplot(corr_weather, hc.order = TRUE, type = &quot;lower&quot;, lab = TRUE, lab_size = 3, method=&quot;circle&quot;, colors = c(&quot;steelblue&quot;, &quot;white&quot;, &quot;orangered3&quot;), title=&quot;Correlogram&quot;, ggtheme=theme_bw) weatherdelays %&gt;% ggplot( aes (x = precip, y = dep_delay))+ geom_point()+ geom_smooth(se = F) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## Warning: Removed 9783 rows containing non-finite values (stat_smooth). ## Warning: Removed 9783 rows containing missing values (geom_point). weatherdelays %&gt;% ggplot( aes (x = humid, y = dep_delay))+ geom_point()+ geom_smooth(se = F) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## Warning: Removed 9800 rows containing non-finite values (stat_smooth). ## Warning: Removed 9800 rows containing missing values (geom_point). 5. What happened on June 13 2013? Display the spatial pattern of delays, and then use Google to cross-reference with the weather. First filter flights for observations from June 13, 2013 using filter(). We can see that there were only flights comming out of EWR, JFK, and LGA on this day! Furthermore, all the flights were considerably delayed (high dep_delay value). I initially found this by overlaying the average dep_delay for the filtered data table onto the map of the US, and was surprised that there were so few origin airports on the map. Looking at the opposite, grouping by destination and looking at arr_delay values, we can see that there are many, many more airports in the destination variable. Most flights flew out of only a few airports (EWR, JFK, and LGA) but arrived at many different destinations, and these arrivals were delayed as well. flights %&gt;% filter (year == 2013, month == 6, day == 13) %&gt;% group_by(origin) %&gt;% summarize ( avg_delay = mean(dep_delay, na.rm=T) ) ## # A tibble: 3 x 2 ## origin avg_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 EWR 45.6 ## 2 JFK 43.7 ## 3 LGA 48.4 avg_delay_byorigin &lt;- flights %&gt;% filter (year == 2013, month == 6, day == 13) %&gt;% group_by(origin) %&gt;% summarize ( avg_delay = mean(dep_delay, na.rm=T) ) %&gt;% left_join(airports, by = c(&quot;origin&quot; = &quot;faa&quot;)) avg_delay_byorigin ## # A tibble: 3 x 9 ## origin avg_delay name lat lon alt tz dst tzone ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 EWR 45.6 Newark Libert… 40.7 -74.2 18 -5 A America/Ne… ## 2 JFK 43.7 John F Kenned… 40.6 -73.8 13 -5 A America/Ne… ## 3 LGA 48.4 La Guardia 40.8 -73.9 22 -5 A America/Ne… # plot the results on top of the map of the US avg_delay_byorigin %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point(aes(size = avg_delay, alpha = 1/5, color = avg_delay)) + coord_quickmap() avg_delay_byDest &lt;- flights %&gt;% filter (year == 2013, month == 6, day == 13) %&gt;% group_by(dest) %&gt;% summarize ( avg_delay = mean(arr_delay, na.rm=T) ) %&gt;% left_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) avg_delay_byDest ## # A tibble: 89 x 9 ## dest avg_delay name lat lon alt tz dst tzone ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ABQ 64 Albuquerque Int… 35.0 -107. 5355 -7 A America… ## 2 ACK 8 Nantucket Mem 41.3 -70.1 48 -5 A America… ## 3 ALB -19 Albany Intl 42.7 -73.8 285 -5 A America… ## 4 ATL 109. Hartsfield Jack… 33.6 -84.4 1026 -5 A America… ## 5 AUS 64.3 Austin Bergstro… 30.2 -97.7 542 -6 A America… ## 6 AVL 90 Asheville Regio… 35.4 -82.5 2165 -5 A America… ## 7 BDL 18.5 Bradley Intl 41.9 -72.7 173 -5 A America… ## 8 BGR 102 Bangor Intl 44.8 -68.8 192 -5 A America… ## 9 BHM 158 Birmingham Intl 33.6 -86.8 644 -6 A America… ## 10 BNA 95.9 Nashville Intl 36.1 -86.7 599 -6 A America… ## # … with 79 more rows # plot the results on top of the map of the US avg_delay_byDest %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point(aes(size = avg_delay, alpha = 1/5, color = avg_delay)) + coord_quickmap() ## Warning: Removed 7 rows containing missing values (geom_point). 13.5 Notes - Filtering Joins There are two types of filtering joins that can be performed, listed by the book: “semi_join(x, y) keeps all observations in x that have a match in y.” “anti_join(x, y) drops all observations in x that have a match in y.” The semi_join() function is useful when you want to filter a table for observations that match a certain criteria set by a second table. The provided example in the book is finding top destinations, then looking for the observations in the orginal data table that corresponded to the top destinations. You could use filter() but this might become difficult when analzying multiple variables. # find top destinations top_dest &lt;- flights %&gt;% count(dest, sort = TRUE) %&gt;% head(10) top_dest ## # A tibble: 10 x 2 ## dest n ## &lt;chr&gt; &lt;int&gt; ## 1 ORD 17283 ## 2 ATL 17215 ## 3 LAX 16174 ## 4 BOS 15508 ## 5 MCO 14082 ## 6 CLT 14064 ## 7 SFO 13331 ## 8 FLL 12055 ## 9 MIA 11728 ## 10 DCA 9705 # use filter() to find observations in flights that have matched key in top destinations table flights %&gt;% filter(dest %in% top_dest$dest) ## # A tibble: 141,145 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 542 540 2 923 ## 2 2013 1 1 554 600 -6 812 ## 3 2013 1 1 554 558 -4 740 ## 4 2013 1 1 555 600 -5 913 ## 5 2013 1 1 557 600 -3 838 ## 6 2013 1 1 558 600 -2 753 ## 7 2013 1 1 558 600 -2 924 ## 8 2013 1 1 558 600 -2 923 ## 9 2013 1 1 559 559 0 702 ## 10 2013 1 1 600 600 0 851 ## # … with 141,135 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # use semi_join() to find obseravtions in flights that have matched key in top destinations table flights %&gt;% semi_join(top_dest) ## Joining, by = &quot;dest&quot; ## # A tibble: 141,145 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 542 540 2 923 ## 2 2013 1 1 554 600 -6 812 ## 3 2013 1 1 554 558 -4 740 ## 4 2013 1 1 555 600 -5 913 ## 5 2013 1 1 557 600 -3 838 ## 6 2013 1 1 558 600 -2 753 ## 7 2013 1 1 558 600 -2 924 ## 8 2013 1 1 558 600 -2 923 ## 9 2013 1 1 559 559 0 702 ## 10 2013 1 1 600 600 0 851 ## # … with 141,135 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The opposite of the semi_join() function is the anti_join() function, which will return mismatches between the two tables (which values in x do not have a matched key in y?). The book uses anti_join() to find which flights do not have a match in the planes data table. # anti join to find which observations in flights do not have a match in the planes table flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(tailnum, sort = TRUE) ## # A tibble: 722 x 2 ## tailnum n ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 2512 ## 2 N725MQ 575 ## 3 N722MQ 513 ## 4 N723MQ 507 ## 5 N713MQ 483 ## 6 N735MQ 396 ## 7 N0EGMQ 371 ## 8 N534MQ 364 ## 9 N542MQ 363 ## 10 N531MQ 349 ## # … with 712 more rows 13.5.1 Exercises 1. What does it mean for a flight to have a missing tailnum? What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.) The flights that have a missing tailnum can be found by filtering the flights table using filter(is.na(tailnum)). We can observe that these entries also have missing values for dep_time, suggesting that these flights were cancelled. If we remove all flights that have a missing dep_time, we observe that the flights with missing tailnum are also removed as well. To find the tail numbers that don’t have a matching record in planes, we can use the anti_join() function. Then, we can examine the columns of the resulting table to see if any particular variables are enriched. We observe that some carriers are particularly enriched in the flights that have tailnums that are not recorded in the planes table, including MQ and AA. # find flights that have a missing tailnum flights %&gt;% filter(is.na(tailnum)) ## # A tibble: 2,512 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 2 NA 1545 NA NA ## 2 2013 1 2 NA 1601 NA NA ## 3 2013 1 3 NA 857 NA NA ## 4 2013 1 3 NA 645 NA NA ## 5 2013 1 4 NA 845 NA NA ## 6 2013 1 4 NA 1830 NA NA ## 7 2013 1 5 NA 840 NA NA ## 8 2013 1 7 NA 820 NA NA ## 9 2013 1 8 NA 1645 NA NA ## 10 2013 1 9 NA 755 NA NA ## # … with 2,502 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # remove flights with missing dep_time and look at whether missing tailnums still exist not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_time)) not_cancelled %&gt;% filter(is.na(tailnum)) ## # A tibble: 0 x 19 ## # … with 19 variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;, ## # sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # anti join to find which observations in flights do not have a match in the planes table flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(tailnum, sort = T) ## # A tibble: 722 x 2 ## tailnum n ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 2512 ## 2 N725MQ 575 ## 3 N722MQ 513 ## 4 N723MQ 507 ## 5 N713MQ 483 ## 6 N735MQ 396 ## 7 N0EGMQ 371 ## 8 N534MQ 364 ## 9 N542MQ 363 ## 10 N531MQ 349 ## # … with 712 more rows # observe that some carriers are enriched flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(carrier, sort = T) %&gt;% ggplot (aes (x = carrier, y = n))+ geom_bar( stat = &quot;identity&quot;, aes(fill = carrier)) 2. Filter flights to only show flights with planes that have flown at least 100 flights. We can do this by using the semi_join() function. First, we have to identify which tailnums (planes) have flown at least 100 flights, using count() and filter(). We can store this output in a separate table which I call flights_100, then use semi_join() to keep only the observations in flights that also have an entry in flights_100. # make a separate table containing tailnums that have over 100 flights flights_100 &lt;- flights %&gt;% count(tailnum) %&gt;% filter(n&gt;100) # use semi join to keep only tailnums in flights that exist in our flights_100 table flights %&gt;% semi_join (flights_100) ## Joining, by = &quot;tailnum&quot; ## # A tibble: 229,202 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 544 545 -1 1004 ## 4 2013 1 1 554 558 -4 740 ## 5 2013 1 1 555 600 -5 913 ## 6 2013 1 1 557 600 -3 709 ## 7 2013 1 1 557 600 -3 838 ## 8 2013 1 1 558 600 -2 849 ## 9 2013 1 1 558 600 -2 853 ## 10 2013 1 1 558 600 -2 923 ## # … with 229,192 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 3. Combine fueleconomy::vehicles and fueleconomy::common to find only the records for the most common models. We can use the semi_join() function to combine vehicles and common, which will keep only the records in vehicles that also have a match in the common table. library(fueleconomy) semi_join(vehicles,common) ## Joining, by = c(&quot;make&quot;, &quot;model&quot;) ## # A tibble: 14,531 x 12 ## id make model year class trans drive cyl displ fuel hwy cty ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1833 Acura Integ… 1986 Subc… Auto… Fron… 4 1.6 Regu… 28 22 ## 2 1834 Acura Integ… 1986 Subc… Manu… Fron… 4 1.6 Regu… 28 23 ## 3 3037 Acura Integ… 1987 Subc… Auto… Fron… 4 1.6 Regu… 28 22 ## 4 3038 Acura Integ… 1987 Subc… Manu… Fron… 4 1.6 Regu… 28 23 ## 5 4183 Acura Integ… 1988 Subc… Auto… Fron… 4 1.6 Regu… 27 22 ## 6 4184 Acura Integ… 1988 Subc… Manu… Fron… 4 1.6 Regu… 28 23 ## 7 5303 Acura Integ… 1989 Subc… Auto… Fron… 4 1.6 Regu… 27 22 ## 8 5304 Acura Integ… 1989 Subc… Manu… Fron… 4 1.6 Regu… 28 23 ## 9 6442 Acura Integ… 1990 Subc… Auto… Fron… 4 1.8 Regu… 24 20 ## 10 6443 Acura Integ… 1990 Subc… Manu… Fron… 4 1.8 Regu… 26 21 ## # … with 14,521 more rows 4. Find the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns? To find the 48 hours that have the worst delays, we can use group_by() and summarize() to find the total delay per day, then combine the total delay per day with lag() to get the total delay per 48 hours. There will be a missing value for the first day of the year, since the method that I use will sum the total delay for the current day with the previous day. Sorting this table will identify the days with the worst dep_delay over 48 hours, which turns out to be March 7-8th, 2013, with a total delay of 84713 min. We can left_join() the weather conditions to this table, and then analyze the weather patterns for this day in particular. We can see that on average, higher 48hr delays are associated with higher humidity values, higher temp/dewpoint, lower pressure, and lower visibility, based on both the correlogram and scatterplots. # identify the 48 hrs with highest combined dep_delay, then join weather conditions most_delayed_48 &lt;- flights %&gt;% group_by(year, month, day) %&gt;% summarize ( delay_per_day = sum( dep_delay, na.rm=T))%&gt;% mutate ( delay_48hr = delay_per_day + lag(delay_per_day)) %&gt;% arrange(desc(delay_48hr)) %&gt;% left_join(weather, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) most_delayed_48 ## # A tibble: 26,116 x 17 ## # Groups: year, month [?] ## year month day delay_per_day delay_48hr origin hour temp dewp ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 3 8 66746 84713 EWR 0 33.8 32 ## 2 2013 3 8 66746 84713 EWR 1 33.1 32 ## 3 2013 3 8 66746 84713 EWR 2 33.1 30.9 ## 4 2013 3 8 66746 84713 EWR 3 33.1 30.9 ## 5 2013 3 8 66746 84713 EWR 4 33.8 30.9 ## 6 2013 3 8 66746 84713 EWR 5 32 30.9 ## 7 2013 3 8 66746 84713 EWR 6 32 30.0 ## 8 2013 3 8 66746 84713 EWR 7 32 30.0 ## 9 2013 3 8 66746 84713 EWR 8 32 30.2 ## 10 2013 3 8 66746 84713 EWR 9 32 30.2 ## # … with 26,106 more rows, and 8 more variables: humid &lt;dbl&gt;, ## # wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, ## # pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; # visualize correlations of variables with delay_48hr most_delayed_48 %&gt;% ggplot( aes (x = delay_48hr, y = humid))+ geom_point(aes (size = precip))+ geom_smooth(method = &quot;lm&quot;, se = F) ## Warning: Removed 857 rows containing non-finite values (stat_smooth). ## Warning: Removed 857 rows containing missing values (geom_point). most_delayed_48 %&gt;% ggplot( aes (x = delay_48hr, y = visib))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, se = F) ## Warning: Removed 856 rows containing non-finite values (stat_smooth). ## Warning: Removed 856 rows containing missing values (geom_point). most_delayed_48 %&gt;% ggplot( aes (x = delay_48hr, y = pressure))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, se = F) ## Warning: Removed 3477 rows containing non-finite values (stat_smooth). ## Warning: Removed 3477 rows containing missing values (geom_point). # plot a correlogram to observe how delay_48hr is associated with other weather variables library(ggcorrplot) weatherdelays &lt;- most_delayed_48 %&gt;% select(-day,-hour,-delay_per_day,-origin, -time_hour) rm_na_weatherdelays &lt;- as.matrix (na.omit(weatherdelays)[,-c(1:2)]) corr_weather &lt;- cor(rm_na_weatherdelays) ggcorrplot(corr_weather, hc.order = TRUE, type = &quot;lower&quot;, lab = TRUE, lab_size = 3, method=&quot;circle&quot;, colors = c(&quot;steelblue&quot;, &quot;white&quot;, &quot;orangered3&quot;), title=&quot;Correlogram&quot;, ggtheme=theme_bw) 5. What does anti_join(flights, airports, by = c(“dest” = “faa”)) tell you? What does anti_join(airports, flights, by = c(“faa” = “dest”)) tell you? anti_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) tells you which observations in flights do not have a destination that is listed in the ‘faa’ variable of the airports table. anti_join(airports, flights, by = c(&quot;faa&quot; = &quot;dest&quot;)) tells you which observations in airports do not have a ‘faa’ value that is present in the ‘dest’ variable of the flights table. anti_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) ## # A tibble: 7,602 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 544 545 -1 1004 ## 2 2013 1 1 615 615 0 1039 ## 3 2013 1 1 628 630 -2 1137 ## 4 2013 1 1 701 700 1 1123 ## 5 2013 1 1 711 715 -4 1151 ## 6 2013 1 1 820 820 0 1254 ## 7 2013 1 1 820 820 0 1249 ## 8 2013 1 1 840 845 -5 1311 ## 9 2013 1 1 909 810 59 1331 ## 10 2013 1 1 913 918 -5 1346 ## # … with 7,592 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; anti_join(airports, flights, by = c(&quot;faa&quot; = &quot;dest&quot;)) ## # A tibble: 1,357 x 8 ## faa name lat lon alt tz dst tzone ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 04G Lansdowne Airport 41.1 -80.6 1044 -5 A America/New_… ## 2 06A Moton Field Municipa… 32.5 -85.7 264 -6 A America/Chic… ## 3 06C Schaumburg Regional 42.0 -88.1 801 -6 A America/Chic… ## 4 06N Randall Airport 41.4 -74.4 523 -5 A America/New_… ## 5 09J Jekyll Island Airport 31.1 -81.4 11 -5 A America/New_… ## 6 0A9 Elizabethton Municip… 36.4 -82.2 1593 -5 A America/New_… ## 7 0G6 Williams County Airp… 41.5 -84.5 730 -5 A America/New_… ## 8 0G7 Finger Lakes Regiona… 42.9 -76.8 492 -5 A America/New_… ## 9 0P2 Shoestring Aviation … 39.8 -76.6 1000 -5 U America/New_… ## 10 0S9 Jefferson County Intl 48.1 -123. 108 -8 A America/Los_… ## # … with 1,347 more rows 6. You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above. To explore whether each plane exclusively flies with a single airline, we can group the flights table by tailnum, then by carrier. Counting this grouped table will yield the number of flights each plane made with its carrier or carriers. We can group this table again by tailnum, then count how many carriers it flew with. If the hypothesis is true, then we would expect that there would be no entries with more than one carrier per plane. However, when we filter this table for planes with carriers &gt; 1, we observe that 17 planes have flown with 2 carriers. Out of the roughly 4000 planes, this is only a very small fraction. This evidence supports our hypothesis that, in general, there is an association between plane and airline. Most of the time, planes only fly with a single carrier. However, a small amount of planes have flown with multiple airlines. This may have occured if an airline sold their planes to another airline. # find the number of carriers that each plane has flown with flights %&gt;% group_by (tailnum, carrier) %&gt;% count() %&gt;% group_by(tailnum)%&gt;% count() %&gt;% arrange(desc(nn)) %&gt;% filter(nn&gt;1) ## # A tibble: 18 x 2 ## # Groups: tailnum [18] ## tailnum nn ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 7 ## 2 N146PQ 2 ## 3 N153PQ 2 ## 4 N176PQ 2 ## 5 N181PQ 2 ## 6 N197PQ 2 ## 7 N200PQ 2 ## 8 N228PQ 2 ## 9 N232PQ 2 ## 10 N933AT 2 ## 11 N935AT 2 ## 12 N977AT 2 ## 13 N978AT 2 ## 14 N979AT 2 ## 15 N981AT 2 ## 16 N989AT 2 ## 17 N990AT 2 ## 18 N994AT 2 13.6 &amp; 13.7 Notes - Join problems and set operations The book recommends the following process for setting up your data to perform join operations. First, identify the variables that form the primary key in each table, usually thinking about the types of variables that would make sense to be keys (id, or a date/time, etc.). See if there are any missing values in the variable(s) for your primary key. Since values cannot be joined to missing values, this might result in issues. Make sure that all the values of the keys match between the tables you are joining. If a value for a key in one of the tables was missing due to a data entry error or other reason, the value will be lost in the subsequent join. Values that are not consistent between keys can be assessed using anti_join(). The last segment of the chapter focuses on set operations. Below are the functions listed by the book for performing set operations. They treat the observations (rows) of two tables that have the same variables as sets. intersect(x, y): return only observations in both x and y. union(x, y): return unique observations in x and y. setdiff(x, y): return observations in x, but not in y. Here are the examples provided by the book for the set operations described above: df1 &lt;- tribble( ~x, ~y, 1, 1, 2, 1 ) df2 &lt;- tribble( ~x, ~y, 1, 1, 1, 2 ) intersect(df1, df2) ## # A tibble: 1 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 union(df1, df2) ## # A tibble: 3 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 ## 2 2 1 ## 3 1 1 setdiff(df1, df2) ## # A tibble: 1 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 1 setdiff(df2, df1) ## # A tibble: 1 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 "],
["chapter-14-strings.html", "Chapter 14 - Strings 14.2.5 Exercises 14.3.1.1 Exercises 14.3.2.1 Exercises 14.3.3.1 Exercises 14.3.4.1 Exercises 14.3.5.1 Exercises 14.4.2 Exercises 14.4.3.1 Exercises 14.4.4.1 Exercises 14.4.5.1 Exercises 14.4.6.1 Exercises 14.5.1 Exercises 14.7.1 Exercises", " Chapter 14 - Strings library(tidyverse) For Chapter 14, since the github_document R markdown format does not support html output, I will comment out the “str_view()” commands and replace them with str_extract() or str_subset(), as appropriate. This will still let you observe which words or strings are matched by the regex. You can also type the str_view() command into your own RStudio instance to observe the output. 14.2.5 Exercises 1. In code that doesn’t use stringr, you’ll often see paste() and paste0(). What’s the difference between the two functions? What stringr function are they equivalent to? How do the functions differ in their handling of NA? The paste() function and paste0() functions are used to concatenate vectors after converting to character. In other words, combine converted strings together into one string. paste0() differs from paste() in that it always combines the strings without a separating value, whereas paste() allows you to specify the sep argument. When sep = “”, paste() provides the same output as paste0(). The stringr function they are equivalent to is str_c(). p = NA paste(&quot;the value of p is &quot;, p) ## [1] &quot;the value of p is NA&quot; str_c(&quot;the value of p is &quot;, p) ## [1] NA 2. In your own words, describe the difference between the sep and collapse arguments to str_c(). The sep argument allows you to specify what types of characters will lie between the items being joined. For example, you can choose to separate things with commas rather than the default no-space “”. These separators will be inserted between each separate item fed into str_c, and will not be placed if a pre-vectorized argument is provided. If you have a premade vector that you want to join into one string with a specified separator, the collapse argument should be used, in which you specify the type of separator you want inserted. Below is an example illustrating these points. join_me &lt;- c(88, &quot;hello&quot;, &quot;world&quot;) str_c(88, &quot;hello&quot;, &quot;world&quot;) ## [1] &quot;88helloworld&quot; str_c(88, &quot;hello&quot;, &quot;world&quot;, sep = &quot;, &quot;) ## [1] &quot;88, hello, world&quot; # sep does not place the commas if a premade vector is fed into str_c str_c(join_me, sep = &quot;, &quot;) ## [1] &quot;88&quot; &quot;hello&quot; &quot;world&quot; # use collapse instead if you want to place separators into premade vector str_c(join_me, collapse = &quot;, &quot;) ## [1] &quot;88, hello, world&quot; 3. Use str_length() and str_sub() to extract the middle character from a string. What will you do if the string has an even number of characters? To extract the middle character from a string, we can use str_length()/2 to find the middle index of the string, then use str_sub() to extract the character at that index. For even character strings, it would make sense to extract the two characters at the very center of the string. I wrote a function get_middle() that would test whether a string has an even number or odd number of characters using %%, then apply the appropriate str_sub() command to get the middle character or middle two characters. get_middle &lt;- function (my_str) { if (str_length(my_str)%%2 != 0){ return (str_sub(my_str, ceiling(str_length(my_str)/2), ceiling(str_length(my_str)/2))) } else { return (str_sub(my_str, ceiling(str_length(my_str)/2), ceiling(str_length(my_str)/2+1))) } } get_middle (&quot;qwert&quot;) ## [1] &quot;e&quot; get_middle (&quot;qwerty&quot;) ## [1] &quot;er&quot; 4. What does str_wrap() do? When might you want to use it? str_wrap(), as the name suggests, wraps a string into a paragraph. You can specify how wide to make each line of the paragraph using the width argument. It does so by inserting a newline '\\n' at the appropriate positions. This would be useful for printing very long strings. Below is an example of using str_wrap(). long_string &lt;- &#39;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.&#39; long_string ## [1] &quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.&quot; str_wrap(long_string, width = 30) ## [1] &quot;Lorem ipsum dolor sit amet,\\nconsectetur adipiscing\\nelit, sed do eiusmod tempor\\nincididunt ut labore et dolore\\nmagna aliqua.&quot; 5. What does str_trim() do? What’s the opposite of str_trim()? str_trim() will remove whitespace from the start and end of a string. The opposite of str_trim() is str_pad(), which can be used to add whitespace to either the left, right, or both sides of a string. Below is an example of using both functions. whitespace &lt;- &quot; why is there whitespace on both sides? &quot; whitespace ## [1] &quot; why is there whitespace on both sides? &quot; str_trim(whitespace) ## [1] &quot;why is there whitespace on both sides?&quot; str_pad(&quot;add some whitespace to the left!&quot;, 40, side = &quot;left&quot;) ## [1] &quot; add some whitespace to the left!&quot; 6. Write a function that turns (e.g.) a vector c(“a”, “b”, “c”) into the string a, b, and c. Think carefully about what it should do if given a vector of length 0, 1, or 2. We can write a function that checks the length of the vector using length() and uses if else statements to perform the appropriate action. For vectors of length 0, we should return an empty string “”. For vectors of length 1, we should just return the single item as-is. For vectors of length &gt; 1, we should return the output specified in the question: “item-1, item-2, … item-n-1, and item-n”. To combine the items in the vector appropriately, we can first collapse the first n-1 items using str_c( collapse = “,”), then use str_c on this collapsed string with “, and”, and with the last item in the vector. This will insert the commas at the right locations, as well as the word “and” right before the last item in the vector. # function name is convert_str convert_str &lt;- function (str_vector) { if (length(str_vector) == 0) { return (&quot;&quot;) } else if (length (str_vector) == 1) { return (str_vector[1]) } else { combined_minus_last &lt;- str_c(str_vector[1:(length(str_vector)-1)], collapse = &quot;, &quot;) # collapse all items except the last one return( str_c (combined_minus_last, &quot;, and &quot;, str_vector[length(str_vector)])) # concatenate the word &quot;and&quot; and the last item } } convert_str(c(&quot;fee&quot;, &quot;fi&quot;, &quot;fo&quot;, &quot;fum&quot;)) ## [1] &quot;fee, fi, fo, and fum&quot; convert_str(c(&quot;fee&quot;)) ## [1] &quot;fee&quot; convert_str(c()) ## [1] &quot;&quot; 14.3.1.1 Exercises 1. Explain why each of these strings don’t match a \\: &quot;\\&quot;, &quot;\\\\&quot;, &quot;\\\\\\&quot;. To match a literal \\, you need to use four backslashes, &quot;\\\\\\\\&quot;. One backslash, two backslashes, and 3 backslashes wont work! One backslash &quot;\\&quot; won’t work because it will “escape” the quotation, resulting in an error. Using 3 backslashes &quot;\\\\\\&quot; will also result in an error, and using 4 backslashes &quot;\\\\\\\\&quot; will finally let us match the single literal \\. literal_backslash &lt;- &quot;hello_\\\\_world&quot; # str_view(literal_backslash, &quot;\\\\\\\\&quot;) str_extract(literal_backslash, &quot;\\\\\\\\&quot;) ## [1] &quot;\\\\&quot; # errors # x &lt;- &quot;\\&quot; # x &lt;- &quot;\\\\\\&quot; 2. How would you match the sequence &quot;'\\? I would use the regex: &quot;\\&quot;\\'\\\\\\\\&quot;. This escapes the “, escapes the ’, then uses the four backslashes to identify the literal \\. example &lt;- &quot;hello_\\&quot;\\&#39;\\\\_world&quot; writeLines(example) ## hello_&quot;&#39;\\_world # str_view(example, &quot;\\&quot;\\&#39;\\\\\\\\&quot; ) str_extract(example, &quot;\\&quot;\\&#39;\\\\\\\\&quot; ) ## [1] &quot;\\&quot;&#39;\\\\&quot; 3. What patterns will the regular expression \\..\\..\\.. match? How would you represent it as a string? If interepreted as a regular expression and not as the string-form of a regular expression, this will match a period \\., followed by any character ., followed by another period, any character, another period, and then any character. Below, I’ve represented it as a string in R. Because of the backslash convention, it will error if you try to plug \\..\\..\\.. in directly. # test &lt;- &quot;\\..\\..\\..&quot; # errors # need to double backslash within the string test &lt;- &quot;\\\\..\\\\..\\\\..&quot; writeLines(test) ## \\..\\..\\.. 14.3.2.1 Exercises 1. How would you match the literal string &quot;$^$&quot;? Below, i use &quot;\\\\$\\\\^\\\\$&quot; to match the literal string &quot;$^$&quot;. This is because you have to double backslash the $ and ^ characters, which usually serve the purpose of matching the end or start of strings, respectively. test &lt;- &quot;$^$hello&quot; writeLines(test) ## $^$hello # str_view(test, &quot;\\\\$\\\\^\\\\$&quot;) str_extract(test, &quot;\\\\$\\\\^\\\\$&quot;) ## [1] &quot;$^$&quot; 2. Given the corpus of common words in stringr::words, create regular expressions that find all words that: Start with “y”. End with “x” Are exactly three letters long. (Don’t cheat by using str_length()!) Have seven letters or more. Since this list is long, you might want to use the match argument to str_view() to show only the matching or non-matching words. To match words that start with y, use the “^” anchor before “y”. To match words that end with x, use the &quot;\\$&quot; anchor after “x”. To match words that are exactly three letters long, we can use both anchors “^” and &quot;\\$&quot; wrapped around 3 dots, which specify a series of any 3 characters. To match words that have seven letters or more, we can use the “^” anchor plus seven dots, which specify a series of any seven characters. # display a sample of stringr::words head(words) ## [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; &quot;account&quot; # starts with &quot;y&quot; # str_view(words, &quot;^y&quot;, match = T) str_subset(words, &quot;^y&quot;) ## [1] &quot;year&quot; &quot;yes&quot; &quot;yesterday&quot; &quot;yet&quot; &quot;you&quot; &quot;young&quot; # ends with &quot;x&quot; # str_view(words, &quot;x$&quot;, match = T) str_subset(words, &quot;x$&quot;) ## [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; # are exactly 3 letters long # str_view(words, &quot;^...$&quot;) str_subset(words, &quot;^...$&quot;) %&gt;% head(10) ## [1] &quot;act&quot; &quot;add&quot; &quot;age&quot; &quot;ago&quot; &quot;air&quot; &quot;all&quot; &quot;and&quot; &quot;any&quot; &quot;arm&quot; &quot;art&quot; # have seven letters or more # str_view(words, &quot;^.......&quot;) str_subset(words, &quot;^.......&quot;) %&gt;% head(10) ## [1] &quot;absolute&quot; &quot;account&quot; &quot;achieve&quot; &quot;address&quot; &quot;advertise&quot; ## [6] &quot;afternoon&quot; &quot;against&quot; &quot;already&quot; &quot;alright&quot; &quot;although&quot; 14.3.3.1 Exercises 1. Create regular expressions to find all words that: Start with a vowel. That only contain consonants. (Hint: thinking about matching “not”-vowels.) End with ed, but not with eed. End with ing or ise. To find words that start with vowels, we can use the “^” anchor along with a selection of vowels using the brackets. To find words that only contain consonants, we can specify that, from start to end (using ^ and \\$), there are no vowels (using ^ within brackets, along with the + sign, which means 1 or more of something). To find words that end in ed, but not eed, we can specify that we want words that end with ed, but have anything other than an e beforehand using [^e]. For words that end in ing or ise, we can use the “or” character “|” to combine two separate regexes, one that looks for ing\\$ and one that looks for ise\\$. Below are the regexes I have described in words above. # starts with a vowel # str_view(words, &quot;^[aeiou]&quot;) str_subset(words, &quot;^[aeiou]&quot;) %&gt;% head(10) ## [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; &quot;account&quot; ## [7] &quot;achieve&quot; &quot;across&quot; &quot;act&quot; &quot;active&quot; # only contain consonants # str_view(words, &quot;^[^aeiou]+$&quot;, match = T) str_subset(words, &quot;^[^aeiou]+$&quot;) ## [1] &quot;by&quot; &quot;dry&quot; &quot;fly&quot; &quot;mrs&quot; &quot;try&quot; &quot;why&quot; # ends with ed, but not eed # str_view(words, &quot;[^e]ed$&quot;, match = T) str_subset(words, &quot;[^e]ed$&quot;) ## [1] &quot;bed&quot; &quot;hundred&quot; &quot;red&quot; # ends with ing or ise # str_view(words, &quot;ing$|ise$&quot;, match = T) str_subset(words, &quot;ing$|ise$&quot;) ## [1] &quot;advertise&quot; &quot;bring&quot; &quot;during&quot; &quot;evening&quot; &quot;exercise&quot; ## [6] &quot;king&quot; &quot;meaning&quot; &quot;morning&quot; &quot;otherwise&quot; &quot;practise&quot; ## [11] &quot;raise&quot; &quot;realise&quot; &quot;ring&quot; &quot;rise&quot; &quot;sing&quot; ## [16] &quot;surprise&quot; &quot;thing&quot; 2. Empirically verify the rule “i before e except after c”. To examine whether any words in the dataset break this rule, we can search for words with the sequence of characters “cie”, which contain an i before e even after a c. This yields the words “science” an “society”. This tells us that the rule is not 100% correct! Searching for “cei”, which will show words that adhere to the rule, yields “receive”, which tells us that the rule might be correct for certain words. Is i always before e, even after something that is not c? Searching for words that do not follow this convention (i AFTER e, after anything other than a c) using the regex “[^c]ei” yields the word “weigh”, which again disproves the rule. Searching for words that follow the rule (ie before e, after anything other than a c) using the regex “[^c]ie” yields many results, which shows us that the rule applies for most words, but not all of them. # list words that contain i before e, after a c. if this yields matches, then the rule is false! # str_view(words, &quot;cie&quot;, match = T) # str_view(words, &quot;cei&quot;, match = T) # str_view(words, &quot;[^c]ei&quot;, match = T) # str_view(words, &quot;[^c]ie&quot;, match = T) str_subset(words, &quot;cie&quot;) ## [1] &quot;science&quot; &quot;society&quot; str_subset(words, &quot;cei&quot;) ## [1] &quot;receive&quot; str_subset(words, &quot;[^c]ei&quot;) ## [1] &quot;weigh&quot; str_subset(words, &quot;[^c]ie&quot;) ## [1] &quot;achieve&quot; &quot;believe&quot; &quot;brief&quot; &quot;client&quot; &quot;die&quot; ## [6] &quot;experience&quot; &quot;field&quot; &quot;friend&quot; &quot;lie&quot; &quot;piece&quot; ## [11] &quot;quiet&quot; &quot;tie&quot; &quot;view&quot; 3. Is “q” always followed by a “u”? In the words dataset, we can identify words that contain q followed by any character using the regex “q.”. All the results contain “q” followed by a “u”. Alternatively, we can identify words that contain q followed by anything other than a “u”, using “q[^u]”. Because this does not yield any results, it does seem like “q” is followed by a “u” in this dataset. # list words that contain q followed by any character # str_view(words, &quot;q.&quot;, match = T) str_subset(words, &quot;q.&quot;) ## [1] &quot;equal&quot; &quot;quality&quot; &quot;quarter&quot; &quot;question&quot; &quot;quick&quot; &quot;quid&quot; ## [7] &quot;quiet&quot; &quot;quite&quot; &quot;require&quot; &quot;square&quot; # list words that contain q followed by something other than u (no matches) # str_view(words, &quot;q[^u]&quot;, match = T) str_subset(words, &quot;q[^u]&quot;) ## character(0) 4. Write a regular expression that matches a word if it’s probably written in British English, not American English. Some common differences between British and American English include colour instead of color, so we could look for the sequence “lou”. Other differences include “summarize” vs “summarise”, so we could also include a search for the sequence “ise”. Of course, this will also fetch words such as “practice” that do not have two forms. # str_view(words, &quot;lou|ise&quot;, match = T) str_subset(words, &quot;lou|ise&quot;) ## [1] &quot;advertise&quot; &quot;colour&quot; &quot;exercise&quot; &quot;otherwise&quot; &quot;practise&quot; &quot;raise&quot; ## [7] &quot;realise&quot; &quot;rise&quot; &quot;surprise&quot; 5. Create a regular expression that will match telephone numbers as commonly written in your country. Phone numbers in the US can have the form (XXX) XXX - XXXX. Below is the regex that can be used to identify, very strictly, numbers that adhere to this exact form (including spacing and parentheses). Note the double backslash required to specify a digit. There is a more efficient way to write the regex below using brackets {}, but that is covered in the next section! # str_view ( c(&quot;(123) 456 - 7890 is a fake number,&quot;,&quot;and (555) 555 - 5555 is also a fake number&quot;), &quot;\\\\(\\\\d\\\\d\\\\d\\\\)\\\\s\\\\d\\\\d\\\\d\\\\s-\\\\s\\\\d\\\\d\\\\d\\\\d&quot;) str_extract ( c(&quot;(123) 456 - 7890 is a fake number,&quot;,&quot;and (555) 555 - 5555 is also a fake number&quot;), &quot;\\\\(\\\\d\\\\d\\\\d\\\\)\\\\s\\\\d\\\\d\\\\d\\\\s-\\\\s\\\\d\\\\d\\\\d\\\\d&quot;) ## [1] &quot;(123) 456 - 7890&quot; &quot;(555) 555 - 5555&quot; # same regex using brackets # str_view ( c(&quot;(123) 456 - 7890 is a fake number,&quot;,&quot;and (555) 555 - 5555 is also a fake number&quot;), &quot;\\\\(\\\\d{3}\\\\)\\\\s\\\\d{3}\\\\s-\\\\s\\\\d{4}&quot;) str_extract ( c(&quot;(123) 456 - 7890 is a fake number,&quot;,&quot;and (555) 555 - 5555 is also a fake number&quot;), &quot;\\\\(\\\\d{3}\\\\)\\\\s\\\\d{3}\\\\s-\\\\s\\\\d{4}&quot;) ## [1] &quot;(123) 456 - 7890&quot; &quot;(555) 555 - 5555&quot; writeLines(c(&quot;(\\\\d\\\\d\\\\d)\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d&quot;, &quot;\\\\(\\\\d{3}\\\\)\\\\s\\\\d{3}\\\\s-\\\\s\\\\d{4}&quot;)) ## (\\d\\d\\d)\\d\\d\\d-\\d\\d\\d\\d ## \\(\\d{3}\\)\\s\\d{3}\\s-\\s\\d{4} 14.3.4.1 Exercises 1. Describe the equivalents of ?, +, * in {m,n} form. ? is equivalent to {0,1} in that it matches either 0 or 1 occurances of the preceding regex or character. + is equivalent to {1,} in that it matches 1 or more occurances of the character/regex. * is equivalent to {0,} in that it matches from 0 to any number of occurances of the character/regex. 2. Describe in words what these regular expressions match: (read carefully to see if I’m using a regular expression or a string that defines a regular expression.) ^.*$ This matches any series of characters of 0 to any length. The anchor ^ specifies the start of the string, the . specifies any character, and the * specifies that the previous regex, (.), can be of 0 to any length, and the $ specifies the end of the string. &quot;\\\\{.+\\\\}&quot; This matches a literal bracket, {, followed by 1 or more series of any characters, followed by a literal closing bracket, }. This will match strings like: &quot;{hello world}&quot;. \\d{4}-\\d{2}-\\d{2} This will match 4 digits, followed by a hyphen, two digits, a hyphen, and another 2 digits. For example: “5555-55-55” &quot;\\\\\\\\{4}&quot; This will match a literal backslash repeated 4 times. For example: \\\\\\\\. Here are the above regexes and some sample input so you can see what types of character sequences they match: # str_view(words, &quot;^.*$&quot;) # str_view(&quot;hello world, {hello world} hello world&quot;, &quot;\\\\{.+\\\\}&quot;) # str_view(&quot;5555-55-55asdf&quot;, &quot;\\\\d{4}-\\\\d{2}-\\\\d{2}&quot;) # str_view(&quot;\\\\\\\\\\\\\\\\asdf&quot;, &quot;\\\\\\\\{4}&quot;) str_subset(words, &quot;^.*$&quot;) %&gt;% head(10) ## [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; &quot;account&quot; ## [7] &quot;achieve&quot; &quot;across&quot; &quot;act&quot; &quot;active&quot; str_extract(&quot;hello world, {hello world} hello world&quot;, &quot;\\\\{.+\\\\}&quot;) ## [1] &quot;{hello world}&quot; str_extract(&quot;5555-55-55asdf&quot;, &quot;\\\\d{4}-\\\\d{2}-\\\\d{2}&quot;) ## [1] &quot;5555-55-55&quot; str_extract(&quot;\\\\\\\\\\\\\\\\asdf&quot;, &quot;\\\\\\\\{4}&quot;) ## [1] &quot;\\\\\\\\\\\\\\\\&quot; 3. Create regular expressions to find all words that: Start with three consonants. Have three or more vowels in a row. Have two or more vowel-consonant pairs in a row. # three consonants # str_view(&quot;street&quot;, &#39;[^aeiou]{3}&#39;) str_extract(&quot;street&quot;, &#39;[^aeiou]{3}&#39;) ## [1] &quot;str&quot; #three or more vowels in a row: # str_view(&quot;streeet&quot;, &#39;[aeiou]{3,}&#39;) str_extract(&quot;streeet&quot;, &#39;[aeiou]{3,}&#39;) ## [1] &quot;eee&quot; # two or more vowel-consonant pairs in a row: # str_view(&quot;streettttt&quot;, &#39;([aeiou][^aeiou]|[^aeiou][aeiou]){2,}&#39;) str_extract(&quot;streettttt&quot;, &#39;([aeiou][^aeiou]|[^aeiou][aeiou]){2,}&#39;) ## [1] &quot;reet&quot; 4. Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner. Here are the solutions with some explanations to how I arrived at the answers: Beginner puzzle 1, Beatles: row 1: HE|LL|O+, row2: [PLEASE]+, column 1: [^SPEAK]+, column 2: EP|IP|EF. Solution: row1: HE, row2: LP. First examine row 1 vs column 2- column 2 can either start with the letter E or I, but the only option in row 1 that will fit with this requirement is “HE”. For row 2, the only letter that can be in the bottom left is L, since P, E, A, and S are not allowed by column 1. The bottom right can only contain letters from PLEASE, which means that the option EF for column 2 is not allowed. This leaves EP as the only possibility for column 2. So row 2 must be “LP”. Beginner puzzle 2, Naughty: row 1: .*M?O.*, row2: (AN|FE|BE), column 1: (A|B|C)\\1, column 2: (AB|OE|SK). Solution: row1: BO, row2: BE. Row 1 specifies that there can be 0-1 of any character, followed by 0-1 of M, followed by O, followed by 0-1 of any character. The only stringent requirement is that there exist an O in row 1. Looking at column 1, this does not allow the letter O, so O must be in column 2 of row 1. The only option that allows this is the “OE” option in column 2. The two options for row 2 that end with letter E are “FE” or “BE”. However, column 1 dictates that the first letter of row2 must be either A, B, or C. The only option that fits this is “BB” for column 2. Beginner puzzle 3, Ghost: row 1: (.)+\\1, row2: [^ABRC]+, column 1: [COBRA]+, column 2: (AB|O|OR)+. Solution: row1: OO, row2: OO. Row 1 specifies that 1 or more of any character should be repeated. Given the 2x2 box, this means a pair of letters. Column 1 gives COBRA as choices, but row2 specifies that ABRC cannot be present. This leaves us with O. Also, for column 2, the only option that is available that would not violate row2’s rules is O+. Beginner puzzle 4, Symbolism: row 1: [*]+, row2: /+, column 1: .?.+, column 2: .+. Solution: row1: **, row2://. Row1 wants 1 or more *, whereas row 2 wants one or more /. Column 1 states that any character can occur 0-1 times followed by another character at least 1 or more times. Column 2 states that any character must appear 1 or more times. Beginner puzzle 5, Airstrip One: row 1: 18|19|20, row2: [6789]\\d, column 1: \\d[2480], column 2: 56|94|73. Solution: row1: 19, row2: 84. The only choice for row1 that would satisfy the requirements for column2 is 19 (row1 ends with 9 and col 2 begins with 9). This means that column2 downwards is 94. The only choice for the bottom left that satisfies row2, and column 1 is 8. This leaves us with 19 in row1 and 84 in row2 as the answer. 14.3.5.1 Exercises 1. Describe, in words, what these expressions will match: (.)\\1\\1 This will match any character repeated 3 times. For example, “ooo” in the string “woohooo!” # str_view(&quot;woohooo!&quot;, &quot;(.)\\\\1\\\\1&quot;) str_extract(&quot;woohooo!&quot;, &quot;(.)\\\\1\\\\1&quot;) ## [1] &quot;ooo&quot; &quot;(.)(.)\\\\2\\\\1&quot; This will match a pair of characters followed by the reverse of the pair. For example, “elle” in “belle”. # str_view(&quot;belle&quot;, &quot;(.)(.)\\\\2\\\\1&quot;) str_extract(&quot;belle&quot;, &quot;(.)(.)\\\\2\\\\1&quot;) ## [1] &quot;elle&quot; (..)\\1 This will match any two characters repeated twice. For example, “caca” in “cacao beans”. # str_view(&quot;cacao beans&quot;, &quot;(..)\\\\1&quot;) str_extract(&quot;cacao beans&quot;, &quot;(..)\\\\1&quot;) ## [1] &quot;caca&quot; &quot;(.).\\\\1.\\\\1&quot; This matches any character repeated every other character, such as “lulul” in “lululemon”. # str_view(&quot;lululemon&quot;, &quot;(.).\\\\1.\\\\1&quot;) str_extract(&quot;lululemon&quot;, &quot;(.).\\\\1.\\\\1&quot;) ## [1] &quot;lulul&quot; &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot; This matches 3 characters followed by any number of characters followed by the first 3 characters in reverse, such as “but the tub” in “but the tub is full”. # str_view(&quot;but the tub is full&quot;, &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot;) str_extract(&quot;but the tub is full&quot;, &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot;) ## [1] &quot;but the tub&quot; 2. Construct regular expressions to match words that: Start and end with the same character. &quot;^(.).*\\\\1$&quot; performs this function. The ^ anchor specifies that any character (.) must also be present at the end of the string using the \\$ anchor after the backreference. Below, it will match “regular” but not “expression”. # str_view(c(&quot;regular&quot;,&quot;expressions&quot;, &quot;a&quot;, &quot;aa&quot;), &quot;^(.).*\\\\1$&quot;) str_subset(c(&quot;regular&quot;,&quot;expressions&quot;, &quot;a&quot;, &quot;aa&quot;), &quot;^(.).*\\\\1$&quot;) ## [1] &quot;regular&quot; &quot;aa&quot; Contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice.) &quot;(..).*\\\\1&quot; will work by specifying that any two characters (..) can be separated by any amount of characters .* followed by the same two characters using a backreference. # str_view(c(&quot;church&quot;, &quot;no repeats&quot;, &quot;papaya&quot;), &quot;(..).*\\\\1&quot;) str_subset(c(&quot;church&quot;, &quot;no repeats&quot;, &quot;papaya&quot;), &quot;(..).*\\\\1&quot;) ## [1] &quot;church&quot; &quot;papaya&quot; Contain one letter repeated in at least three places (e.g. “eleven” contains three “e”s.) &quot;(.).*\\\\1.*\\\\1&quot; would work for single word strings. If there is a sentence with multiple words, this regex would match the first occurance of 3 characters, including whitespace, even if they were in different words. If we don’t want this to happen, we would use a “anything except whitespace” instead of .*. This would be a regexp for that purpose: &quot;([^\\\\s])[^\\\\s]*\\\\1[^\\\\s]*\\\\1&quot;. We could also use the boundary regexp \\\\b. # this only works for individual words # str_view(c(&quot;eleven&quot;, &quot;apply&quot;, &quot;papaya&quot;, &quot;bananas&quot;, &quot;will this match spaces?&quot;,&quot;letters next&quot;), &quot;(.).*\\\\1.*\\\\1&quot;) str_subset(c(&quot;eleven&quot;, &quot;apply&quot;, &quot;papaya&quot;, &quot;bananas&quot;, &quot;will this match spaces?&quot;,&quot;letters next&quot;), &quot;(.).*\\\\1.*\\\\1&quot;) ## [1] &quot;eleven&quot; &quot;papaya&quot; ## [3] &quot;bananas&quot; &quot;will this match spaces?&quot; ## [5] &quot;letters next&quot; # this will exclude whitespace and only match if a word within a sentence has a letter repeated 3 times. # str_view(c(&quot;eleven&quot;, &quot;apply&quot;, &quot;papaya&quot;, &quot;bananas&quot;, &quot;will this match spaces?&quot;, &quot;letters next&quot;), &quot;([^\\\\s])[^\\\\s]*\\\\1[^\\\\s]*\\\\1&quot;) str_subset(c(&quot;eleven&quot;, &quot;apply&quot;, &quot;papaya&quot;, &quot;bananas&quot;, &quot;will this match spaces?&quot;, &quot;letters next&quot;), &quot;([^\\\\s])[^\\\\s]*\\\\1[^\\\\s]*\\\\1&quot;) ## [1] &quot;eleven&quot; &quot;papaya&quot; &quot;bananas&quot; 14.4.2 Exercises 1. For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls. Find all words that start or end with x. # single expression str_subset(words, &quot;^x|x$&quot;) ## [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; # multiple str_detect() calls c(words[str_detect(words, &quot;^x&quot;)], words[str_detect(words, &quot;x$&quot;)]) ## [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; Find all words that start with a vowel and end with a consonant. # single expression str_subset(words, &quot;^[aeiou].*[^aeiou]$&quot;) ## [1] &quot;about&quot; &quot;accept&quot; &quot;account&quot; &quot;across&quot; &quot;act&quot; ## [6] &quot;actual&quot; &quot;add&quot; &quot;address&quot; &quot;admit&quot; &quot;affect&quot; ## [11] &quot;afford&quot; &quot;after&quot; &quot;afternoon&quot; &quot;again&quot; &quot;against&quot; ## [16] &quot;agent&quot; &quot;air&quot; &quot;all&quot; &quot;allow&quot; &quot;almost&quot; ## [21] &quot;along&quot; &quot;already&quot; &quot;alright&quot; &quot;although&quot; &quot;always&quot; ## [26] &quot;amount&quot; &quot;and&quot; &quot;another&quot; &quot;answer&quot; &quot;any&quot; ## [31] &quot;apart&quot; &quot;apparent&quot; &quot;appear&quot; &quot;apply&quot; &quot;appoint&quot; ## [36] &quot;approach&quot; &quot;arm&quot; &quot;around&quot; &quot;art&quot; &quot;as&quot; ## [41] &quot;ask&quot; &quot;at&quot; &quot;attend&quot; &quot;authority&quot; &quot;away&quot; ## [46] &quot;awful&quot; &quot;each&quot; &quot;early&quot; &quot;east&quot; &quot;easy&quot; ## [51] &quot;eat&quot; &quot;economy&quot; &quot;effect&quot; &quot;egg&quot; &quot;eight&quot; ## [56] &quot;either&quot; &quot;elect&quot; &quot;electric&quot; &quot;eleven&quot; &quot;employ&quot; ## [61] &quot;end&quot; &quot;english&quot; &quot;enjoy&quot; &quot;enough&quot; &quot;enter&quot; ## [66] &quot;environment&quot; &quot;equal&quot; &quot;especial&quot; &quot;even&quot; &quot;evening&quot; ## [71] &quot;ever&quot; &quot;every&quot; &quot;exact&quot; &quot;except&quot; &quot;exist&quot; ## [76] &quot;expect&quot; &quot;explain&quot; &quot;express&quot; &quot;identify&quot; &quot;if&quot; ## [81] &quot;important&quot; &quot;in&quot; &quot;indeed&quot; &quot;individual&quot; &quot;industry&quot; ## [86] &quot;inform&quot; &quot;instead&quot; &quot;interest&quot; &quot;invest&quot; &quot;it&quot; ## [91] &quot;item&quot; &quot;obvious&quot; &quot;occasion&quot; &quot;odd&quot; &quot;of&quot; ## [96] &quot;off&quot; &quot;offer&quot; &quot;often&quot; &quot;okay&quot; &quot;old&quot; ## [101] &quot;on&quot; &quot;only&quot; &quot;open&quot; &quot;opportunity&quot; &quot;or&quot; ## [106] &quot;order&quot; &quot;original&quot; &quot;other&quot; &quot;ought&quot; &quot;out&quot; ## [111] &quot;over&quot; &quot;own&quot; &quot;under&quot; &quot;understand&quot; &quot;union&quot; ## [116] &quot;unit&quot; &quot;university&quot; &quot;unless&quot; &quot;until&quot; &quot;up&quot; ## [121] &quot;upon&quot; &quot;usual&quot; # multiple str_detect() calls words [ str_detect(words, &quot;^[aeiou]&quot;) &amp; str_detect(words, &quot;[^aeiou]$&quot;)] ## [1] &quot;about&quot; &quot;accept&quot; &quot;account&quot; &quot;across&quot; &quot;act&quot; ## [6] &quot;actual&quot; &quot;add&quot; &quot;address&quot; &quot;admit&quot; &quot;affect&quot; ## [11] &quot;afford&quot; &quot;after&quot; &quot;afternoon&quot; &quot;again&quot; &quot;against&quot; ## [16] &quot;agent&quot; &quot;air&quot; &quot;all&quot; &quot;allow&quot; &quot;almost&quot; ## [21] &quot;along&quot; &quot;already&quot; &quot;alright&quot; &quot;although&quot; &quot;always&quot; ## [26] &quot;amount&quot; &quot;and&quot; &quot;another&quot; &quot;answer&quot; &quot;any&quot; ## [31] &quot;apart&quot; &quot;apparent&quot; &quot;appear&quot; &quot;apply&quot; &quot;appoint&quot; ## [36] &quot;approach&quot; &quot;arm&quot; &quot;around&quot; &quot;art&quot; &quot;as&quot; ## [41] &quot;ask&quot; &quot;at&quot; &quot;attend&quot; &quot;authority&quot; &quot;away&quot; ## [46] &quot;awful&quot; &quot;each&quot; &quot;early&quot; &quot;east&quot; &quot;easy&quot; ## [51] &quot;eat&quot; &quot;economy&quot; &quot;effect&quot; &quot;egg&quot; &quot;eight&quot; ## [56] &quot;either&quot; &quot;elect&quot; &quot;electric&quot; &quot;eleven&quot; &quot;employ&quot; ## [61] &quot;end&quot; &quot;english&quot; &quot;enjoy&quot; &quot;enough&quot; &quot;enter&quot; ## [66] &quot;environment&quot; &quot;equal&quot; &quot;especial&quot; &quot;even&quot; &quot;evening&quot; ## [71] &quot;ever&quot; &quot;every&quot; &quot;exact&quot; &quot;except&quot; &quot;exist&quot; ## [76] &quot;expect&quot; &quot;explain&quot; &quot;express&quot; &quot;identify&quot; &quot;if&quot; ## [81] &quot;important&quot; &quot;in&quot; &quot;indeed&quot; &quot;individual&quot; &quot;industry&quot; ## [86] &quot;inform&quot; &quot;instead&quot; &quot;interest&quot; &quot;invest&quot; &quot;it&quot; ## [91] &quot;item&quot; &quot;obvious&quot; &quot;occasion&quot; &quot;odd&quot; &quot;of&quot; ## [96] &quot;off&quot; &quot;offer&quot; &quot;often&quot; &quot;okay&quot; &quot;old&quot; ## [101] &quot;on&quot; &quot;only&quot; &quot;open&quot; &quot;opportunity&quot; &quot;or&quot; ## [106] &quot;order&quot; &quot;original&quot; &quot;other&quot; &quot;ought&quot; &quot;out&quot; ## [111] &quot;over&quot; &quot;own&quot; &quot;under&quot; &quot;understand&quot; &quot;union&quot; ## [116] &quot;unit&quot; &quot;university&quot; &quot;unless&quot; &quot;until&quot; &quot;up&quot; ## [121] &quot;upon&quot; &quot;usual&quot; Are there any words that contain at least one of each different vowel? To determine whether there are any words that contain at least one of each vowel, we can look for words that have just one of each vowel to make things simpler (any words with more than one of any vowell will still be matched). This is much easier done using multiple str_detect() calls, beacuse we do not care where the vowels show up within the word. “a” can be before “i”, or after “i”, or between “o” and “u”, which makes things complicated if we want to use a single regular expression. We would have to type out all the permutations and chain them using .* and OR (|). This would be a huge expression! Below is the simpler way of chaining multiple str_detect() calls. No words have at least one of each vowell, but some have a,e i, and o (no u). # multiple str_detect() calls words[str_detect(words, &quot;a&quot;) &amp; str_detect(words, &quot;e&quot;) &amp; str_detect(words, &quot;i&quot;) &amp; str_detect(words, &quot;o&quot;) &amp; str_detect (words, &quot;u&quot;)] ## character(0) # words with a, e, i, and o (no u) words[str_detect(words, &quot;a&quot;) &amp; str_detect(words, &quot;e&quot;) &amp; str_detect(words, &quot;i&quot;) &amp; str_detect(words, &quot;o&quot;) ] ## [1] &quot;appropriate&quot; &quot;associate&quot; &quot;organize&quot; &quot;relation&quot; 2. What word has the highest number of vowels? What word has the highest proportion of vowels? (Hint: what is the denominator?) To figure this out, we need a way to count the number of vowels in each word in words, which we can do with str_count() and the regexp [aeiou]. This will let us know the number of vowels in each word, but we also want to keep the information about what the original word was. This means we should perform str_count() on a data frame that contains the original words as well as their index. The book provides this as a tibble. We can perform a mutate() using str_count() and also get the length of each word using str_length(), then calculate the proportion by dividing the two metrics. Then we can use arrange() to find the words with the highest vowel count and vowel proportion. The highest vowel count is 5, which includes appropriate, associate, available, colleague, encourage, experience, individual, and television. The highest vowel proportion is the word “a”, which has a proportion of 1 (it itself is a vowel). Non-1 letter words with the highest vowel proportion include area and idea, at 0.75. # create indexed words tibble df &lt;- tibble( word = words, i = seq_along(word) ) # calculate number of vowels and proportion of vowels by word vowels &lt;- df %&gt;% mutate ( num_vowels = str_count(words, &quot;[aeiou]&quot;), length_word = str_length(words), proportion_vowel = num_vowels/length_word) # sort by words with highest number or proportion of vowels vowels %&gt;% arrange (desc(num_vowels)) ## # A tibble: 980 x 5 ## word i num_vowels length_word proportion_vowel ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 appropriate 48 5 11 0.455 ## 2 associate 57 5 9 0.556 ## 3 available 62 5 9 0.556 ## 4 colleague 166 5 9 0.556 ## 5 encourage 268 5 9 0.556 ## 6 experience 292 5 10 0.5 ## 7 individual 423 5 10 0.5 ## 8 television 846 5 10 0.5 ## 9 absolute 4 4 8 0.5 ## 10 achieve 7 4 7 0.571 ## # … with 970 more rows vowels %&gt;% arrange (desc(proportion_vowel)) ## # A tibble: 980 x 5 ## word i num_vowels length_word proportion_vowel ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 a 1 1 1 1 ## 2 area 49 3 4 0.75 ## 3 idea 412 3 4 0.75 ## 4 age 22 2 3 0.667 ## 5 ago 24 2 3 0.667 ## 6 air 26 2 3 0.667 ## 7 die 228 2 3 0.667 ## 8 due 250 2 3 0.667 ## 9 eat 256 2 3 0.667 ## 10 europe 278 4 6 0.667 ## # … with 970 more rows 14.4.3.1 Exercises 1. In the previous example, you might have noticed that the regular expression matched “flickered”, which is not a colour. Modify the regex to fix the problem. We can modify the regex to detect only legitimate colors by adding a whitespace character on both sides of the name of each color. This will match sentences where a colour is only one word. However, dual-colors like “orangered” or “orange-red” will not be matched unless explicitly stated in the colours vector. To do this, we could change the whitespace to a boundary regex &quot;\\\\b&quot;. colours &lt;- c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colours_fixed &lt;- str_c(&quot;\\\\s&quot;, colours, &quot;\\\\s&quot;) colours_fixed ## [1] &quot;\\\\sred\\\\s&quot; &quot;\\\\sorange\\\\s&quot; &quot;\\\\syellow\\\\s&quot; &quot;\\\\sgreen\\\\s&quot; ## [5] &quot;\\\\sblue\\\\s&quot; &quot;\\\\spurple\\\\s&quot; colour_match &lt;- str_c(colours_fixed, collapse = &quot;|&quot;) has_colour &lt;- str_subset(sentences, colour_match) has_colour ## [1] &quot;Glue the sheet to the dark blue background.&quot; ## [2] &quot;Two blue fish swam in the tank.&quot; ## [3] &quot;A wisp of cloud hung in the blue air.&quot; ## [4] &quot;Leaves turn brown and yellow in the fall.&quot; ## [5] &quot;The spot on the blotter was made by green ink.&quot; ## [6] &quot;The sofa cushion is red and of light weight.&quot; ## [7] &quot;A blue crane is a tall wading bird.&quot; ## [8] &quot;It is hard to erase blue or red ink.&quot; ## [9] &quot;The lamp shone with a steady green flame.&quot; ## [10] &quot;The box is held by a bright red snapper.&quot; ## [11] &quot;The houses are built of red clay bricks.&quot; ## [12] &quot;The red tape bound the smuggled food.&quot; ## [13] &quot;The plant grew large and green in the window.&quot; ## [14] &quot;The purple tie was ten years old.&quot; ## [15] &quot;Bathe and relax in the cool green grass.&quot; ## [16] &quot;The lake sparkled in the red hot sun.&quot; ## [17] &quot;A man in a blue sweater sat at the desk.&quot; ## [18] &quot;The small red neon lamp went out.&quot; ## [19] &quot;Wake and rise, and step into the green outdoors.&quot; ## [20] &quot;The green light in the brown box flickered.&quot; ## [21] &quot;Tear a thin sheet from the yellow pad.&quot; ## [22] &quot;The sky in the west is tinged with orange red.&quot; ## [23] &quot;The red paper brightened the dim stage.&quot; ## [24] &quot;The big red apple fell to the ground.&quot; more &lt;- sentences[str_count(sentences, colour_match) &gt; 1] # str_view_all(more, colour_match) str_extract_all (more, colour_match) ## [[1]] ## [1] &quot; blue &quot; &quot; red &quot; 2. From the Harvard sentences data, extract: The first word from each sentence. To do this, use the ^ anchor and look for all characters that lie before the first whitespace. first_words &lt;- str_extract(sentences, &quot;^.+?\\\\s&quot;) head(first_words,20) ## [1] &quot;The &quot; &quot;Glue &quot; &quot;It&#39;s &quot; &quot;These &quot; &quot;Rice &quot; &quot;The &quot; &quot;The &quot; ## [8] &quot;The &quot; &quot;Four &quot; &quot;Large &quot; &quot;The &quot; &quot;A &quot; &quot;The &quot; &quot;Kick &quot; ## [15] &quot;Help &quot; &quot;A &quot; &quot;Smoky &quot; &quot;The &quot; &quot;The &quot; &quot;The &quot; All words ending in ing. To do this, first find the subset of sentences that have “ing” before a boundary character. Then, within these sentences, extract the word containing “ing” before the boundary character. with_ing &lt;- str_subset(sentences, &quot;ing\\\\b&quot;) head(with_ing) ## [1] &quot;The source of the huge river is the clear spring.&quot; ## [2] &quot;A pot of tea helps to pass the evening.&quot; ## [3] &quot;It snowed, rained, and hailed the same morning.&quot; ## [4] &quot;Take the winding path to reach the lake.&quot; ## [5] &quot;What joy there is in living.&quot; ## [6] &quot;A king ruled the state in the early days.&quot; words_ing &lt;- str_extract(with_ing, &quot;\\\\b\\\\w+ing\\\\b&quot;) words_ing ## [1] &quot;spring&quot; &quot;evening&quot; &quot;morning&quot; &quot;winding&quot; &quot;living&quot; ## [6] &quot;king&quot; &quot;Adding&quot; &quot;making&quot; &quot;raging&quot; &quot;playing&quot; ## [11] &quot;sleeping&quot; &quot;ring&quot; &quot;glaring&quot; &quot;sinking&quot; &quot;dying&quot; ## [16] &quot;Bring&quot; &quot;lodging&quot; &quot;filing&quot; &quot;making&quot; &quot;morning&quot; ## [21] &quot;wearing&quot; &quot;Bring&quot; &quot;wading&quot; &quot;swing&quot; &quot;nothing&quot; ## [26] &quot;ring&quot; &quot;morning&quot; &quot;sing&quot; &quot;sleeping&quot; &quot;painting&quot; ## [31] &quot;king&quot; &quot;walking&quot; &quot;bring&quot; &quot;bring&quot; &quot;shipping&quot; ## [36] &quot;spring&quot; &quot;ring&quot; &quot;winding&quot; &quot;puzzling&quot; &quot;spring&quot; ## [41] &quot;landing&quot; &quot;thing&quot; &quot;waiting&quot; &quot;whistling&quot; &quot;nothing&quot; ## [46] &quot;timing&quot; &quot;thing&quot; &quot;spring&quot; &quot;changing&quot; &quot;drenching&quot; ## [51] &quot;moving&quot; &quot;working&quot; &quot;ring&quot; All plurals. Below is a way to find all words that end in “s”, which should capture plurals that end in “s”, as well as other non-plural words that also end in “s”, such as “press”. To decipher which of these are plurals of singular words, we could subtract the “s” from the words and see if the resulting word matches a dictionary of specified singular words, which we don’t have here. To decipher which words are plurals using only a regex doesn’t seem straightforward and would require something very convoluted. with_plural &lt;- str_subset(sentences, &quot;\\\\b\\\\w+s\\\\s&quot;) head(with_plural) ## [1] &quot;These days a chicken leg is a rare dish.&quot; ## [2] &quot;Rice is often served in round bowls.&quot; ## [3] &quot;The juice of lemons makes fine punch.&quot; ## [4] &quot;The box was thrown beside the parked truck.&quot; ## [5] &quot;The hogs were fed chopped corn and garbage.&quot; ## [6] &quot;Four hours of steady work faced us.&quot; words_plural &lt;- str_extract(with_plural, &quot;\\\\b\\\\w+s\\\\s&quot;) head(words_plural) ## [1] &quot;days &quot; &quot;is &quot; &quot;lemons &quot; &quot;was &quot; &quot;hogs &quot; &quot;hours &quot; 14.4.4.1 Exercises 1. Find all words that come after a “number” like “one”, “two”, “three” etc. Pull out both the number and the word. I modeled the answers after the example provided in the book, and modified the code to work with the new regex. The regex I made has numbers one - ten, then followed by a suffix “teen” for cases such as “fourteen” or “ty” for cases such as “sixty”. There are some other cases that I added in that do not follow this convention such as “twenty” or “thirteen”. This should be followed by a whitespace then 1 or more non-whitespace character. We can then extract the results using a combination of str_subset() and str_extract(). Str_match() or tidyr::extract() can pull out both the number and the words that are encased in the parentheses. # regex to detect numbers with words after them afternumber &lt;- &quot;\\\\b((one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fifteen|eighteen|twenty|thirty|fifty|eighty)(ty|teen)?) ([^ ]+)&quot; # find all sentences that have a match with the regex has_number &lt;- sentences %&gt;% str_subset(afternumber) %&gt;% head(10) # display what the match for each sentence was has_number %&gt;% str_extract(afternumber) ## [1] &quot;fifty bonds.&quot; &quot;seven books&quot; &quot;two met&quot; &quot;sixteen weeks.&quot; ## [5] &quot;two factors&quot; &quot;three lists&quot; &quot;thirty times.&quot; &quot;seven is&quot; ## [9] &quot;two when&quot; &quot;ten inches.&quot; # display the separate parts of each match. has_number %&gt;% str_match(afternumber) ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;fifty bonds.&quot; &quot;fifty&quot; &quot;fifty&quot; NA &quot;bonds.&quot; ## [2,] &quot;seven books&quot; &quot;seven&quot; &quot;seven&quot; NA &quot;books&quot; ## [3,] &quot;two met&quot; &quot;two&quot; &quot;two&quot; NA &quot;met&quot; ## [4,] &quot;sixteen weeks.&quot; &quot;sixteen&quot; &quot;six&quot; &quot;teen&quot; &quot;weeks.&quot; ## [5,] &quot;two factors&quot; &quot;two&quot; &quot;two&quot; NA &quot;factors&quot; ## [6,] &quot;three lists&quot; &quot;three&quot; &quot;three&quot; NA &quot;lists&quot; ## [7,] &quot;thirty times.&quot; &quot;thirty&quot; &quot;thirty&quot; NA &quot;times.&quot; ## [8,] &quot;seven is&quot; &quot;seven&quot; &quot;seven&quot; NA &quot;is&quot; ## [9,] &quot;two when&quot; &quot;two&quot; &quot;two&quot; NA &quot;when&quot; ## [10,] &quot;ten inches.&quot; &quot;ten&quot; &quot;ten&quot; NA &quot;inches.&quot; # use tidyr::extract() to extract matches for each part of the regex and display output in a neat table tibble(sentence = sentences) %&gt;% tidyr::extract( sentence, c(&quot;complete_number&quot;, &quot;number&quot;,&quot;teen_or_ty&quot;, &quot;word&quot;), afternumber, remove = FALSE ) %&gt;% select(sentence, complete_number, word) %&gt;% na.omit() ## # A tibble: 26 x 3 ## sentence complete_number word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The girl at the booth sold fifty bonds. fifty bonds. ## 2 The rope will bind the seven books at once. seven books ## 3 The two met while playing on the sand. two met ## 4 The lease ran out in sixteen weeks. sixteen weeks. ## 5 There are more than two factors here. two factors ## 6 Type out three lists of orders. three lists ## 7 He said the same phrase thirty times. thirty times. ## 8 Two plus seven is less than ten. seven is ## 9 Drop the two when you add the figures. two when ## 10 There the flood mark is ten inches. ten inches. ## # … with 16 more rows 2. Find all contractions. Separate out the pieces before and after the apostrophe. To find contractions, we can search for a sequence of letters using \\w, followed by a contraction with either “’ or -”, and then another sequence of words after that, followed by a boundary. Using this regex we can then use str_subset(), str_extract(), str_match(), and tidyr::extract() to obtain the pieces before and after the apostrophe. If we don’t care about hyphens, we can remove it from the regex. contractions &lt;- &quot;\\\\b(\\\\w+)(&#39;|-)(\\\\w+)\\\\b&quot; has_contraction &lt;- sentences %&gt;% str_subset(contractions) %&gt;% head(10) has_contraction %&gt;% str_extract(contractions) ## [1] &quot;It&#39;s&quot; &quot;man&#39;s&quot; &quot;don&#39;t&quot; &quot;store&#39;s&quot; ## [5] &quot;apple-shaped&quot; &quot;hot-cross&quot; &quot;workmen&#39;s&quot; &quot;Let&#39;s&quot; ## [9] &quot;sun&#39;s&quot; &quot;child&#39;s&quot; has_contraction %&gt;% str_match(contractions) ## [,1] [,2] [,3] [,4] ## [1,] &quot;It&#39;s&quot; &quot;It&quot; &quot;&#39;&quot; &quot;s&quot; ## [2,] &quot;man&#39;s&quot; &quot;man&quot; &quot;&#39;&quot; &quot;s&quot; ## [3,] &quot;don&#39;t&quot; &quot;don&quot; &quot;&#39;&quot; &quot;t&quot; ## [4,] &quot;store&#39;s&quot; &quot;store&quot; &quot;&#39;&quot; &quot;s&quot; ## [5,] &quot;apple-shaped&quot; &quot;apple&quot; &quot;-&quot; &quot;shaped&quot; ## [6,] &quot;hot-cross&quot; &quot;hot&quot; &quot;-&quot; &quot;cross&quot; ## [7,] &quot;workmen&#39;s&quot; &quot;workmen&quot; &quot;&#39;&quot; &quot;s&quot; ## [8,] &quot;Let&#39;s&quot; &quot;Let&quot; &quot;&#39;&quot; &quot;s&quot; ## [9,] &quot;sun&#39;s&quot; &quot;sun&quot; &quot;&#39;&quot; &quot;s&quot; ## [10,] &quot;child&#39;s&quot; &quot;child&quot; &quot;&#39;&quot; &quot;s&quot; tibble(sentence = sentences) %&gt;% tidyr::extract( sentence, c(&quot;before_contraction&quot;, &quot;contraction&quot;,&quot;after_contraction&quot;), contractions, remove = FALSE ) %&gt;% mutate ( complete = str_extract (sentence, contractions) )%&gt;% na.omit() ## # A tibble: 17 x 5 ## sentence before_contracti… contraction after_contracti… complete ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 It&#39;s easy to te… It &#39; s It&#39;s ## 2 The soft cushio… man &#39; s man&#39;s ## 3 Open the crate … don &#39; t don&#39;t ## 4 Add the store&#39;s… store &#39; s store&#39;s ## 5 The fruit of a … apple - shaped apple-s… ## 6 A zestful food … hot - cross hot-cro… ## 7 The beam droppe… workmen &#39; s workmen… ## 8 Let&#39;s all join … Let &#39; s Let&#39;s ## 9 The copper bowl… sun &#39; s sun&#39;s ## 10 A child&#39;s wit s… child &#39; s child&#39;s ## 11 A ripe plum is … king &#39; s king&#39;s ## 12 It&#39;s a dense cr… It &#39; s It&#39;s ## 13 We don&#39;t get mu… don &#39; t don&#39;t ## 14 Ripe pears are … queen &#39; s queen&#39;s ## 15 We don&#39;t like t… don &#39; t don&#39;t ## 16 Dig deep in the… pirate &#39; s pirate&#39;s ## 17 She saw a cat i… neighbor &#39; s neighbo… 14.4.5.1 Exercises 1. Replace all forward slashes in a string with backslashes. sample &lt;- &quot;this/has/too/many/slashes/!&quot; sample ## [1] &quot;this/has/too/many/slashes/!&quot; str_replace_all (sample, &quot;/&quot;, &quot;\\\\\\\\&quot;) ## [1] &quot;this\\\\has\\\\too\\\\many\\\\slashes\\\\!&quot; 2. Implement a simple version of str_to_lower() using replace_all(). It looks like stringr doesn’t have a “to lowercase” regex such as \\\\L in perl, which can convert a backreference to lower case (\\\\U will convert it to uppercase). This makes us write bulky and inefficient code, having to specify “A = a”, “B = b”, etc. Instead of using str_replace_all(), we can use the baseR gsub(), and group all caps ([A-Z]) in parentheses, and then use &quot;\\\\L\\\\1&quot; to turn every instance of a capital to lowercase. sample &lt;- &quot;The CaPS iS gOiNg CrAZy!&quot; # gsub does this more efficiently! gsub (&quot;([A-Z])&quot;, &quot;\\\\L\\\\1&quot;, perl=TRUE, sample) ## [1] &quot;the caps is going crazy!&quot; # this doesn&#39;t work in stringr!! str_replace_all ( sample, &quot;([A-Z])&quot;, &quot;\\\\L\\\\1&quot;) ## [1] &quot;LThe LCaLPLS iLS gLOiLNg LCrLALZy!&quot; # if we must use stringr, we have to painfully write out all the replacements... str_replace_all (sample, c(&quot;A&quot; =&quot;a&quot;, &quot;C&quot; = &quot;c&quot;, &quot;T&quot; = &quot;t&quot;, &quot;P&quot; = &quot;p&quot;, &quot;S&quot; = &quot;s&quot;, &quot;O&quot; = &quot;o&quot;, &quot;N&quot; = &quot;n&quot;, &quot;Z&quot; = &quot;z&quot;)) ## [1] &quot;the caps is going crazy!&quot; 3. Switch the first and last letters in words. Which of those strings are still words? To swich the first and last letters, we can use paretheses along with anchor ^ and $, and then swap the backreferences. Then, we can use the function intersect() to determine which of these swapped words are still present in the original words dataset. # swap the first and last letters in the words dataset swapped &lt;- str_replace (words, &quot;^(.)(.*)(.)$&quot;, &quot;\\\\3\\\\2\\\\1&quot;) head(swapped, 10) ## [1] &quot;a&quot; &quot;ebla&quot; &quot;tboua&quot; &quot;ebsoluta&quot; &quot;tccepa&quot; &quot;tccouna&quot; ## [7] &quot;echieva&quot; &quot;scrosa&quot; &quot;tca&quot; &quot;ectiva&quot; # find out which words in the swapped list still match words in the original words dataset intersect(words, swapped) ## [1] &quot;a&quot; &quot;america&quot; &quot;area&quot; &quot;dad&quot; &quot;dead&quot; ## [6] &quot;deal&quot; &quot;dear&quot; &quot;depend&quot; &quot;dog&quot; &quot;educate&quot; ## [11] &quot;else&quot; &quot;encourage&quot; &quot;engine&quot; &quot;europe&quot; &quot;evidence&quot; ## [16] &quot;example&quot; &quot;excuse&quot; &quot;exercise&quot; &quot;expense&quot; &quot;experience&quot; ## [21] &quot;eye&quot; &quot;god&quot; &quot;health&quot; &quot;high&quot; &quot;knock&quot; ## [26] &quot;lead&quot; &quot;level&quot; &quot;local&quot; &quot;nation&quot; &quot;no&quot; ## [31] &quot;non&quot; &quot;on&quot; &quot;rather&quot; &quot;read&quot; &quot;refer&quot; ## [36] &quot;remember&quot; &quot;serious&quot; &quot;stairs&quot; &quot;test&quot; &quot;tonight&quot; ## [41] &quot;transport&quot; &quot;treat&quot; &quot;trust&quot; &quot;window&quot; &quot;yesterday&quot; 14.4.6.1 Exercises 1. Split up a string like “apples, pears, and bananas” into individual components. sample &lt;- &quot;apples, pears, and bananas&quot; # use &quot;, &quot; to split the string str_split(sample, &quot; &quot;)[[1]] ## [1] &quot;apples,&quot; &quot;pears,&quot; &quot;and&quot; &quot;bananas&quot; # use boundary() to split the string str_split(sample, boundary(&quot;word&quot;))[[1]] ## [1] &quot;apples&quot; &quot;pears&quot; &quot;and&quot; &quot;bananas&quot; 2. Why is it better to split up by boundary(&quot;word&quot;) than &quot; &quot;? It is better to split up by boundary(“word”) than by &quot; &quot; because splitting by &quot; &quot; will cause some words to include puncutation marks, such as commmas or periods. boundary(“word”) takes these into consideration when splitting, and will not include them in the output. See the comparison provided in my answer to the previous exercise, which uses both ways to split the string. 3. What does splitting with an empty string (“”) do? Experiment, and then read the documentation. Splitting with an empty string (“”) will split the string into each individual character. The empty string can be thought of as the “space” between each individual character in the string. The documentation states that “an empty pattern,”“, is equivalent to boundary(”character“)”. Below I compare the two methods, which indeed provide the same output. sample &lt;- &quot;apples, pears, and bananas&quot; # use &quot;, &quot; to split the string str_split(sample, &quot;&quot;)[[1]] ## [1] &quot;a&quot; &quot;p&quot; &quot;p&quot; &quot;l&quot; &quot;e&quot; &quot;s&quot; &quot;,&quot; &quot; &quot; &quot;p&quot; &quot;e&quot; &quot;a&quot; &quot;r&quot; &quot;s&quot; &quot;,&quot; &quot; &quot; &quot;a&quot; &quot;n&quot; ## [18] &quot;d&quot; &quot; &quot; &quot;b&quot; &quot;a&quot; &quot;n&quot; &quot;a&quot; &quot;n&quot; &quot;a&quot; &quot;s&quot; str_split(sample, boundary(&quot;character&quot;))[[1]] ## [1] &quot;a&quot; &quot;p&quot; &quot;p&quot; &quot;l&quot; &quot;e&quot; &quot;s&quot; &quot;,&quot; &quot; &quot; &quot;p&quot; &quot;e&quot; &quot;a&quot; &quot;r&quot; &quot;s&quot; &quot;,&quot; &quot; &quot; &quot;a&quot; &quot;n&quot; ## [18] &quot;d&quot; &quot; &quot; &quot;b&quot; &quot;a&quot; &quot;n&quot; &quot;a&quot; &quot;n&quot; &quot;a&quot; &quot;s&quot; 14.5.1 Exercises 1. How would you find all strings containing with regex() vs. with fixed()? When using regex(), we must use four backslashes for the regex to detect literal backslashes in a string. When using fixed(), we can get away with only using two backslashes to detect literal backslashes in a string. Below is an example of using both regex() and fixed() in str_subset. sample &lt;- c(&quot;this has a \\\\ backslash&quot;, &quot;this doesn&#39;t have one&quot;, &quot;this has a \\\\ backslash&quot; ) sample ## [1] &quot;this has a \\\\ backslash&quot; &quot;this doesn&#39;t have one&quot; ## [3] &quot;this has a \\\\ backslash&quot; str_subset(sample, regex(&quot;\\\\\\\\&quot;)) ## [1] &quot;this has a \\\\ backslash&quot; &quot;this has a \\\\ backslash&quot; str_subset(sample, fixed(&quot;\\\\&quot;)) ## [1] &quot;this has a \\\\ backslash&quot; &quot;this has a \\\\ backslash&quot; 2. What are the five most common words in sentences? The top five most common words are “the”/“The”, “of”, “a”, “to”, and “and”. To answer this question, we first use str_extract_all() with boundary(“word”), to get all the words in a character matrix. Then, we convert the matrix into a vector, then in to a tibble so we can perform dplyr commands. Then, we can use group_by() to group by individual words, then count() to determine how many times each word was used, then arrange() to find the top words. # extract all the words in sentences word_matrix &lt;- str_extract_all(sentences, boundary(&quot;word&quot;), simplify = T) word_vector &lt;- as.vector(word_matrix) head(word_vector) ## [1] &quot;The&quot; &quot;Glue&quot; &quot;It&#39;s&quot; &quot;These&quot; &quot;Rice&quot; &quot;The&quot; # use group_by, count(), and arrange() to find the top 5 words tibble(word_vector) %&gt;% group_by(word_vector) %&gt;% count() %&gt;% arrange(desc(n)) ## # A tibble: 2,062 x 2 ## # Groups: word_vector [2,062] ## word_vector n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;&quot; 2892 ## 2 the 489 ## 3 The 262 ## 4 of 132 ## 5 a 130 ## 6 to 119 ## 7 and 118 ## 8 in 85 ## 9 is 81 ## 10 A 72 ## # … with 2,052 more rows 14.7.1 Exercises 1. Find the stringi functions that: Count the number of words. Find duplicated strings. Generate random text. To find these functions, first load the stringi package and type “stri_”. If using rstudio, you can then see all the various functions that show up and read a short description if you hover over their names. Below are the functions that perform the actions specified. library(stringi) # count number of words stri_count_words(sentences) %&gt;% head(10) ## [1] 8 8 9 9 7 7 8 8 7 8 # find duplicated strings sample &lt;- c(&quot;hello&quot;, &quot;hello&quot;, &quot;world&quot;, &quot;this&quot;, &quot;hello&quot;) stri_duplicated(sample) ## [1] FALSE TRUE FALSE FALSE TRUE # generate random text stri_rand_strings(5, 10) ## [1] &quot;kGVJfpf8Mq&quot; &quot;NAIC4xVNM2&quot; &quot;ROs0bGZSoF&quot; &quot;CFEtDIxyfV&quot; &quot;Fv69xsnoCY&quot; 2. How do you control the language that stri_sort() uses for sorting? To determine the language that stri_sort() uses for sorting, you must specify the locale argument. For stri_sort(), you input a character vector, and then specify the decreasing argument to get a sorted version of the character vector. Decreasing = T will sort the vector such that the strings that start with a letter closer to the end of the alphabet are ranked earlier. # sort the sentences vector, locale = en_US stri_sort(sentences, decreasing = TRUE, locale = &quot;en_US&quot;) %&gt;% head(10) ## [1] &quot;You cannot brew tea in a cold pot.&quot; ## [2] &quot;Yell and clap as the curtain slides back.&quot; ## [3] &quot;Xew pants lack cuffs and pockets.&quot; ## [4] &quot;Write fast, if you want to finish early.&quot; ## [5] &quot;Write at once or you may forget it.&quot; ## [6] &quot;Write a fond note to the friend you cherish.&quot; ## [7] &quot;Wood is best for making toys and blocks.&quot; ## [8] &quot;Women form less than half of the group.&quot; ## [9] &quot;Wipe the grease off his dirty face.&quot; ## [10] &quot;Will you please answer that phone.&quot; "],
["chapter-15-factors.html", "Chapter 15 - Factors 15.3.1 Exercises 15.4.1 Exercises 15.5.1 Exercises", " Chapter 15 - Factors library(forcats) library(tidyverse) 15.3.1 Exercises 1. Explore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot? We can explore the distribution either by looking at summary(gss_cat$rincome) or by plotting the data using geom_bar(). rincome is a column of factors divided in to several categories. From the summary we can see that most people who reported their income lie in the 25,000 or more category, but a lot of people did not answer the survey or were not applicable as well. The names of each category are fairly long, and in the default plot the labels are overlapping. To improve the default plot, we can tilt the axis labels so that they are readable using the theme() option in ggplot2. summary(gss_cat$rincome) ## No answer Don&#39;t know Refused $25000 or more $20000 - 24999 ## 183 267 975 7363 1283 ## $15000 - 19999 $10000 - 14999 $8000 to 9999 $7000 to 7999 $6000 to 6999 ## 1048 1168 340 188 215 ## $5000 to 5999 $4000 to 4999 $3000 to 3999 $1000 to 2999 Lt $1000 ## 227 226 276 395 286 ## Not applicable ## 7043 ggplot(gss_cat, aes(rincome)) + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 2. What is the most common relig in this survey? What’s the most common partyid? Based on the bar plots, the most common relig in this survey is Protestant. The most common partyid is Independent. ggplot(gss_cat, aes(relig)) + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) ggplot(gss_cat, aes(partyid)) + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 3. Which relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualisation? You can find out with a table by using dplyr commands, first grouping by denom, then finding the proportion of each religion within a denom. To find out with a visualization, you can use an aesthetic mapping in ggplot2 to fill in a barplot with colors based on the relig. To make the proportion easier to see, you can specify position = “fill”. gss_cat %&gt;% group_by (denom, relig) %&gt;% count() ## # A tibble: 47 x 3 ## # Groups: denom, relig [47] ## denom relig n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 No answer No answer 93 ## 2 No answer Christian 2 ## 3 No answer Protestant 22 ## 4 Don&#39;t know Christian 11 ## 5 Don&#39;t know Protestant 41 ## 6 No denomination Christian 452 ## 7 No denomination Other 7 ## 8 No denomination Protestant 1224 ## 9 Other Protestant 2534 ## 10 Episcopal Protestant 397 ## # … with 37 more rows ggplot(gss_cat, aes(denom)) + geom_bar(aes(fill = relig)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) ggplot(gss_cat, aes(denom)) + geom_bar(aes(fill = relig), position = &quot;fill&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 15.4.1 Exercises 1. There are some suspiciously high numbers in tvhours. Is the mean a good summary? If there are very high outliers in any distribution, the mean will be inflated. Since the mean is the average of the numbers, any extremely high numbers will increase the mean. Therefore, the mean is not a good summary. In this instance, the median may give a better measure of where the data is centered. It is always a good idea to be aware of what types of outliers exist in your data. Plotting a histogram of tvhours below, we can see that in some cases there are over 20 hours of tv! The distribution is skewed to the right. ggplot(gss_cat, aes (tvhours)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 10146 rows containing non-finite values (stat_bin). 2. For each factor in gss_cat identify whether the order of the levels is arbitrary or principled. We can determine this by examining the levels of each one of the factor columns in gss_cat using levels(), and then determining whether there is a principle to the ordering listed. Here is my assessment: marital - arbitrary, race - arbitrary, rincome - principled (based on decreasing income levels), partyid - principled (going from strong republican, slowly towards strong democrat), relig - arbitrary, denom - arbitrary. levels(gss_cat$marital) ## [1] &quot;No answer&quot; &quot;Never married&quot; &quot;Separated&quot; &quot;Divorced&quot; ## [5] &quot;Widowed&quot; &quot;Married&quot; levels(gss_cat$race) ## [1] &quot;Other&quot; &quot;Black&quot; &quot;White&quot; &quot;Not applicable&quot; levels(gss_cat$rincome) ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Refused&quot; &quot;$25000 or more&quot; ## [5] &quot;$20000 - 24999&quot; &quot;$15000 - 19999&quot; &quot;$10000 - 14999&quot; &quot;$8000 to 9999&quot; ## [9] &quot;$7000 to 7999&quot; &quot;$6000 to 6999&quot; &quot;$5000 to 5999&quot; &quot;$4000 to 4999&quot; ## [13] &quot;$3000 to 3999&quot; &quot;$1000 to 2999&quot; &quot;Lt $1000&quot; &quot;Not applicable&quot; levels(gss_cat$partyid) ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Other party&quot; ## [4] &quot;Strong republican&quot; &quot;Not str republican&quot; &quot;Ind,near rep&quot; ## [7] &quot;Independent&quot; &quot;Ind,near dem&quot; &quot;Not str democrat&quot; ## [10] &quot;Strong democrat&quot; levels(gss_cat$relig) ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; ## [3] &quot;Inter-nondenominational&quot; &quot;Native american&quot; ## [5] &quot;Christian&quot; &quot;Orthodox-christian&quot; ## [7] &quot;Moslem/islam&quot; &quot;Other eastern&quot; ## [9] &quot;Hinduism&quot; &quot;Buddhism&quot; ## [11] &quot;Other&quot; &quot;None&quot; ## [13] &quot;Jewish&quot; &quot;Catholic&quot; ## [15] &quot;Protestant&quot; &quot;Not applicable&quot; levels(gss_cat$denom) ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;No denomination&quot; ## [4] &quot;Other&quot; &quot;Episcopal&quot; &quot;Presbyterian-dk wh&quot; ## [7] &quot;Presbyterian, merged&quot; &quot;Other presbyterian&quot; &quot;United pres ch in us&quot; ## [10] &quot;Presbyterian c in us&quot; &quot;Lutheran-dk which&quot; &quot;Evangelical luth&quot; ## [13] &quot;Other lutheran&quot; &quot;Wi evan luth synod&quot; &quot;Lutheran-mo synod&quot; ## [16] &quot;Luth ch in america&quot; &quot;Am lutheran&quot; &quot;Methodist-dk which&quot; ## [19] &quot;Other methodist&quot; &quot;United methodist&quot; &quot;Afr meth ep zion&quot; ## [22] &quot;Afr meth episcopal&quot; &quot;Baptist-dk which&quot; &quot;Other baptists&quot; ## [25] &quot;Southern baptist&quot; &quot;Nat bapt conv usa&quot; &quot;Nat bapt conv of am&quot; ## [28] &quot;Am bapt ch in usa&quot; &quot;Am baptist asso&quot; &quot;Not applicable&quot; 3. Why did moving “Not applicable” to the front of the levels move it to the bottom of the plot? In the text, the “after” argument was not specified, so the default value of after = OL was used, which puts “Not applicable” to the front. This results in it being placed before “No answer”. The way that geom_point() plots the categories is such that the first level is plotted at the bottom. This is why “Not applicable” appears at the bottom of the plot. levels(gss_cat$rincome) ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Refused&quot; &quot;$25000 or more&quot; ## [5] &quot;$20000 - 24999&quot; &quot;$15000 - 19999&quot; &quot;$10000 - 14999&quot; &quot;$8000 to 9999&quot; ## [9] &quot;$7000 to 7999&quot; &quot;$6000 to 6999&quot; &quot;$5000 to 5999&quot; &quot;$4000 to 4999&quot; ## [13] &quot;$3000 to 3999&quot; &quot;$1000 to 2999&quot; &quot;Lt $1000&quot; &quot;Not applicable&quot; levels(fct_relevel(gss_cat$rincome, &quot;Not applicable&quot;)) ## [1] &quot;Not applicable&quot; &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Refused&quot; ## [5] &quot;$25000 or more&quot; &quot;$20000 - 24999&quot; &quot;$15000 - 19999&quot; &quot;$10000 - 14999&quot; ## [9] &quot;$8000 to 9999&quot; &quot;$7000 to 7999&quot; &quot;$6000 to 6999&quot; &quot;$5000 to 5999&quot; ## [13] &quot;$4000 to 4999&quot; &quot;$3000 to 3999&quot; &quot;$1000 to 2999&quot; &quot;Lt $1000&quot; 15.5.1 Exercises 1. How have the proportions of people identifying as Democrat, Republican, and Independent changed over time? To answer this, first lump together all the Democrat-associated categories into one category named “All Democrats”, and likewise for Republican and Independent. Then, we can plot the change in the number of people that associate with these categories over time. gss_cat %&gt;% mutate(partyid = fct_recode(partyid, &quot;Republican&quot; = &quot;Strong republican&quot;, &quot;Republican&quot; = &quot;Not str republican&quot;, &quot;Independent&quot; = &quot;Ind,near rep&quot;, &quot;Independent&quot; = &quot;Ind,near dem&quot;, &quot;Democrat&quot; = &quot;Not str democrat&quot;, &quot;Democrat&quot; = &quot;Strong democrat&quot;, &quot;Other&quot; = &quot;No answer&quot;, &quot;Other&quot; = &quot;Don&#39;t know&quot;, &quot;Other&quot; = &quot;Other party&quot; )) %&gt;% ggplot(aes(x = year))+ geom_bar(aes(fill = partyid)) 2. How could you collapse rincome into a small set of categories? We could make the income-blocks larger, by combining the income categories into blocks of “less than 5000”, “5000 to 10000”, “10000 to 25000”, and “25000 or more”. We could also lump together “Refused”, “dont know”, “no answer”, and “not applicable” into an “other” category, although this may be dangerous. Some questions we might ask before lumping these groups together are: do people who refused to take the survey have behavioral differences that may matter in some contexts? Why did people not answer the survey–did they not have had the means to do so? Do these categories preferentially lie in specific districts? gss_cat %&gt;% mutate(rincome = fct_recode(rincome, &quot;less than 5000&quot; = &quot;Lt $1000&quot;, &quot;less than 5000&quot; = &quot;$1000 to 2999&quot;, &quot;less than 5000&quot; = &quot;$3000 to 3999&quot;, &quot;less than 5000&quot; = &quot;$4000 to 4999&quot;, &quot;5000 to 10000&quot; = &quot;$5000 to 5999&quot;, &quot;5000 to 10000&quot; = &quot;$6000 to 6999&quot;, &quot;5000 to 10000&quot; = &quot;$7000 to 7999&quot;, &quot;5000 to 10000&quot; = &quot;$8000 to 9999&quot;, &quot;10000 to 25000&quot; = &quot;$10000 - 14999&quot;, &quot;10000 to 25000&quot; = &quot;$15000 - 19999&quot;, &quot;10000 to 25000&quot; = &quot;$20000 - 24999&quot;, &quot;Other&quot; = &quot;No answer&quot;, &quot;Other&quot; = &quot;Don&#39;t know&quot;, &quot;Other&quot; = &quot;Refused&quot;, &quot;Other&quot; = &quot;Not applicable&quot; )) %&gt;% count(rincome) ## # A tibble: 5 x 2 ## rincome n ## &lt;fct&gt; &lt;int&gt; ## 1 Other 8468 ## 2 $25000 or more 7363 ## 3 10000 to 25000 3499 ## 4 5000 to 10000 970 ## 5 less than 5000 1183 "],
["chapter-16-dates-and-times.html", "Chapter 16 - Dates and times 16.2.4 Exercises 16.3.4 Exercises 16.4.5 Exercises", " Chapter 16 - Dates and times library(lubridate) library(nycflights13) 16.2.4 Exercises 1. What happens if you parse a string that contains invalid dates? You get an error message telling you the number of strings that failed to parse. In this instance, the error message is “1 failed to parse.” A vector is returned containing the strings of dates that parsed correctly with the invalid dates replaced with NA. ymd(c(&quot;2010-10-10&quot;, &quot;bananas&quot;)) ## Warning: 1 failed to parse. ## [1] &quot;2010-10-10&quot; NA 2. What does the tzone argument to today() do? Why is it important? The documentation states that the tzone argument in today() accepts a character vector specifying the time zone you would like to find the curent date of. It will default to the system time zone on your computer if left unspecified. This is important because, depending on the time zone, the date could be different. For example, the US west coast is 3 hours behind the US east coast. If it is 11PM on the west coast, the east coast is one day ahead. Below is the example provided in the documentation. today() ## [1] &quot;2020-01-21&quot; today(&quot;GMT&quot;) ## [1] &quot;2020-01-22&quot; today() == today(&quot;GMT&quot;) # not always true ## [1] FALSE 3. Use the appropriate lubridate function to parse each of the following dates: Based on the format of the string, the order of the month, year, and day parameters will determine which appropriate lubridate function should be used. For example, the first string, d1, is in month-day-year format, so we should use the lubridate function mdy. I apply the same principle to the other cases. d1 &lt;- &quot;January 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 mdy(d1) ## [1] &quot;2010-01-01&quot; ymd(d2) ## [1] &quot;2015-03-07&quot; dmy(d3) ## [1] &quot;2017-06-06&quot; mdy(d4) ## [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; mdy(d5) ## [1] &quot;2014-12-30&quot; 16.3.4 Exercises make_datetime_100 &lt;- function(year, month, day, time) { make_datetime(year, month, day, time %/% 100, time %% 100) } flights_dt &lt;- flights %&gt;% filter(!is.na(dep_time), !is.na(arr_time)) %&gt;% mutate( dep_time = make_datetime_100(year, month, day, dep_time), arr_time = make_datetime_100(year, month, day, arr_time), sched_dep_time = make_datetime_100(year, month, day, sched_dep_time), sched_arr_time = make_datetime_100(year, month, day, sched_arr_time) ) %&gt;% select(origin, dest, ends_with(&quot;delay&quot;), ends_with(&quot;time&quot;)) flights_dt ## # A tibble: 328,063 x 9 ## origin dest dep_delay arr_delay dep_time sched_dep_time ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 EWR IAH 2 11 2013-01-01 05:17:00 2013-01-01 05:15:00 ## 2 LGA IAH 4 20 2013-01-01 05:33:00 2013-01-01 05:29:00 ## 3 JFK MIA 2 33 2013-01-01 05:42:00 2013-01-01 05:40:00 ## 4 JFK BQN -1 -18 2013-01-01 05:44:00 2013-01-01 05:45:00 ## 5 LGA ATL -6 -25 2013-01-01 05:54:00 2013-01-01 06:00:00 ## 6 EWR ORD -4 12 2013-01-01 05:54:00 2013-01-01 05:58:00 ## 7 EWR FLL -5 19 2013-01-01 05:55:00 2013-01-01 06:00:00 ## 8 LGA IAD -3 -14 2013-01-01 05:57:00 2013-01-01 06:00:00 ## 9 JFK MCO -3 -8 2013-01-01 05:57:00 2013-01-01 06:00:00 ## 10 LGA ORD -2 8 2013-01-01 05:58:00 2013-01-01 06:00:00 ## # … with 328,053 more rows, and 3 more variables: arr_time &lt;dttm&gt;, ## # sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt; 1. How does the distribution of flight times within a day change over the course of the year? Within a day, we want to observe how the flight times differ. This means we should look at how flight times differ by the hour (ie how many flights are taking off at every hour of the day). Now, we want to see how this behavior changes over the course of the year (ie how does this graph look like when plotted monthly)? We observe that the distribution of flights within a day does not significantly change over the course of the year. The same trend is followed in which there is a peak of flights around 8am, a dip in flights from 10am-12pm, and then a slow drop off in number of flights past 7pm. # flights per hour for the entire year flights_dt %&gt;% mutate(hour = hour(dep_time)) %&gt;% group_by(hour)%&gt;% summarize(numflights_per_hour = n())%&gt;% ggplot(aes(x = hour, y = numflights_per_hour)) + geom_line() # split the above graph into months flights_dt %&gt;% mutate(hour = hour(dep_time), month = as.factor(month(dep_time))) %&gt;% group_by(month,hour)%&gt;% summarize(numflights_per_hour = n())%&gt;% ggplot(aes(x = hour, y = numflights_per_hour)) + geom_line(aes(color = month)) 2. Compare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings. First calculate our own version of dep_time by adding sched_dep_time and dep_delay together, then compare the result to the provided dep_time in the table. When filtering for values in dep_time that do not match the calculated version, we find 1205 discrepancies out of the 328,063 observations. For the most part, they are consistent, but we should find out why the 1205 inconsistencies exist. flights_dt %&gt;% mutate ( calculated_dep_time = sched_dep_time + dep_delay*60) %&gt;% select (calculated_dep_time, sched_dep_time, dep_delay, dep_time) %&gt;% filter ( calculated_dep_time != dep_time) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1205 3. Compare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.) Theoretically, the duration should match the difference between the arrival time and hte departure time, after accounting for time zone differences between airport locations. If the time zone difference is not accounted for, the air_time will not match the simple difference. This seems to be the case, because 327,150 of the 328,063 observations in the dataset have a reported air_time that does not match the difference between the arrival and departure times. flights_dt %&gt;% mutate ( calculated_air_time = arr_time - dep_time) %&gt;% select (calculated_air_time, air_time, arr_time, dep_time) %&gt;% filter (calculated_air_time != air_time) ## # A tibble: 327,150 x 4 ## calculated_air_time air_time arr_time dep_time ## &lt;time&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 193 mins 227 2013-01-01 08:30:00 2013-01-01 05:17:00 ## 2 197 mins 227 2013-01-01 08:50:00 2013-01-01 05:33:00 ## 3 221 mins 160 2013-01-01 09:23:00 2013-01-01 05:42:00 ## 4 260 mins 183 2013-01-01 10:04:00 2013-01-01 05:44:00 ## 5 138 mins 116 2013-01-01 08:12:00 2013-01-01 05:54:00 ## 6 106 mins 150 2013-01-01 07:40:00 2013-01-01 05:54:00 ## 7 198 mins 158 2013-01-01 09:13:00 2013-01-01 05:55:00 ## 8 72 mins 53 2013-01-01 07:09:00 2013-01-01 05:57:00 ## 9 161 mins 140 2013-01-01 08:38:00 2013-01-01 05:57:00 ## 10 115 mins 138 2013-01-01 07:53:00 2013-01-01 05:58:00 ## # … with 327,140 more rows 4. How does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why? Use the hour() function to group observations based on hour, then group by this parameter. Use this to calculate the average dep_delay per hour over the year. Plot using geom_line(). This can also be achieved using date-time components function update(), although this would change the number of points connected on the graph. I’ve plotted both results using either dep_time or sched_dep_time. You should use sched_dep_time instead of dep_time since this will tell you which scheduled flights might have a higher chance of being delayed. We observe that flights scheduled later on during the day have higher chances of being delayed, with a peak around hour 20 (8pm). Organizing by dep_time will let you know what time of the day most of the flights are delayed, which will intuitively occur later than the scheduled time as the flights start backing up. We observe that this is indeed the case, in which the peak of the late flights for dep_time occurs after the peak for the sched_dep_time plot, in which the flights are now delayed past midnight. flights_dt %&gt;% mutate ( dep_hour = hour(sched_dep_time) )%&gt;% group_by(dep_hour) %&gt;% summarize(avg_delay_hour = mean(dep_delay, na.rm = T)) %&gt;% ggplot(aes(dep_hour, avg_delay_hour)) + geom_line() flights_dt %&gt;% mutate ( dep_hour = hour(dep_time) )%&gt;% group_by(dep_hour) %&gt;% summarize(avg_delay_hour = mean(dep_delay, na.rm = T)) %&gt;% ggplot(aes(dep_hour, avg_delay_hour)) + geom_line() 5. On what day of the week should you leave if you want to minimise the chance of a delay? To find days of the week that have the lowest average delay, first assign a day to each observation using wday(). Then group by the day of the week, and use summarize() to find the average delay time on for each day of the week. We see that Saturday has the lowest average delay at 7.61, and on average the flights even arrive earlier than expected! flights_dt %&gt;% mutate(wday = wday(sched_dep_time, label = TRUE)) %&gt;% group_by(wday) %&gt;% summarize ( avg_dep_delay_week = mean(dep_delay, na.rm = TRUE), avg_arr_delay_week = mean(arr_delay, na.rm = TRUE)) ## # A tibble: 7 x 3 ## wday avg_dep_delay_week avg_arr_delay_week ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sun 11.5 4.82 ## 2 Mon 14.7 9.65 ## 3 Tue 10.6 5.39 ## 4 Wed 11.7 7.05 ## 5 Thu 16.1 11.7 ## 6 Fri 14.7 9.07 ## 7 Sat 7.62 -1.45 6. What makes the distribution of diamonds\\(carat and flights\\)sched_dep_time similar? Let’s first examine the distribution of each of these datasets, using histograms. Using just the default value for binwidth, there is no apparent similarity between the distribution of values between the two datasets upon initial observation. The carat values are skewed to the right, but the sched_dep_time is not. I suppose one could argue that there are more flights with an earlier sched_dep_time, and likewise there are more diamonds with a low carat value. However, if we bin the values using smaller binwidth, we observe “spikes” of values in both datasets. This “spike” phenomenon occurs at carat 1, 1.5, 2, etc. and around flight times near the hour. This is likely an example of human “bias” for flights leaving at “nice” departure times, as mentioned by Hadley. ggplot (diamonds, aes(x = carat)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot (flights, aes(x = sched_dep_time)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot (diamonds, aes(x = carat)) + geom_freqpoly(binwidth = 0.1) ggplot (flights, aes(x = sched_dep_time)) + geom_freqpoly(binwidth = 10) 7. Confirm my hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed. To test this hypothesis, we need to see whether the total number of flights that departed early was increased during minutes 20-30 and 50-60. If this is true, then his hypothesis is supported. If this is not true, then maybe other factors are contributing more significantly to the lower flight delays during these time slots. To figure this out, we first use mutate() and ifelse() to assign whether or not a flight left early using TRUE or FALSE. Then, grouping by minute, we can count the number of flights that left early using sum(), and plot this value using ggplot(). We observe that there are indeed more flights leaving early during the 20-30 and 50-60 time slots (the graph looks like and inverted plot of the avg_delay by minute graph). flights_dt %&gt;% mutate(minute = minute(dep_time), early = ifelse(dep_delay&gt;=0, FALSE, TRUE)) %&gt;% group_by(minute) %&gt;% summarise( avg_delay = mean(arr_delay, na.rm = TRUE), num_flights_early = sum(early), n = n()) %&gt;% ggplot(aes(minute, num_flights_early)) + geom_line() 16.4.5 Exercises 1. Why is there months() but no dmonths()? Durations must be standardized lengths of time. There is no dmonths() since months do not have a standard number of days. For example, February has a shorter number of days than January. 2. Explain days(overnight * 1) to someone who has just started learning R. How does it work? The days() function converts the input into a datetime, for example, days(5) returns “5d 0H 0M 0S”. In this case, the input is overnight * 1. The variable overnight corresponds to the output of arr_time &lt; dep_time, which is evaluated as a boolean (TRUE or FALSE). The multiplication with 1 will cause TRUE or FALSE to be converted to 1 or 0. Thus, days(overnight *1) will give you either 0 or 1 days in datetime form, which can then be added to arr_time. # proof of concepts days(5) ## [1] &quot;5d 0H 0M 0S&quot; TRUE*1 ## [1] 1 FALSE*1 ## [1] 0 days(1) + days (TRUE *1) ## [1] &quot;2d 0H 0M 0S&quot; # Example from text using days (overnight * 1) flights_dt &lt;- flights_dt %&gt;% mutate( overnight = arr_time &lt; dep_time, arr_time = arr_time + days(overnight * 1), sched_arr_time = sched_arr_time + days(overnight * 1) ) 3. Create a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year. To do this, we can use ymd() to create a date for January 1, 2015. Then we can add this to a vector of months in order to generate a vector of 1st days for every month in that year. To do the first day of everymonth for the current year, we should write code that will work no matter what year you run it. To do this, we can use the today() function to get todays date, then extract the year from this date. This can be done two ways. You can either use floor_date(), which can round the date to the nearest year if specified. Or, since the date object can be manipulated like a string, you can use substr() to extract the first 4 characters (the year), then use str_c() to add “-01-01” which will create “january 1st”&quot; for the current year. Then we can add the vector of months as we did previously to get the vector of dates giving the first day of every month in the current year. first_days_2015 &lt;- ymd(&#39;2015-01-01&#39;) + months(seq(0,11,1)) # months(0:11) also works first_days_2015 ## [1] &quot;2015-01-01&quot; &quot;2015-02-01&quot; &quot;2015-03-01&quot; &quot;2015-04-01&quot; &quot;2015-05-01&quot; ## [6] &quot;2015-06-01&quot; &quot;2015-07-01&quot; &quot;2015-08-01&quot; &quot;2015-09-01&quot; &quot;2015-10-01&quot; ## [11] &quot;2015-11-01&quot; &quot;2015-12-01&quot; # do it using strings, extracting the year from today() jan01_current &lt;- str_c(substr(today(),1,4), &quot;-01-01&quot;) ymd(jan01_current) + months(0:11) ## [1] &quot;2020-01-01&quot; &quot;2020-02-01&quot; &quot;2020-03-01&quot; &quot;2020-04-01&quot; &quot;2020-05-01&quot; ## [6] &quot;2020-06-01&quot; &quot;2020-07-01&quot; &quot;2020-08-01&quot; &quot;2020-09-01&quot; &quot;2020-10-01&quot; ## [11] &quot;2020-11-01&quot; &quot;2020-12-01&quot; # do it using floor_date() to round today() to the current year floor_date(today(), &quot;year&quot;) + months(0:11) ## [1] &quot;2020-01-01&quot; &quot;2020-02-01&quot; &quot;2020-03-01&quot; &quot;2020-04-01&quot; &quot;2020-05-01&quot; ## [6] &quot;2020-06-01&quot; &quot;2020-07-01&quot; &quot;2020-08-01&quot; &quot;2020-09-01&quot; &quot;2020-10-01&quot; ## [11] &quot;2020-11-01&quot; &quot;2020-12-01&quot; 4. Write a function that given your birthday (as a date), returns how old you are in years. We can use intervals as described in the chapter to make this work. Another way is to extract the year by taking the first four characters of the date using substr, converting to integer, then subtracting the result from today() from the result from your birthday. However this wouldnt work if you were born prior to year 1000, or if today() was past year 9999. Given that this code probably wont be used in either of these cases, I’d say we’re OK here. date &lt;- ymd(&quot;1992-01-01&quot;) # method 1 get_age_interval &lt;- function(birthday) { return( (birthday %--% today()) %/% years(1) ) } get_age_interval(date) ## [1] 28 # method 2 get_age_string &lt;- function(birthday) { return(as.integer(substr(today(),1,4)) - as.integer(substr(birthday,1,4))) } get_age_string(date) ## [1] 28 5. Why can’t (today() %–% (today() + years(1)) / months(1) work? Aside from the missing parenthesis after “years(1))”, this code doesn’t throw any errors. It seems to work with either the / or %/% operators. Might be a version issue. We are adding one year interval to todays date, then calculating how many months there are between todays date and 1 year from today, which should be 12. The code returns the value 12. (today() %--% (today() + years(1))) / months(1) ## [1] 12 "],
["chapter-19-functions.html", "Chapter 19 - Functions 19.2.1 Exercises 19.3.1 Exercises 19.4.4 Exercises 19.5.5 Exercises", " Chapter 19 - Functions 19.2.1 Exercises 1. Why is TRUE not a parameter to rescale01()? What would happen if x contained a single missing value, and na.rm was FALSE? TRUE is not a parameter to rescale01() because it is an option for one of the arguments in the range() function. It can be specified within the function itself rather than having to be passed in as a function parameter. If na.rm was FALSE, NA values would not be “removed” from the analysis, and the function would produce a vector of NA values. Below, I show an example what would happen if na.rm was FALSE and a vector with an NA value was used. rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } rescale01_FALSE &lt;- function(x) { rng &lt;- range(x, na.rm = FALSE) (x - rng[1]) / (rng[2] - rng[1]) } test &lt;- c(1,2,3,NA,4,5) rescale01(test) ## [1] 0.00 0.25 0.50 NA 0.75 1.00 rescale01_FALSE(test) ## [1] NA NA NA NA NA NA 2. In the second variant of rescale01(), infinite values are left unchanged. Rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1. To map Inf to 1 and -Inf to 0, we can search for the indicies which have these values and assign 0 or 1 accordingly, using the which() function in base R. We then return the modified vector using return(). x &lt;- c(1:10,Inf, c(1:3), Inf, c(1:5), -Inf) rescale01_mapInf &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE, finite = TRUE) x &lt;- (x - rng[1]) / (rng[2] - rng[1]) x[which(x==Inf)] &lt;- 1 x[which(x==-Inf)] &lt;- 0 return (x) } rescale01_mapInf(x) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 ## [8] 0.7777778 0.8888889 1.0000000 1.0000000 0.0000000 0.1111111 0.2222222 ## [15] 1.0000000 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.0000000 3. Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative? mean(is.na(x)) is a snippet that calculates what proportion of the values in a vector are NA values. is.na(x) will return a boolean for each value in x (FALSE if not NA, TRUE if NA). TRUE is 1 and FALSE is 0 when used in mean(). x = c(1:5, NA, 1:2, NA, 1:3) mean(is.na(x)) ## [1] 0.1666667 # rewrite the snippet into a function proportion_na &lt;- function (x) { sum(is.na(x))/length(x) } # see if the function output matches the snippet proportion_na(x) ## [1] 0.1666667 x / sum(x, na.rm = TRUE) is a snippet that divides each of the values in X by the total sum of the non-NA values in x. x / sum(x, na.rm = TRUE) ## [1] 0.04166667 0.08333333 0.12500000 0.16666667 0.20833333 NA ## [7] 0.04166667 0.08333333 NA 0.04166667 0.08333333 0.12500000 divide_by_sum &lt;- function (x) { x / sum(x, na.rm = TRUE) } divide_by_sum(x) ## [1] 0.04166667 0.08333333 0.12500000 0.16666667 0.20833333 NA ## [7] 0.04166667 0.08333333 NA 0.04166667 0.08333333 0.12500000 sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) is a snippet that divides the standard deviation of the values in x by the mean of the values in x. sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) ## [1] 0.5624571 sd_div_mean &lt;- function (x) { sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) } sd_div_mean(x) ## [1] 0.5624571 Each of the functions only requires the vector x as an input argument. 4. Follow http://nicercode.github.io/intro/writing-functions.html to write your own functions to compute the variance and skew of a numeric vector. sample_vector &lt;- c(1:10) variance &lt;- function (x) { n &lt;- length(x) m &lt;- mean(x) (1/(n - 1)) * sum((x - m)^2) } skew &lt;- function (x) { n &lt;- length(x) v &lt;- var(x) m &lt;- mean(x) third.moment &lt;- (1/(n - 2)) * sum((x - m)^3) third.moment/(var(x)^(3/2)) } # might have to cross-reference this function with other sources to make sure it&#39;s correct. variance(sample_vector) ## [1] 9.166667 var(sample_vector) ## [1] 9.166667 skew(sample_vector) ## [1] 0 5. Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors. I interpreted this question as finding the index numbers of positions in both vectors that both contain NA values. For example, if you had one vector c(1, 2, NA, 5, 6) and another vector c(NA, 6, NA, 4, 5), this function should return “3”. To do this, we first evaluate which values in both vectors are NA values using is.na(). Then, we can use which() to find out the index of all TRUE (NA) values in these vectors. Then, we use intersect() to determine which indecies are common between the two vectors. This function should work even if the vectors are different lengths. vector1 = c(1, 2, NA, 5, 6) vector2 = c(NA, 6, NA, 4, 5) both_na &lt;- function (v1, v2) { na1 &lt;- is.na(v1) na2 &lt;- is.na(v2) intersect(which(na1==TRUE),which(na2==TRUE)) } vector1 ## [1] 1 2 NA 5 6 vector2 ## [1] NA 6 NA 4 5 both_na(vector1,vector2) ## [1] 3 6. What do the following functions do? Why are they useful even though they are so short? is_directory &lt;- function(x) file.info(x)$isdir is_readable &lt;- function(x) file.access(x, 4) == 0 is_directory() is a function that tells the user whether the object (x) is a directory or not (returns a TRUE or FALSE value). is_readable() is a function that tells the user whether the file is readable or not (also returns TRUE or FALSE). The second argument (4) indicates that it “tests for read permission” based on the documentation. The function file.access() returns 0 for success and -1 for failure. These functions are useful because it they provide information that guides the user with how to proceed with the file. 7. Read the complete lyrics to “Little Bunny Foo Foo”. There’s a lot of duplication in this song. Extend the initial piping example to recreate the complete song, and use functions to reduce the duplication. The lyrics are repeated three times, each time with the number of chances decreased by one. We can lump all the lyrics into one function, and then call the function 3 times using a loop while decreasing the number of chances each iteration of the loop. # I commented out the pseudocode so the R markdown file can compile. # foo_foo %&gt;% # hop(through = forest) %&gt;% # scoop(up = field_mice) %&gt;% # bop(on = head) %&gt;% # down(came = good_fairy) %&gt;% # scoop(up = field_mice) %&gt;% # bop (on = head) %&gt;% # give (chances = three) %&gt;% # turn (into = goonie) # # repeat 3 times, with the # of chances decreasing each time # # play_through (foo_foo, chances) { # hop(through = forest) %&gt;% # scoop(up = field_mice) %&gt;% # bop(on = head) %&gt;% # down(came = good_fairy) %&gt;% # scoop(up = field_mice) %&gt;% # bop (on = head) %&gt;% # give (chances = three) %&gt;% # turn (into = goonie) # } # # chances &lt;- 3 # while (chances &gt; 0) { # play_through (foo_foo, chances) # chances &lt;- chances - 1 # } # 19.3.1 Exercises 1. Read the source code for each of the following three functions, puzzle out what they do, and then brainstorm better names. f1 &lt;- function(string, prefix) { substr(string, 1, nchar(prefix)) == prefix } f1(&quot;hello&quot;, &quot;he&quot;) ## [1] TRUE f1(&quot;hello&quot;, &quot;ell&quot;) ## [1] FALSE This function returns TRUE or FALSE depending on whether the second argument (prefix) matches the corresponding first letters in the first argument (string). A better function name would be “is_prefix”. f2 &lt;- function(x) { if (length(x) &lt;= 1) return(NULL) x[-length(x)] } x = c(1:10) x ## [1] 1 2 3 4 5 6 7 8 9 10 f2(x) ## [1] 1 2 3 4 5 6 7 8 9 This is a function that deletes the last entry of the input vector, x. If the vector is of length 1 or less, the function returns NULL. A better function name would be “delete_last”. f3 &lt;- function(x, y) { rep(y, length.out = length(x)) } x = c(1:10) x ## [1] 1 2 3 4 5 6 7 8 9 10 f3(x, 5) ## [1] 5 5 5 5 5 5 5 5 5 5 This is a function that returns a vector that is the same length as x, but all of its values consist of y. A better name for this function would be “rep_constant_values”. Although we are not necessarily changing the values in the input vector, but rather generating a new vector with the values “replaced”, the user can infer that the output vector consist of a constant value. 2. Take a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments. Below is an example of a task that could be optimized by writing a function instead. library(tidyverse) preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) # without using functions, convert the pregnant and female columns to booleans (TRUE or FALSE values). gather(preg, sex, count, male, female) %&gt;% mutate(pregnant = pregnant == &quot;yes&quot;, female = sex == &quot;female&quot;) %&gt;% select(-sex) ## # A tibble: 4 x 3 ## pregnant count female ## &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 TRUE NA FALSE ## 2 FALSE 20 FALSE ## 3 TRUE 10 TRUE ## 4 FALSE 12 TRUE Instead of having to place the “==” clause in the mutate function, we can write a function to do so instead. # write function to convert a vector of strings to TRUE or FALSE, depending on whether the values match a specified string: string_to_boolean &lt;- function (x, string) { x == string } # use the function to do the job gather(preg, sex, count, male, female) %&gt;% mutate(pregnant = string_to_boolean(pregnant,&quot;yes&quot;), female = string_to_boolean(sex,&quot;female&quot;)) %&gt;% select(-sex) ## # A tibble: 4 x 3 ## pregnant count female ## &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 TRUE NA FALSE ## 2 FALSE 20 FALSE ## 3 TRUE 10 TRUE ## 4 FALSE 12 TRUE 3. Compare and contrast rnorm() and MASS::mvrnorm(). How could you make them more consistent? First we can compare and contrast the type of output from the two functions by generating a list of 100 random normal values with mean 0 and SD 1 using both functions (plotted below): plot(rnorm(100), main = &quot;rnorm()&quot;) plot(MASS::mvrnorm(100,0,1), main = &quot;MASS::mvrnorm()&quot;) I noticed that rnorm(100) automatically sets the mean to 0 and SD to 1, whereas MASS::mvrnorm(100) does not work unless you manually specify the mean to 0 and SD to 1 in the 2nd and 3rd arguments MASS::mvrnorm(100,0,1). To make them more consistent, I would set the default values for mu and Sigma for MASS::mvrnorm() to 0 and 1, so that both functions will work without having to explicitly state the mu (mean) and Sigma (SD) values. 4. Make a case for why norm_r(), norm_d() etc would be better than rnorm(), dnorm(). Make a case for the opposite. norm_r(), norm_d() etc would be better since you know that these functions have are common with each other in that they deal with the normal distribution. The suffix “_r“, etc lets you know how the functions differ from each other. You can also get a list of all the related functions by typing”norm&quot; in RStudio and seeing the autocomplete options. One could argue that this is not as good as the current rnorm(), dnorm() etc convention because of the added finger work required to place an underscore in the name. Also, norm_r() is a rather cryptic name that might not have intuitive meaning, whereas rnorm immediately suggests “random normal” to one familiar with statistics. 19.4.4 Exercises 1. What’s the difference between if and ifelse()? Carefully read the help and construct three examples that illustrate the key differences. if, if used by itself, will only execute its contents if the conditional statement is TRUE. If the statement is FALSE, nothing will be executed unless there is an else if or else statement afterwards. ifelse() gives you an option to execute code if the conditional statement is FALSE. It can also be used to filter or reassign values in a vector depending on a condition. 2. Write a greeting function that says “good morning”, “good afternoon”, or “good evening”, depending on the time of day. (Hint: use a time argument that defaults to lubridate::now(). That will make it easier to test your function.) library(lubridate) greet &lt;- function ( datetime = lubridate::now() ) { print(datetime) time &lt;- hour(datetime) if (time &lt; 12) { print(&quot;good morning&quot;) } else if (time &gt;= 12 &amp;&amp; time &lt; 18) { print(&quot;good afternoon&quot;) } else print(&quot;good evening&quot;) } greet() ## [1] &quot;2020-01-21 21:47:04 PST&quot; ## [1] &quot;good evening&quot; greet(ymd_hms(&quot;2016-07-08 08:34:56&quot;)) ## [1] &quot;2016-07-08 08:34:56 UTC&quot; ## [1] &quot;good morning&quot; greet(ymd_hms(&quot;2016-07-08 20:34:56&quot;)) ## [1] &quot;2016-07-08 20:34:56 UTC&quot; ## [1] &quot;good evening&quot; greet(ymd_hms(&quot;2016-07-08 15:34:56&quot;)) ## [1] &quot;2016-07-08 15:34:56 UTC&quot; ## [1] &quot;good afternoon&quot; 3. Implement a fizzbuzz function. It takes a single number as input. If the number is divisible by three, it returns “fizz”. If it’s divisible by five it returns “buzz”. If it’s divisible by three and five, it returns “fizzbuzz”. Otherwise, it returns the number. Make sure you first write working code before you create the function. fizzbuzz &lt;- function ( input ) { if (input %% 3 == 0 &amp;&amp; input %% 5 == 0) { print(&quot;fizzbuzz&quot;) } else if ( input %% 5 == 0 ) { print (&quot;buzz&quot;) } else if ( input %% 3 == 0 ) { print (&quot;fizz&quot;) } else print (input) } fizzbuzz(15) ## [1] &quot;fizzbuzz&quot; fizzbuzz(10) ## [1] &quot;buzz&quot; fizzbuzz(9) ## [1] &quot;fizz&quot; fizzbuzz(4) ## [1] 4 4. How could you use cut() to simplify this set of nested if-else statements? # if (temp &lt;= 0) { # &quot;freezing&quot; # } else if (temp &lt;= 10) { # &quot;cold&quot; # } else if (temp &lt;= 20) { # &quot;cool&quot; # } else if (temp &lt;= 30) { # &quot;warm&quot; # } else { # &quot;hot&quot; # } temp_range &lt;- c(-5:40) # without labels cut(temp_range, breaks = c(-100, 0, 10, 20, 30, 100), right = TRUE) ## [1] (-100,0] (-100,0] (-100,0] (-100,0] (-100,0] (-100,0] (0,10] ## [8] (0,10] (0,10] (0,10] (0,10] (0,10] (0,10] (0,10] ## [15] (0,10] (0,10] (10,20] (10,20] (10,20] (10,20] (10,20] ## [22] (10,20] (10,20] (10,20] (10,20] (10,20] (20,30] (20,30] ## [29] (20,30] (20,30] (20,30] (20,30] (20,30] (20,30] (20,30] ## [36] (20,30] (30,100] (30,100] (30,100] (30,100] (30,100] (30,100] ## [43] (30,100] (30,100] (30,100] (30,100] ## Levels: (-100,0] (0,10] (10,20] (20,30] (30,100] table(cut(temp_range, breaks = c(-100, 0, 10, 20, 30, 100), right = TRUE)) ## ## (-100,0] (0,10] (10,20] (20,30] (30,100] ## 6 10 10 10 10 # with labels cut(temp_range, breaks = c(-100, 0, 10, 20, 30, 100), labels = c(&quot;freezing&quot;, &quot;cold&quot;, &quot;cool&quot;, &quot;warm&quot;, &quot;hot&quot;), right = TRUE) ## [1] freezing freezing freezing freezing freezing freezing cold ## [8] cold cold cold cold cold cold cold ## [15] cold cold cool cool cool cool cool ## [22] cool cool cool cool cool warm warm ## [29] warm warm warm warm warm warm warm ## [36] warm hot hot hot hot hot hot ## [43] hot hot hot hot ## Levels: freezing cold cool warm hot table(cut(temp_range, breaks = c(-100, 0, 10, 20, 30, 100), labels = c(&quot;freezing&quot;, &quot;cold&quot;, &quot;cool&quot;, &quot;warm&quot;, &quot;hot&quot;), right = TRUE)) ## ## freezing cold cool warm hot ## 6 10 10 10 10 How would you change the call to cut() if I’d used &lt; instead of &lt;=? What is the other chief advantage of cut() for this problem? (Hint: what happens if you have many values in temp?) If &lt; was used instead of &lt;=, I would change the argument from right = TRUE to right = FALSE. This will split the interval to be closed on the left and open on the right. The other advantage of cut() for this problem is that it is able to sort a vector of values into the various intervals, whereas the nested if else statement that was provided only works on a single value. This makes cut() more efficient when there are many values to be sorted into the intervals. 5. What happens if you use switch() with numeric values? Based on the documentation for switch(), if numerical values are input for the EXPR parameter (first argument), switch() will choose the corresponding element of the list of alternatives (…). I wrote an example below. Switch(1) will choose the first argument after EXPR (the second total argument in the switch() function). If a numerical input for EXPR is larger than the number of alternatives, it seems no output is provided (typeof() returns NULL). test_switch &lt;- function ( x ) { switch(x, &quot;first choice after EXPR&quot;, 2, &quot;third&quot;, &quot;4th choice after EXPR&quot; ) } test_switch(1) ## [1] &quot;first choice after EXPR&quot; test_switch(2) ## [1] 2 test_switch(3) ## [1] &quot;third&quot; test_switch(4) ## [1] &quot;4th choice after EXPR&quot; typeof(test_switch(5)) ## [1] &quot;NULL&quot; 6. What does this switch() call do? What happens if x is “e”? Below I turn the provided switch() call into a function and test each of the possibilities for x, as well as try what happens when x is “e”. Since there is no output provided for options a or c, the switch() function returns the value immediately afterwards. So test_switch(“a”) returns the same value as test_switch(“b”), and test_switch(“c”) returns the same value as test_switch(“d”). If x is “e”, a NULL value is returned. There seems to be no output but if you use typeof(test_switch(“e”)), we see that the value is NULL. test_switch &lt;- function (x) { switch(x, a = , b = &quot;ab&quot;, c = , d = &quot;cd&quot; ) } test_switch(&quot;a&quot;) ## [1] &quot;ab&quot; test_switch(&quot;b&quot;) ## [1] &quot;ab&quot; test_switch(&quot;c&quot;) ## [1] &quot;cd&quot; test_switch(&quot;d&quot;) ## [1] &quot;cd&quot; test_switch(&quot;e&quot;) typeof(test_switch(&quot;e&quot;)) ## [1] &quot;NULL&quot; 19.5.5 Exercises 1. What does commas(letters, collapse = “-”) do? Why? commas(letters, collapse = “-”) results in an error. I think the intention of the code was to combine all the letters separated by hyphens (for example, a-b-c-d-e…). In order to do this, we would have to modify the commas function directly (shown below, a new function called hyphens()). The code that was provided seems to attempt to incorrectly override the collapse argument by specifying it after letters. Since commas() has ... as its only argument, this means that collapse = “-” is incorporated into the ..., resulting in the error. commas &lt;- function(...) stringr::str_c(..., collapse = &quot;, &quot;) commas(letters[1:10]) ## [1] &quot;a, b, c, d, e, f, g, h, i, j&quot; # this produces an error: # commas(letters[1:10], collapse = &quot;-&quot;) # to produce the intended output, modify the commas function. hyphens &lt;- function(...) stringr::str_c(..., collapse = &quot;-&quot;) hyphens(letters[1:10]) ## [1] &quot;a-b-c-d-e-f-g-h-i-j&quot; Alternatively, we could modify the function to allow the user to specify what type of insertion to use as an argument after ..., with the default being commas. I name the function insert_between(), below. insert_between &lt;- function(..., insert = &quot;, &quot;) stringr::str_c(..., collapse = insert) insert_between (letters[1:10]) ## [1] &quot;a, b, c, d, e, f, g, h, i, j&quot; insert_between (letters[1:10], insert = &quot;-&quot;) ## [1] &quot;a-b-c-d-e-f-g-h-i-j&quot; 2. It’d be nice if you could supply multiple characters to the pad argument, e.g. rule(“Title”, pad = “-+”). Why doesn’t this currently work? How could you fix it? This doesn’t currently work optimally because the rule becomes twice as long (due to there being 2 characters instead of 1 for the pad argument). To fix this, we could divide the width according to the length of the pad argument. To figure out how many characters are in pad, use nchar(). Then, divide the width parameter by this number. This results in a rule that is the appropriate length. I show what the existing function does below, along with a modified function that performs the correct result. # The original rule function, showing what happens if multiple characters were supplied: rule &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) width &lt;- getOption(&quot;width&quot;) - nchar(title) - 5 cat(title, &quot; &quot;, stringr::str_dup(pad, width), &quot;\\n&quot;, sep = &quot;&quot;) } rule(&quot;Important output&quot;) ## Important output ------------------------------------------------------ rule(&quot;Title&quot;, pad = &quot;-+&quot;) ## Title -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ # An improved version that scales the padding based on the character length: scaled_rule &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) pad_length &lt;- nchar(pad) print(str_c(&quot;Number of characters in pad: &quot;, pad_length)) width &lt;- (getOption(&quot;width&quot;) - nchar(title) - 5)/pad_length print(str_c(&quot;Number of times pad was duplicated: &quot;, width)) cat(title, &quot; &quot;, stringr::str_dup(pad, width), &quot;\\n&quot;, sep = &quot;&quot;) } scaled_rule(&quot;Important output&quot;) ## [1] &quot;Number of characters in pad: 1&quot; ## [1] &quot;Number of times pad was duplicated: 54&quot; ## Important output ------------------------------------------------------ scaled_rule(&quot;Title&quot;, pad = &quot;-+&quot;) ## [1] &quot;Number of characters in pad: 2&quot; ## [1] &quot;Number of times pad was duplicated: 32.5&quot; ## Title -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 3. What does the trim argument to mean() do? When might you use it? The trim argument in mean() will remove a proportion of values from both sides of the vector before calculating the mean of the remaining values. The proportion is between 0 to 0.5, with default value being 0 (no trim applied). Examples below of how trim may affect mean calculation. You might use this when you are calculating something in which the edge cases have high variability or contain unreliable data. x = c(1,1,1:10,1,1) mean(x) ## [1] 4.214286 mean(x, trim = 0.2) ## [1] 3.8 4. The default value for the method argument to cor() is c(“pearson”, “kendall”, “spearman”). What does that mean? What value is used by default? Based on the documentation for cor(), these specify the type of correlation coefficient that is calculated by the function. The default value used is “pearson”. Below is an example of how changing these parameters may affect the output, since each method uses a different equation. x = c(1:20) y = c(1,1,1,1:15, 5, 6) cor(x,y) ## [1] 0.796562 cor(x,y, method = &quot;kendall&quot;) ## [1] 0.7743726 cor(x,y, method = &quot;spearman&quot;) ## [1] 0.8157183 "],
["chapter-20-vectors.html", "Chapter 20 - Vectors 20.3.5 Exercises 20.4.6 Exercises 20.5.4 Exercises 20.7.4 Exercises", " Chapter 20 - Vectors 20.3.5 Exercises 1. Describe the difference between is.finite(x) and !is.infinite(x). is.finite(x) should only evaluate to TRUE if the value is not NA, NaN, or +/-Inf. However, !is.infinite(x) will evaluate to TRUE if the value is NA, NaN, or a double/integer. is.finite(0) ## [1] TRUE is.finite(NA) ## [1] FALSE !is.infinite(NA) ## [1] TRUE 2. Read the source code for dplyr::near() (Hint: to see the source code, drop the ()). How does it work? The source code is below: # function (x, y, tol = .Machine$double.eps^0.5) # { # abs(x - y) &lt; tol # } # &lt;bytecode: 0x10a60a198&gt; # &lt;environment: namespace:dplyr&gt; Based on the code, the function subtracts the input (x) with the number that you want to compare it to (y), takes the absolute value of that operation, and then checks to see if it is below a certain threshold (tol). If so, it returns TRUE. If not, it returns FALSE. You can toggle the size of the threshold to your liking by changing the tol parameter. 3. A logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use google to do some research. The maximum possible values for integer and double values is related to the bit-representation of each type. For integers, there are 2^32 possible values since R uses 32-bit representation for integers. For doubles, R uses 64-bit representation so there would be 2^64 possible values. 4. Brainstorm at least four functions that allow you to convert a double to an integer. How do they differ? Be precise. A double could be converted to an integer by rounding either up or down (floor() or ceiling()). In the case of a tie value (doubles ending in .5) we could either round up or down, or towards the either the even or odd digit. 5. What functions from the readr package allow you to turn a string into logical, integer, and double vector? Respectively, the functions parse_logical(), parse_integer(), parse_double() will turn a string into a logical, integer, or double. library(readr) parse_logical(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) ## [1] TRUE FALSE parse_integer(c(&quot;100&quot;, &quot;200&quot;)) ## [1] 100 200 parse_double(c(&quot;100.3&quot;, &quot;200&quot;)) ## [1] 100.3 200.0 20.4.6 Exercises 1. What does mean(is.na(x)) tell you about a vector x? What about sum(!is.finite(x))? mean(is.na(x)) tells you what proportion of the values in the vector x are NA. sum(!is.finite(x)) tells you how many values in the vector are NA (total count), because NA is not a finite value. x &lt;- c(NA, 1, 2, NA, 5:10, NA, NA, NA) is.na(x) ## [1] TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [12] TRUE TRUE mean(is.na(x)) ## [1] 0.3846154 is.finite(x) ## [1] FALSE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [12] FALSE FALSE !is.finite(x) ## [1] TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [12] TRUE TRUE sum(is.finite(x)) ## [1] 8 sum(!is.finite(x)) ## [1] 5 2. Carefully read the documentation of is.vector(). What does it actually test for? Why does is.atomic() not agree with the definition of atomic vectors above? is.vector() tests for whether a vector is of the specified mode and has no attributes other than names. For example, the named vector ‘x’ below will return TRUE. See examples of use below. The definition above states that atomic vectors are homogenous, in which each value of the vector should be of the same type. One way that is.atomic() deviates from this definition of atomic vectors is that it still returns TRUE for named vectors, which can have character-based names for numerical values. x &lt;- c(a = 1, b = 2) is.vector(x) ## [1] TRUE is.atomic(x) ## [1] TRUE x &lt;- c(a = 1, b = &quot;hello&quot;) is.vector(x, mode = &quot;integer&quot;) ## [1] FALSE is.vector(x, mode = &quot;character&quot;) ## [1] TRUE is.atomic(x) ## [1] TRUE 3. Compare and contrast setNames() with purrr::set_names(). purrr::set_names() is a more flexible version of stats::setNames() that has more features. In the example below, setNames fails to work when the “names” are not explicitly provided as one vector of the same length as the vector to be named. purrr::set_names() still works when the names are provided separately. # using stats::setNames() setNames(1:4, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) ## a b c d ## 1 2 3 4 #setNames(1:4, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) # Error in setNames(1:4, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) :unused arguments (&quot;b&quot;, &quot;c&quot;, &quot;d&quot;) # using purrr::setNames() library(purrr) set_names(1:4, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) ## a b c d ## 1 2 3 4 set_names(1:4, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) ## a b c d ## 1 2 3 4 4. Create functions that take a vector as input and returns: The last value. Should you use [ or [[? # we should use [, instead of [[]] example &lt;- letters[1:10] return_last &lt;- function (x) { return (x[length(x)]) } example ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; return_last(example) ## [1] &quot;j&quot; The elements at even numbered positions. example &lt;- letters[1:10] return_even &lt;- function (x) { even_indicies &lt;- c(1:length(x)) %% 2 == 0 return (x[even_indicies]) } example ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; return_even(example) ## [1] &quot;b&quot; &quot;d&quot; &quot;f&quot; &quot;h&quot; &quot;j&quot; Every element except the last value. example &lt;- letters[1:10] remove_last &lt;- function (x) { return(x[-length(x)]) } example ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; remove_last(example) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; Only even numbers (and no missing values). example &lt;- c(1:5, NA, 6:12, NA, 13:20) return_even &lt;- function (x) { return (x[x %% 2 == 0 &amp; !is.na(x)]) } example ## [1] 1 2 3 4 5 NA 6 7 8 9 10 11 12 NA 13 14 15 16 17 18 19 20 return_even(example) ## [1] 2 4 6 8 10 12 14 16 18 20 5. Why is x[-which(x &gt; 0)] not the same as x[x &lt;= 0]? which(x &gt; 0) returns a vector of indicies in x which contain values that are greater than zero. x[-which(x &gt; 0)] selects the values in x which do not correspond to those indicies. x &lt;= 0 returns a vector of boolean values corresponding to the values in x which are less than or equal to zero, and x[x &lt;= 0] selects the values in x which satisfy the boolean condition. x &lt;- c(-5:5) which(x &gt; 0) ## [1] 7 8 9 10 11 x &lt;= 0 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE x[-which(x &gt; 0)] ## [1] -5 -4 -3 -2 -1 0 x[x &lt;= 0] ## [1] -5 -4 -3 -2 -1 0 6. What happens when you subset with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist? Subsetting with a positive integer that’s bigger than the length of the vector returns NA. When you subset with a name that doesn’t exist, it returns an error saying doesnt exist. x &lt;- c(-5:5) # length(x) is 11 x[12] ## [1] NA # add names to x, then try a name that doesn&#39;t exist names(x) &lt;- letters[1:length(x)] x ## a b c d e f g h i j k ## -5 -4 -3 -2 -1 0 1 2 3 4 5 # x[l] # Error: object &#39;l&#39; not found 20.5.4 Exercises 1. Draw the following lists as nested sets: list(a, b, list(c, d), list(e, f)) The structure is as follows: [ a, b, [c,d], [e,f] ] list(list(list(list(list(list(a)))))) The structure is as follows (the value “a” is nested within 6 lists): [ [ [ [ [ [ a ] ] ] ] ] ] 2. What happens if you subset a tibble as if you’re subsetting a list? What are the key differences between a list and a tibble? Subsetting a tibble using the names of the columns will pull out the respective columns of the tibble as a new tibble, if multiple columns are selected. If only one column of the tibble is selected, the column is pulled out as the data type of the values stored in it. The same applies to a list, which returns a new list if multiple named constituents are selected, or a vector if one named constituent is selected. A key difference is that a tibble has a fixed dimension and each column must be of the same length, whereas a list can contain vectors of differing lengths. A tibble can also be manipulated using dplyr commands and functions that apply to data frames, which provides more functionality/flexibility for data analysis. iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa ## 51 7.0 3.2 4.7 1.4 versicolor ## 52 6.4 3.2 4.5 1.5 versicolor ## 53 6.9 3.1 4.9 1.5 versicolor ## 54 5.5 2.3 4.0 1.3 versicolor ## 55 6.5 2.8 4.6 1.5 versicolor ## 56 5.7 2.8 4.5 1.3 versicolor ## 57 6.3 3.3 4.7 1.6 versicolor ## 58 4.9 2.4 3.3 1.0 versicolor ## 59 6.6 2.9 4.6 1.3 versicolor ## 60 5.2 2.7 3.9 1.4 versicolor ## 61 5.0 2.0 3.5 1.0 versicolor ## 62 5.9 3.0 4.2 1.5 versicolor ## 63 6.0 2.2 4.0 1.0 versicolor ## 64 6.1 2.9 4.7 1.4 versicolor ## 65 5.6 2.9 3.6 1.3 versicolor ## 66 6.7 3.1 4.4 1.4 versicolor ## 67 5.6 3.0 4.5 1.5 versicolor ## 68 5.8 2.7 4.1 1.0 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 70 5.6 2.5 3.9 1.1 versicolor ## 71 5.9 3.2 4.8 1.8 versicolor ## 72 6.1 2.8 4.0 1.3 versicolor ## 73 6.3 2.5 4.9 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 75 6.4 2.9 4.3 1.3 versicolor ## 76 6.6 3.0 4.4 1.4 versicolor ## 77 6.8 2.8 4.8 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 79 6.0 2.9 4.5 1.5 versicolor ## 80 5.7 2.6 3.5 1.0 versicolor ## 81 5.5 2.4 3.8 1.1 versicolor ## 82 5.5 2.4 3.7 1.0 versicolor ## 83 5.8 2.7 3.9 1.2 versicolor ## 84 6.0 2.7 5.1 1.6 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 86 6.0 3.4 4.5 1.6 versicolor ## 87 6.7 3.1 4.7 1.5 versicolor ## 88 6.3 2.3 4.4 1.3 versicolor ## 89 5.6 3.0 4.1 1.3 versicolor ## 90 5.5 2.5 4.0 1.3 versicolor ## 91 5.5 2.6 4.4 1.2 versicolor ## 92 6.1 3.0 4.6 1.4 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 94 5.0 2.3 3.3 1.0 versicolor ## 95 5.6 2.7 4.2 1.3 versicolor ## 96 5.7 3.0 4.2 1.2 versicolor ## 97 5.7 2.9 4.2 1.3 versicolor ## 98 6.2 2.9 4.3 1.3 versicolor ## 99 5.1 2.5 3.0 1.1 versicolor ## 100 5.7 2.8 4.1 1.3 versicolor ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 107 4.9 2.5 4.5 1.7 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 113 6.8 3.0 5.5 2.1 virginica ## 114 5.7 2.5 5.0 2.0 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 116 6.4 3.2 5.3 2.3 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 120 6.0 2.2 5.0 1.5 virginica ## 121 6.9 3.2 5.7 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 125 6.7 3.3 5.7 2.1 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 127 6.2 2.8 4.8 1.8 virginica ## 128 6.1 3.0 4.9 1.8 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 136 7.7 3.0 6.1 2.3 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 140 6.9 3.1 5.4 2.1 virginica ## 141 6.7 3.1 5.6 2.4 virginica ## 142 6.9 3.1 5.1 2.3 virginica ## 143 5.8 2.7 5.1 1.9 virginica ## 144 6.8 3.2 5.9 2.3 virginica ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica typeof(iris) ## [1] &quot;list&quot; typeof(iris[,c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;)]) ## [1] &quot;list&quot; typeof(iris$Sepal.Length) ## [1] &quot;double&quot; typeof(iris[,c(&quot;Sepal.Length&quot;)]) ## [1] &quot;double&quot; mylist &lt;- list(nums = c(1:5), myletters = letters[1:15]) mylist ## $nums ## [1] 1 2 3 4 5 ## ## $myletters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; typeof(mylist) ## [1] &quot;list&quot; typeof(mylist$nums) ## [1] &quot;integer&quot; 20.7.4 Exercises 1. What does hms::hms(3600) return? How does it print? What primitive type is the augmented vector built on top of? What attributes does it use? It returns 01:00:00. It is built on top of double. It uses the attributes “class”, which has values “hms” and “difftime”, and “units”, which has the value “secs”. hms::hms(3600) ## 01:00:00 typeof(hms::hms(3600)) ## [1] &quot;double&quot; attributes(hms::hms(3600)) ## $class ## [1] &quot;hms&quot; &quot;difftime&quot; ## ## $units ## [1] &quot;secs&quot; 2. Try and make a tibble that has columns with different lengths. What happens? Trying to make a tibble with differing column lengths results in an error. # tibble (a = c(1:5), # b = letters[1:3]) # Error: Tibble columns must have consistent lengths, only values of length one are recycled: * Length 3: Column `b` * Length 5: Column `a` However, there is an exception to this, in which the values of length one are repeated until the column length matches the other columns. An example is below: tibble (a = c(1:5), b = letters[1]) ## # A tibble: 5 x 2 ## a b ## &lt;int&gt; &lt;chr&gt; ## 1 1 a ## 2 2 a ## 3 3 a ## 4 4 a ## 5 5 a 3. Based on the definition above, is it ok to have a list as a column of a tibble? Yes, it is OK to have a list as a column of a tibble, as long as the the length of the list matches the length of the other columns in the tibble. An example of a tibble with a list for one of its columns and how to select a value from that column is below: mytib &lt;- tibble (a = c(1:3), b = letters[1:3], mylist = list(x = c(1:5), y = c(10:20), z = c(2:3))) mytib$mylist[[1]] ## [1] 1 2 3 4 5 "],
["chapter-21-iteration.html", "Chapter 21 - Iteration 21.2.1 Exercises 21.3.5 Exercises 21.4.1 Exercises 21.5.3 Exercises 21.9.3 Exercises", " Chapter 21 - Iteration 21.2.1 Exercises 1. Write for loops to: Compute the mean of every column in mtcars. # loop through each column of mtcars and compute mean for (column in colnames(mtcars)) { print (c(column,mean(mtcars[,column]))) } ## [1] &quot;mpg&quot; &quot;20.090625&quot; ## [1] &quot;cyl&quot; &quot;6.1875&quot; ## [1] &quot;disp&quot; &quot;230.721875&quot; ## [1] &quot;hp&quot; &quot;146.6875&quot; ## [1] &quot;drat&quot; &quot;3.5965625&quot; ## [1] &quot;wt&quot; &quot;3.21725&quot; ## [1] &quot;qsec&quot; &quot;17.84875&quot; ## [1] &quot;vs&quot; &quot;0.4375&quot; ## [1] &quot;am&quot; &quot;0.40625&quot; ## [1] &quot;gear&quot; &quot;3.6875&quot; ## [1] &quot;carb&quot; &quot;2.8125&quot; Determine the type of each column in nycflights13::flights. for (column in colnames(nycflights13::flights)) { print ( c(column,class(nycflights13::flights[[column]])) ) } ## [1] &quot;year&quot; &quot;integer&quot; ## [1] &quot;month&quot; &quot;integer&quot; ## [1] &quot;day&quot; &quot;integer&quot; ## [1] &quot;dep_time&quot; &quot;integer&quot; ## [1] &quot;sched_dep_time&quot; &quot;integer&quot; ## [1] &quot;dep_delay&quot; &quot;numeric&quot; ## [1] &quot;arr_time&quot; &quot;integer&quot; ## [1] &quot;sched_arr_time&quot; &quot;integer&quot; ## [1] &quot;arr_delay&quot; &quot;numeric&quot; ## [1] &quot;carrier&quot; &quot;character&quot; ## [1] &quot;flight&quot; &quot;integer&quot; ## [1] &quot;tailnum&quot; &quot;character&quot; ## [1] &quot;origin&quot; &quot;character&quot; ## [1] &quot;dest&quot; &quot;character&quot; ## [1] &quot;air_time&quot; &quot;numeric&quot; ## [1] &quot;distance&quot; &quot;numeric&quot; ## [1] &quot;hour&quot; &quot;numeric&quot; ## [1] &quot;minute&quot; &quot;numeric&quot; ## [1] &quot;time_hour&quot; &quot;POSIXct&quot; &quot;POSIXt&quot; Compute the number of unique values in each column of iris. for (column in colnames(iris)) { print (c(column, length(unique(iris[,column])))) } ## [1] &quot;Sepal.Length&quot; &quot;35&quot; ## [1] &quot;Sepal.Width&quot; &quot;23&quot; ## [1] &quot;Petal.Length&quot; &quot;43&quot; ## [1] &quot;Petal.Width&quot; &quot;22&quot; ## [1] &quot;Species&quot; &quot;3&quot; Generate 10 random normals for each of μ=−10,0,10, and 100. means &lt;- c(-10, 0, 10, 100) for (i in means) { print(rnorm (10, i)) } ## [1] -9.542133 -10.383358 -10.617098 -9.313501 -7.463658 -9.586247 ## [7] -9.421090 -9.638472 -9.422668 -10.978171 ## [1] -0.7398129 1.0338034 -0.3695387 0.2969773 -0.2615178 0.4294088 ## [7] -0.5317923 -0.6347465 1.1473329 -0.8340858 ## [1] 11.511716 10.842998 10.932023 10.068180 10.532980 10.034497 11.432468 ## [8] 8.899156 9.640771 10.233389 ## [1] 96.89362 100.77170 100.80290 98.66342 99.94461 100.67008 99.43201 ## [8] 98.76813 101.47166 99.92212 2. Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors: out &lt;- &quot;&quot; for (x in letters) { out &lt;- stringr::str_c(out, x) } out ## [1] &quot;abcdefghijklmnopqrstuvwxyz&quot; # use str_c() with collapse to put all the letters together. str_c(letters, collapse = &quot;&quot;) ## [1] &quot;abcdefghijklmnopqrstuvwxyz&quot; x &lt;- sample(100) sd &lt;- 0 for (i in seq_along(x)) { sd &lt;- sd + (x[i] - mean(x)) ^ 2 } sd &lt;- sqrt(sd / (length(x) - 1)) sd ## [1] 29.01149 # if we only want to replace the for loop, then use: sum( (x-mean(x))^2 ) sqrt( sum( (x-mean(x))^2 ) / (length(x) - 1) ) ## [1] 29.01149 # otherwise just use the sd() function instead of the for loop sd(x) ## [1] 29.01149 x &lt;- runif(100) out &lt;- vector(&quot;numeric&quot;, length(x)) out[1] &lt;- x[1] for (i in 2:length(x)) { out[i] &lt;- out[i - 1] + x[i] } out ## [1] 0.6313162 0.6350445 1.1486326 1.2040190 2.1091825 2.5919667 ## [7] 3.5285872 3.5669447 3.8200857 4.2717039 5.2522156 6.2230116 ## [13] 6.2547521 6.8618498 7.6779305 8.0026124 8.5884170 8.9350433 ## [19] 9.4358700 9.6799386 9.8543304 9.9217561 10.6799236 11.2705327 ## [25] 11.3392938 11.5473192 11.7873142 12.5941426 12.6774564 12.7050889 ## [31] 13.2318285 14.1942259 14.8976633 15.6473616 16.1710684 16.6063363 ## [37] 17.5369760 18.0194281 18.8200097 19.2988724 20.0333152 20.9567123 ## [43] 21.8056936 22.6026529 23.1870958 23.9420246 24.1123016 24.6263081 ## [49] 24.8890961 25.8257526 26.3472017 27.3124038 27.3452063 27.8161866 ## [55] 27.8432487 27.8733143 28.0645089 28.5740391 28.6505543 29.1707966 ## [61] 29.4401222 29.8309588 30.0853641 30.7624207 31.5143308 32.0077487 ## [67] 32.5189378 33.1957695 33.3022747 33.9595799 34.4133002 34.9947384 ## [73] 35.3375869 35.5890653 35.8753518 36.1899076 37.1314045 37.6922446 ## [79] 38.3816194 39.1572676 39.4777520 40.3120925 40.4726386 41.1175236 ## [85] 42.0939471 42.7347619 43.1511472 43.4366199 43.7438335 44.4980710 ## [91] 44.5894311 45.4152320 46.1072375 47.0610684 47.3500387 48.2433428 ## [97] 48.4237648 49.4193446 50.2648524 50.3503730 # the same output can be achieved by calculating the cumulative sum (cumsum()): cumsum(x) ## [1] 0.6313162 0.6350445 1.1486326 1.2040190 2.1091825 2.5919667 ## [7] 3.5285872 3.5669447 3.8200857 4.2717039 5.2522156 6.2230116 ## [13] 6.2547521 6.8618498 7.6779305 8.0026124 8.5884170 8.9350433 ## [19] 9.4358700 9.6799386 9.8543304 9.9217561 10.6799236 11.2705327 ## [25] 11.3392938 11.5473192 11.7873142 12.5941426 12.6774564 12.7050889 ## [31] 13.2318285 14.1942259 14.8976633 15.6473616 16.1710684 16.6063363 ## [37] 17.5369760 18.0194281 18.8200097 19.2988724 20.0333152 20.9567123 ## [43] 21.8056936 22.6026529 23.1870958 23.9420246 24.1123016 24.6263081 ## [49] 24.8890961 25.8257526 26.3472017 27.3124038 27.3452063 27.8161866 ## [55] 27.8432487 27.8733143 28.0645089 28.5740391 28.6505543 29.1707966 ## [61] 29.4401222 29.8309588 30.0853641 30.7624207 31.5143308 32.0077487 ## [67] 32.5189378 33.1957695 33.3022747 33.9595799 34.4133002 34.9947384 ## [73] 35.3375869 35.5890653 35.8753518 36.1899076 37.1314045 37.6922446 ## [79] 38.3816194 39.1572676 39.4777520 40.3120925 40.4726386 41.1175236 ## [85] 42.0939471 42.7347619 43.1511472 43.4366199 43.7438335 44.4980710 ## [91] 44.5894311 45.4152320 46.1072375 47.0610684 47.3500387 48.2433428 ## [97] 48.4237648 49.4193446 50.2648524 50.3503730 3. Combine your function writing and for loop skills: Write a for loop that prints() the lyrics to the children’s song “Alice the camel”. Alice the Camel has one hump. Alice the Camel has one hump. Alice the Camel has one hump. Go Alice go! The package english has a nice function to convert numericals to their corresponding english words, which we can utilize to write out this poem. For example, the number 1 can be converted to “one” and written out. This makes looping through numbers and printing them out easy. The code below prints the poem out if Alice started out with 3 humps. This isn’t exactly the same as the real poem, but you can get the idea. num_humps &lt;- 3 while (num_humps &gt;=0) { eng_num_humps &lt;- english::as.english(num_humps) if (num_humps == 0) cat(&quot;Alice the Camel has no more humps! End of poem.&quot;) else if (num_humps == 1){ cat(str_c(rep(paste(&quot;Alice the Camel has&quot;, eng_num_humps, &quot;hump.\\n&quot;),3), collapse = &quot;&quot;)) cat(&quot;Go Alice go!\\n\\n&quot;) } else{ cat(str_c(rep(paste(&quot;Alice the Camel has&quot;, eng_num_humps, &quot;humps.\\n&quot;),3), collapse = &quot;&quot;)) cat(&quot;Go Alice go!\\n\\n&quot;) } num_humps &lt;- num_humps -1 } ## Alice the Camel has three humps. ## Alice the Camel has three humps. ## Alice the Camel has three humps. ## Go Alice go! ## ## Alice the Camel has two humps. ## Alice the Camel has two humps. ## Alice the Camel has two humps. ## Go Alice go! ## ## Alice the Camel has one hump. ## Alice the Camel has one hump. ## Alice the Camel has one hump. ## Go Alice go! ## ## Alice the Camel has no more humps! End of poem. Convert the nursery rhyme “ten in the bed” to a function. Generalise it to any number of people in any sleeping structure. There were ten in the bed And the little one said, &quot;Roll over! Roll over!&quot; So they all rolled over and one fell out ... nine ... eight ... etc. There were two in the bed And the little one said, &quot;Roll over! Roll over!&quot; So they all rolled over and one fell out There was one in the bed And the little one said, &quot;Alone at last!&quot; sleeping_people_poem &lt;- function (num_people = 10, struct = &quot;bed&quot;) { while (num_people &gt; 0) { if (num_people == 1) { cat( paste0( &quot;\\nThere was &quot;, english::as.english(num_people), &quot; in the &quot;, struct ) ) cat( &quot;\\nAnd the little one said,\\nAlone at last!&quot; ) } else { cat( paste0( &quot;\\nThere were &quot;, english::as.english(num_people), &quot; in the &quot;, struct ) ) cat( &quot;\\nAnd the little one said,\\nRoll over! Roll over!\\nSo they all rolled over and one fell out\\n&quot; ) } num_people &lt;- num_people - 1 } } sleeping_people_poem() ## ## There were ten in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were nine in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were eight in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were seven in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were six in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were five in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were four in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were three in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There were two in the bed ## And the little one said, ## Roll over! Roll over! ## So they all rolled over and one fell out ## ## There was one in the bed ## And the little one said, ## Alone at last! We use a similar concept as above to generate the poem. Convert the song “99 bottles of beer on the wall” to a function. Generalise to any number of any vessel containing any liquid on any surface. 99 bottles of beer on the wall, 99 bottles of beer. Take one down and pass it around, 98 bottles of beer on the wall. ... No more bottles of beer on the wall, no more bottles of beer. Go to the store and buy some more, 99 bottles of beer on the wall. We use a similar concept as above, but to generalize for any number of any vessel for any liquid, we pass in parameters to a function containing the loop, with the defaults set to bottles of beer. So that this markdown document isn’t insanely long, I’ve cut down the number of bottles to 3. count_down_poem &lt;- function (num_vessels=99, vessel_type=&quot;bottles&quot;, liquid_type = &quot;beer&quot;, surface_type = &quot;wall&quot; ){ item = paste0(vessel_type, &quot; of &quot;, liquid_type) starting_vessels &lt;- num_vessels while (num_vessels &gt;0) { cat (paste0( num_vessels,&quot; &quot;, item, &quot; on the &quot;, surface_type, &quot;, &quot;, num_vessels,&quot; &quot;, item, &quot;.\\n&quot; )) num_vessels &lt;- num_vessels-1 if(num_vessels == 0) cat( paste0( &quot;Take one down and pass it around, no more &quot;, item, &quot; on the &quot;, surface_type, &quot;.\\n\\n&quot; )) else cat ( paste0( &quot;Take one down and pass it around, &quot;, num_vessels, &quot; &quot;, item, &quot; on the &quot;, surface_type, &quot;.\\n\\n&quot; )) } cat(paste0(&quot;No more &quot;, item, &quot; on the &quot;, surface_type, &quot;, &quot;, &quot;no more &quot;, item, &quot;.\\n&quot;) ) cat(paste0(&quot;Go to the store and buy some more, &quot;, starting_vessels, &quot; &quot;, item, &quot; on the &quot;, surface_type, &quot;.\\n\\n&quot;) ) } count_down_poem(num_vessels = 3) ## 3 bottles of beer on the wall, 3 bottles of beer. ## Take one down and pass it around, 2 bottles of beer on the wall. ## ## 2 bottles of beer on the wall, 2 bottles of beer. ## Take one down and pass it around, 1 bottles of beer on the wall. ## ## 1 bottles of beer on the wall, 1 bottles of beer. ## Take one down and pass it around, no more bottles of beer on the wall. ## ## No more bottles of beer on the wall, no more bottles of beer. ## Go to the store and buy some more, 3 bottles of beer on the wall. count_down_poem(num_vessels = 3, vessel_type = &quot;tanks&quot;, liquid_type = &quot;water&quot;, surface_type = &quot;farm&quot;) ## 3 tanks of water on the farm, 3 tanks of water. ## Take one down and pass it around, 2 tanks of water on the farm. ## ## 2 tanks of water on the farm, 2 tanks of water. ## Take one down and pass it around, 1 tanks of water on the farm. ## ## 1 tanks of water on the farm, 1 tanks of water. ## Take one down and pass it around, no more tanks of water on the farm. ## ## No more tanks of water on the farm, no more tanks of water. ## Go to the store and buy some more, 3 tanks of water on the farm. 4. It’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step: # make a list of 100000 lists of differing length x &lt;- vector(&quot;list&quot;, 100000) for (i in seq_along(x)) { n &lt;- sample(100,1) x[[i]] &lt;- rnorm(n, 10, 1) } # time a loop that increases the length of a vector at each step: output &lt;- vector(&quot;integer&quot;, 0) system.time( for (i in seq_along(x)) { output &lt;- c(output, lengths(x[i])) } ) ## user system elapsed ## 15.977 7.459 23.675 # time a loop that places the output inside a preallocated vector: output &lt;- vector(&quot;integer&quot;, length(x)) system.time( for (i in seq_along(x)) { output[i] &lt;- lengths(x[i]) } ) ## user system elapsed ## 0.07 0.00 0.07 How does this affect performance? Design and execute an experiment. Preallocation significantly increases the performance of the loop, especially when there are large numbers of iterations involved. I generate a list of 100,000 lists of differing length. The loops will assess the length of each list within the list. Using system.time to measure the time it takes to execute the for loop, we find that it takes roughly 17 seconds if the vector’s length is increased at each step, whereas the loop takes less than a tenth of a second if preallocation is used. This is quite a significant performance improvement! 21.3.5 Exercises 1. Imagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files &lt;- dir(“data/”, pattern = “\\.csv$”, full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame. I wrote the diamonds dataset to a csv file twice and stored the files in a folder called test_output (this folder is part of .gitignore so it does not show up in this repo). The loop below should read the csv files, store the tables in a list, then bind the list into a data frame. #write.csv(diamonds, file = &quot;diamonds.csv&quot;) files &lt;- dir(&quot;test_output/&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) files ## character(0) output &lt;- vector(&quot;list&quot;, length(files)) for (i in seq_along(files)) { output[[i]] &lt;- read_csv(files[i]) } output &lt;- bind_cols(output) 2. What happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique? If there are no names, names(x) is NULL and the loop does not execute. If only some of the elements are named, all elements of X are iterated through but nm will be NA for the elements that are unnamed. If the names are not unique, the loop acts normally–all elements will be iterated through and the non-unique names will still be used. Example below: print_names &lt;- function (x){ for(nm in names(x)) { print(nm) } } # if x has no names: x &lt;- c(1:10) print_names(x) # nothing happens names(x)[2:5] &lt;- letters[1:4] print_names(x) # NA is printed when names do not exist ## [1] NA ## [1] &quot;a&quot; ## [1] &quot;b&quot; ## [1] &quot;c&quot; ## [1] &quot;d&quot; ## [1] NA ## [1] NA ## [1] NA ## [1] NA ## [1] NA names(x) &lt;- c(letters[1:5], letters[1:5]) print_names(x) ## [1] &quot;a&quot; ## [1] &quot;b&quot; ## [1] &quot;c&quot; ## [1] &quot;d&quot; ## [1] &quot;e&quot; ## [1] &quot;a&quot; ## [1] &quot;b&quot; ## [1] &quot;c&quot; ## [1] &quot;d&quot; ## [1] &quot;e&quot; 3. Write a function that prints the mean of each numeric column in a data frame, along with its name. For example, show_mean(iris) would print: print_means &lt;- function (df) { for (i in seq_along(df)) { if (is.numeric(df[,i])){ print(paste0( colnames(df)[i], &quot;: &quot;, mean(df[,i]))) } } } print_means(iris) ## [1] &quot;Sepal.Length: 5.84333333333333&quot; ## [1] &quot;Sepal.Width: 3.05733333333333&quot; ## [1] &quot;Petal.Length: 3.758&quot; ## [1] &quot;Petal.Width: 1.19933333333333&quot; # show_mean(iris) #&gt; Sepal.Length: 5.84 #&gt; Sepal.Width: 3.06 #&gt; Petal.Length: 3.76 #&gt; Petal.Width: 1.20 (Extra challenge: what function did I use to make sure that the numbers lined up nicely, even though the variable names had different lengths?) If we want to make the numbers line up nicely, we can find the length of the longest column name and add spaces to the other column names to let them match up. The function that could do this (add spaces to strings) would be str_pad, part of stringr. I would first find which column names correspond to numeric columns. Then, I would add the colon using str_c(). I would then determine the maximum str length using str_length() and max(), and then apply this length to the str_pad() function in order to add the appropriate number of spaces. print_means &lt;- function (df) { numeric_cols &lt;- vector() # find out which columns are numeric for (i in seq_along(df)) { if (is.numeric(df[,i])){ numeric_cols &lt;- append(numeric_cols, colnames(df)[i]) } } # add the colon to the column name padded_cols &lt;- str_c(numeric_cols, &quot;: &quot;) # determine lenth of each name max_str_length &lt;- max(str_length(padded_cols)) # print out a padded version of the name with a rounded mean value for (i in seq_along(padded_cols)) { print(paste0( str_pad(padded_cols[i], max_str_length, &quot;right&quot;), round(mean(df[[numeric_cols[i]]]),2) )) } } print_means(iris) ## [1] &quot;Sepal.Length: 5.84&quot; ## [1] &quot;Sepal.Width: 3.06&quot; ## [1] &quot;Petal.Length: 3.76&quot; ## [1] &quot;Petal.Width: 1.2&quot; 4. What does this code do? How does it work? trans &lt;- list( disp = function(x) x * 0.0163871, am = function(x) { factor(x, labels = c(&quot;auto&quot;, &quot;manual&quot;)) } ) head(mtcars[,c(&quot;disp&quot;, &quot;am&quot;)]) ## disp am ## Mazda RX4 160 1 ## Mazda RX4 Wag 160 1 ## Datsun 710 108 1 ## Hornet 4 Drive 258 0 ## Hornet Sportabout 360 0 ## Valiant 225 0 for (var in names(trans)) { mtcars[[var]] &lt;- trans[[var]](mtcars[[var]]) } head(mtcars[,c(&quot;disp&quot;, &quot;am&quot;)]) ## disp am ## Mazda RX4 2.621936 manual ## Mazda RX4 Wag 2.621936 manual ## Datsun 710 1.769807 manual ## Hornet 4 Drive 4.227872 auto ## Hornet Sportabout 5.899356 auto ## Valiant 3.687098 auto The first chunk of code defines a list called “trans” which contains two entries, one named “disp” which is a function that multiplies a value by 0.0163871, and another function named “am” which categorizes values based on whether they are equal to “auto” or “manual”. Afterwards, the for loop iterates through the items in the list trans (which are “disp” and “am”), which both also happen to be names of columns in the built-in R dataset mtcars. The code overwrites the existing columns with new values according to the function called by trans[[var]]. For example, trans[disp] will multiply the disp column in mtcars by 0.0163871 and then update that column with the new value. The column “am” will be updated from 1 and 0 values to “manual” and “auto”. 21.4.1 Exercises 1. Read the documentation for apply(). In the 2d case, what two for loops does it generalise? apply(), as the name suggests, will apply a function of your choosing to either all the rows (MARGIN = 1), all the columns (MARGIN=2), or all the rows and columns (MARGIN = c(1,2)). The for loops that it generalizes are those that iterate sequentially through each column of the df, or through each row of the df, or a nested loop that iterates through each row within each column. A short example is below, which calculates the mean each column of a dataset (MARGIN = 2), or of each row of dataset (MARGIN = 1). mtcars_subset &lt;- mtcars[1:5,1:5] apply(mtcars_subset, 2, mean) # by column ## mpg cyl disp hp drat ## 20.980000 6.000000 3.428181 119.600000 3.576000 apply(mtcars_subset, 1, mean) # by row ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 28.70439 28.70439 25.08396 28.94157 ## Hornet Sportabout ## 42.14987 apply(mtcars_subset, c(1,2), mean) # by both col and row ## mpg cyl disp hp drat ## Mazda RX4 21.0 6 2.621936 110 3.90 ## Mazda RX4 Wag 21.0 6 2.621936 110 3.90 ## Datsun 710 22.8 4 1.769807 93 3.85 ## Hornet 4 Drive 21.4 6 4.227872 110 3.08 ## Hornet Sportabout 18.7 8 5.899356 175 3.15 # theoretical loop that apply (MARGIN = 2) generalizes, similar to col_summary() from this chapter apply_mean_col &lt;- function(df, fun) { out &lt;- vector(&quot;double&quot;, ncol(df)) for (i in seq_along(df)) { out[i] &lt;- fun(df[[i]]) } out } apply_mean_col(mtcars_subset, mean) ## [1] 20.980000 6.000000 3.428181 119.600000 3.576000 # theoretical loop that apply (MARGIN = 1) generalizes apply_mean_row &lt;- function(df, fun) { out &lt;- vector(&quot;double&quot;, nrow(df)) for (i in 1:nrow(df)) { out[i] &lt;- fun(unlist(df[i,])) } out } apply_mean_row(mtcars_subset, mean) ## [1] 28.70439 28.70439 25.08396 28.94157 42.14987 2. Adapt col_summary() so that it only applies to numeric columns. You might want to start with an is_numeric() function that returns a logical vector that has a TRUE corresponding to each numeric column. col_summary_numeric &lt;- function(df, fun) { numeric_cols &lt;- vector(&quot;logical&quot;, length(df)) for ( i in seq_along(df)) { if ( is.numeric(df[[i]]) ) numeric_cols[i] &lt;- T else numeric_cols[i] &lt;- F } df_numeric &lt;- df[,numeric_cols] out &lt;- vector(&quot;double&quot;, length(df_numeric)) for (i in seq_along(df_numeric)) { out[i] &lt;- fun(df_numeric[[i]]) } names(out) &lt;- colnames(df_numeric) out } col_summary_numeric(mtcars, mean) ## mpg cyl disp hp drat wt ## 20.090625 6.187500 3.780862 146.687500 3.596563 3.217250 ## qsec vs gear carb ## 17.848750 0.437500 3.687500 2.812500 21.5.3 Exercises 1. Write code that uses one of the map functions to: Compute the mean of every column in mtcars. map_dbl(mtcars, mean) ## Warning in mean.default(.x[[i]], ...): argument is not numeric or logical: ## returning NA ## mpg cyl disp hp drat wt ## 20.090625 6.187500 3.780862 146.687500 3.596563 3.217250 ## qsec vs am gear carb ## 17.848750 0.437500 NA 3.687500 2.812500 Determine the type of each column in nycflights13::flights. map_chr(nycflights13::flights, typeof) ## year month day dep_time sched_dep_time ## &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## dep_delay arr_time sched_arr_time arr_delay carrier ## &quot;double&quot; &quot;integer&quot; &quot;integer&quot; &quot;double&quot; &quot;character&quot; ## flight tailnum origin dest air_time ## &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;double&quot; ## distance hour minute time_hour ## &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; Compute the number of unique values in each column of iris. map_int(iris, function(a) length(unique(a))) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 35 23 43 22 3 Generate 10 random normals for each of μ=−10,0,10, and 100. map(c(10,0,10,100), function (b) rnorm(10, b, 1)) ## Error in paste(&quot;(^&quot;, regions, &quot;)&quot;, sep = &quot;&quot;, collapse = &quot;|&quot;): cannot coerce type &#39;closure&#39; to vector of type &#39;character&#39; # also works # map(c(10,0,10,100), ~ rnorm(10, ., 1)) 2. How can you create a single vector that for each column in a data frame indicates whether or not it’s a factor? Use map() to run is.factor() on each column. map_lgl(iris, is.factor) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## FALSE FALSE FALSE FALSE TRUE 3. What happens when you use the map functions on vectors that aren’t lists? What does map(1:5, runif) do? Why? The output is the same whether the vector is a list or an atomic vector. map(1:5,runif) calls runif(1), runif(2), runif(3), runif(4), and runif(5), as does map(list(1,2,3,4,5), runif). map(1:5, runif) ## Error in paste(&quot;(^&quot;, regions, &quot;)&quot;, sep = &quot;&quot;, collapse = &quot;|&quot;): cannot coerce type &#39;closure&#39; to vector of type &#39;character&#39; map(list(1,2,3,4,5), runif) ## Error in paste(&quot;(^&quot;, regions, &quot;)&quot;, sep = &quot;&quot;, collapse = &quot;|&quot;): cannot coerce type &#39;closure&#39; to vector of type &#39;character&#39; 4. What does map(-2:2, rnorm, n = 5) do? Why? What does map_dbl(-2:2, rnorm, n = 5) do? Why? map(-2:2, rnorm, n = 5) calls rnorm(n=5,-2), rnorm(n=5,-1), rnorm(n=5,0), rnorm(n=5,1), and rnorm(n=5,2), and returns the output as a list of vectors. However, map_dbl(-2:2, rnorm, n = 5) results in an error. This is because map_dbl cannot return a list of vectors, and can only return one vector in which the values are all doubles. map(-2:2, rnorm, n = 5) ## Error in map(-2:2, rnorm, n = 5): argument 3 matches multiple formal arguments # map_dbl(-2:2, rnorm, n = 5) # Error: Result 1 must be a single double, not a double vector of length 5 5. Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) to eliminate the anonymous function. This mapping function assumes that the dataset x has multiple entries in which a linear model can be fitted between variables mpg and wt. This looks like it was meant to analyze the mtcars dataset, which we can subset using split(). For example, let’s split by column “am”. Then, the mapping call should calculate a linear model between mpg and wt for “auto” cars as well as “manual” cars. x &lt;- split(mtcars, mtcars$am) # orig function map(x, function(df) lm(mpg ~ wt, data = df)) ## Error in paste(&quot;(^&quot;, regions, &quot;)&quot;, sep = &quot;&quot;, collapse = &quot;|&quot;): cannot coerce type &#39;closure&#39; to vector of type &#39;character&#39; # rewrite function to eliminate anonymous function map(x, ~ lm(mpg ~ wt, data = .)) ## Error in map.poly(database, regions, exact, xlim, ylim, boundary, interior, : no recognized region names 21.9.3 Exercises 1. Implement your own version of every() using a for loop. Compare it with purrr::every(). What does purrr’s version do that your version doesn’t? # purrr::every() every(mtcars, is.numeric) ## [1] FALSE every(mtcars, is.atomic) ## [1] TRUE # my implementation my_every &lt;- function (x, fun, ...) { # default value to return is TRUE is_every &lt;- TRUE for (i in seq_along(x)) { # if an item in x does not satisfy the function, change is_every to FALSE if (fun(x[[i]],...) == F) { is_every &lt;- FALSE } } is_every } my_every(mtcars, is.numeric) ## [1] FALSE my_every(mtcars, is.atomic) ## [1] TRUE When looking at the source code behind purrr:every(), I see that they tested whether there are any NA values in the input, and return NA if true. They also use is_false() instead of == F, which seems to be safer. They also return a default value of TRUE unless one of the items in the input does not satisfy the logical function, which then ends the for loop early by returning FALSE. In retrospect, this method is much more efficient compared to my version because it will end the loop early rather than having to finish iterating through what could potentially be a very large loop. 2. Create an enhanced col_summary() that applies a summary function to every numeric column in a data frame. col_summary_numeric &lt;- function(df, fun) { # first determine which columns are numeric numeric_cols &lt;- vector(&quot;logical&quot;, length(df)) for ( i in seq_along(df)) { if ( is.numeric(df[[i]]) ) numeric_cols[i] &lt;- T else numeric_cols[i] &lt;- F } # subset the data based on only the numeric columns df_numeric &lt;- df[,numeric_cols] # apply the summary function to each of the columns in the subsetted data out &lt;- vector(&quot;double&quot;, length(df_numeric)) for (i in seq_along(df_numeric)) { out[i] &lt;- fun(df_numeric[[i]]) } # annotate and return the output names(out) &lt;- colnames(df_numeric) out } col_summary_numeric(mtcars, mean) ## mpg cyl disp hp drat wt ## 20.090625 6.187500 3.780862 146.687500 3.596563 3.217250 ## qsec vs gear carb ## 17.848750 0.437500 3.687500 2.812500 3. A possible base R equivalent of col_summary() is: col_sum3 &lt;- function(df, f) { is_num &lt;- sapply(df, is.numeric) df_num &lt;- df[, is_num] sapply(df_num, f) } But it has a number of bugs as illustrated with the following inputs: df &lt;- tibble( x = 1:3, y = 3:1, z = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ) # OK col_sum3(df, mean) ## x y ## 2 2 # Has problems: don&#39;t always return numeric vector col_sum3(df[1:2], mean) ## x y ## 2 2 col_sum3(df[1], mean) ## x ## 2 # col_sum3(df[0], mean) # Error: Can&#39;t subset with `[` using an object of class list. What causes the bugs? To view the errors more in-depth, we can use purrr::safely() to observe what’s going on. For the first two “problematic” entries, there does not seem to be an error, as the $error portion is NULL. Furthermore, using typeof( col_sum3(df[1:2], mean)) returns “double”, suggesting that the function is indeed returning a numeric vector in this instance. We do observe an error for col_sum3(df[0], mean), which tries to call the col_sum3 function on an empty data frame. The error comes from the line, df_num &lt;- df[, is_num]. This is because is_num is an emtpy list, and trying to subset an empty data frame with an empty list results in the error. The code below will walk through this phenomenon. safely_col_sum3 &lt;- safely(col_sum3) safely_col_sum3(df[1:2], mean) ## $result ## x y ## 2 2 ## ## $error ## NULL safely_col_sum3(df[1], mean) ## $result ## x ## 2 ## ## $error ## NULL safely_col_sum3(df[0], mean) ## $result ## NULL ## ## $error ## &lt;error&gt; ## message: Can&#39;t subset with `[` using an object of class list. ## class: `rlang_error` ## backtrace: ## 1. rmarkdown::render_site(output_format = &quot;bookdown::gitbook&quot;, encoding = &quot;UTF-8&quot;) ## 30. purrr:::safely_col_sum3(df[0], mean) ## 39. global::.f(...) ## 41. tibble:::`[.tbl_df`(df, , is_num) ## 42. tibble:::check_names_df(j, x) ## Call `rlang::last_trace()` to see the full backtrace # df[0] creates an empty data frame df &lt;- df[0] df ## # A tibble: 3 x 0 # is_num is an empty list is_num &lt;- sapply(df, is.numeric) is_num ## named list() # this throws the error # df_num &lt;- df[, is_num] # Error: Can&#39;t subset with `[` using an object of class list. "],
["chapter-23-model-basics.html", "Chapter 23 - Model basics Notes - making simple models 23.2.1 Exercises 23.3.3 Exercises 23.4.5 Exercises", " Chapter 23 - Model basics Notes - making simple models library(tidyverse) library(modelr) The datasets that are used in this chapter are simulated datasets, such as the one shown below (sim1) head(sim1) ## # A tibble: 6 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 4.20 ## 2 1 7.51 ## 3 1 2.13 ## 4 2 8.99 ## 5 2 10.2 ## 6 2 11.3 Below are the functions used in this chapter, written by Hadley for demonstration purposes: model1 &lt;- function(a, data) { a[1] + data$x * a[2] } measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) sqrt(mean(diff ^ 2)) } sim1_dist &lt;- function(a1, a2) { measure_distance(c(a1, a2), sim1) } grid &lt;- expand.grid( a1 = seq(-5, 20, length = 25), a2 = seq(1, 3, length = 25) ) %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) head(grid) ## a1 a2 dist ## 1 -5.0000000 1 15.45248 ## 2 -3.9583333 1 14.44317 ## 3 -2.9166667 1 13.43881 ## 4 -1.8750000 1 12.44058 ## 5 -0.8333333 1 11.45009 ## 6 0.2083333 1 10.46955 dim(grid) ## [1] 625 3 grid %&gt;% ggplot(aes(a1, a2)) + geom_point(data = filter(grid, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(grid, rank(dist) &lt;= 10) ) 23.2.1 Exercises 1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model? In the simulated dataset, there are a couple of outliers that are far displaced from the rest of the points. These outliers can skew the linear approximation, because these points are so ‘distant’ from the other points in the dataset. Because the linear model tries to minimize the distance between each point and the fitted model (the “residuals”), these outliers will skew the approximation, pulling the line closer to them. The larger the residual, the more it contributes to the RMSE. We notice that the fitted line is slightly skewed towards the direction of the outlying point. sim1a &lt;- tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rt(length(x), df = 2) ) # first, take a look at the data sim1a ## # A tibble: 30 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 7.18 ## 2 1 8.82 ## 3 1 6.37 ## 4 2 10.5 ## 5 2 8.93 ## 6 2 7.69 ## 7 3 11.3 ## 8 3 9.12 ## 9 3 10.5 ## 10 4 9.71 ## # … with 20 more rows ggplot(sim1a, aes (x, y)) + geom_point() mod &lt;- lm(y~x, data = sim1a) summary(mod) ## ## Call: ## lm(formula = y ~ x, data = sim1a) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5890 -1.0620 0.1046 1.0860 3.7401 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.6279 0.6715 8.381 4.07e-09 *** ## x 1.5487 0.1082 14.310 2.10e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.703 on 28 degrees of freedom ## Multiple R-squared: 0.8797, Adjusted R-squared: 0.8754 ## F-statistic: 204.8 on 1 and 28 DF, p-value: 2.105e-14 # add the fitted linear model to the scatterplot ggplot(sim1a, aes (x, y)) + geom_point()+ geom_abline(intercept = mod$coefficients[1], slope = mod$coefficients[2]) # compare with the baseR lm with geom_smooth() overlay, looks like they overlap, as expected ggplot(sim1a, aes (x, y)) + geom_point()+ geom_abline(intercept = mod$coefficients[1], slope = mod$coefficients[2], size=3, color = &quot;red&quot;)+ geom_smooth(method = &#39;lm&#39;) 2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance: measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) mean(abs(diff)) } # use optim() function to best &lt;- optim(c(0, 0), measure_distance, data = sim1a) best$par ## [1] 5.592108 1.586087 # compare the parameters from optim() with the parameters obtained from lm() mod &lt;- lm (y~x, data = sim1a) coef(mod) ## (Intercept) x ## 5.627914 1.548750 # plot the two lines on the scatterplot to observe differences in fit ggplot(data = sim1a, aes(x, y))+ geom_point()+ geom_abline(slope = mod$coefficients[2], intercept = mod$coefficients[1], color = &quot;red&quot;)+ geom_abline(slope = best$par[2], intercept = best$par[1], color = &quot;blue&quot;)+ labs(title = &quot;Red = root-mean-squared distance fit using lm() \\n Blue = mean-absolute distance fit using optim()&quot;) Use optim() to fit this model to the simulated data above and compare it to the linear model. The measure_distance() function provided above uses the absolute-mean distance (mean(abs(diff))) instead of the root-mean-squared distance, sqrt(mean(diff^2)). Using optim() and the absolute-mean distance, we find that the line is less skewed by the outlying points. The red line is “pulled” more towards the outliers, whereas the blue line remains more embbeded with the bulk of the data. This is because squaring the residuals results in much greater values when the residuals are large, so minimizing the residuals for outliers takes more priority when using the squared distance. 3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this? A quadratic or higher order function may have more than one local minimum / maximum. This may result in the optim() function providing an unideal result. In the provided function, since a[1] and a[3] are both constants that are not multiplied by a column in data (such as data$x), they can be added together and represent the intercept of the line. This results in the sum of a[1] and a[3] equalling the intercept we found before using the equation a[1] + data$x * a[2]. a[1] and a[3] can therefore have infinite possibilites of values, as long as the sum of a[1] and a[3] are equal to the local optimum of a[1] + data$x * a[2]. In the example below, if we use the dataset sim1, we find that a[1] and a[3] must sum to 4.220074. So, depending on where you start with the optim() function, a[1] and a[3] will have differing values, but still add up to 4.220074. We see in the graph that the optim() function and lm() again provde the same result. model1 &lt;- function(a, data) { a[1] + data$x * a[2] + a[3] } measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) sqrt(mean(diff ^ 2)) } best &lt;- optim(c(0, 0, 0), measure_distance, data = sim1) best$par ## [1] 3.3672228 2.0515737 0.8528513 best &lt;- optim(c(0, 0, 1), measure_distance, data = sim1) best$par ## [1] -3.469885 2.051509 7.690289 # since in the model above, a[1] and a[3] may be theoretically combined to represent the intercept of the line, we can graph it as such: ggplot(data = sim1, aes(x, y))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, size = 2)+ geom_abline(slope = best$par[2], intercept = best$par[1] + best$par[3], color = &quot;blue&quot;)+ labs(title = &quot;Red = using lm() \\n Blue = optim() using the provided 3 parameter model&quot;) 23.3.3 Exercises 1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()? Using the loess() function instead of lm(), the line curves more towards the direction of variation and is not strictly a straight line. The default method for fitting using geom_smooth() is loess(), so the line that is superimposed on the ggplot is the same as the line generated by the predictions using the loess() model. When we superimpose all 3 options (loess() prediction, lm() prediction, and geom_smooth()), we see that geom_smooth() and the loess() prediction precisely overlap, whereas the lm() prediction does not. # using lm() sim1_mod_lm &lt;- lm(y ~ x, data = sim1) grid &lt;- sim1 %&gt;% data_grid(x) grid ## # A tibble: 10 x 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 grid &lt;- grid %&gt;% add_predictions(sim1_mod_lm) grid ## # A tibble: 10 x 2 ## x pred ## &lt;int&gt; &lt;dbl&gt; ## 1 1 6.27 ## 2 2 8.32 ## 3 3 10.4 ## 4 4 12.4 ## 5 5 14.5 ## 6 6 16.5 ## 7 7 18.6 ## 8 8 20.6 ## 9 9 22.7 ## 10 10 24.7 sim1 &lt;- sim1 %&gt;% add_residuals(sim1_mod_lm) sim1 ## # A tibble: 30 x 3 ## x y resid ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4.20 -2.07 ## 2 1 7.51 1.24 ## 3 1 2.13 -4.15 ## 4 2 8.99 0.665 ## 5 2 10.2 1.92 ## 6 2 11.3 2.97 ## 7 3 7.36 -3.02 ## 8 3 10.5 0.130 ## 9 3 10.5 0.136 ## 10 4 12.4 0.00763 ## # … with 20 more rows ggplot(sim1, aes(x)) + geom_point(aes(y = y)) + geom_line(aes(y = pred), data = grid, colour = &quot;red&quot;, size = 1) ggplot(sim1, aes(x, resid)) + geom_ref_line(h = 0) + geom_point() # using loess() sim1_mod_loess &lt;- loess(y ~ x, data = sim1) grid &lt;- sim1 %&gt;% data_grid(x) grid ## # A tibble: 10 x 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 grid &lt;- grid %&gt;% add_predictions(sim1_mod_loess) grid ## # A tibble: 10 x 2 ## x pred ## &lt;int&gt; &lt;dbl&gt; ## 1 1 5.34 ## 2 2 8.27 ## 3 3 10.8 ## 4 4 12.8 ## 5 5 14.6 ## 6 6 16.6 ## 7 7 18.7 ## 8 8 20.8 ## 9 9 22.6 ## 10 10 24.0 sim1 &lt;- sim1 %&gt;% add_residuals(sim1_mod_loess) sim1 ## # A tibble: 30 x 3 ## x y resid ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4.20 -1.14 ## 2 1 7.51 2.17 ## 3 1 2.13 -3.21 ## 4 2 8.99 0.714 ## 5 2 10.2 1.97 ## 6 2 11.3 3.02 ## 7 3 7.36 -3.45 ## 8 3 10.5 -0.304 ## 9 3 10.5 -0.298 ## 10 4 12.4 -0.345 ## # … with 20 more rows # residuals plot ggplot(sim1, aes(x, resid)) + geom_ref_line(h = 0) + geom_point() # plot the regression line ggplot(sim1, aes(x)) + geom_point(aes(y = y)) + geom_line(aes(y = pred), data = grid, colour = &quot;red&quot;, size = 1) # compare to geom_smooth() and lm() ggplot(sim1, aes(x, y)) + geom_point() + geom_line(aes(y = pred), data = grid, colour = &quot;red&quot;, size = 3)+ geom_smooth(color = &quot;blue&quot;, se = F)+ geom_smooth(method = &quot;lm&quot;, color = &quot;green&quot;, se = F) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 2. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ? add_predictions() can only work with one supplied model, in which it will generate a new column named “pred” in your data frame. gather_predictions() and spread_predictions() can work with multiple supplied models and generate predictions for each model supplied. gather_predictions() appends the model name to the data frame along with the predictions as new rows to the data frame, in a tidy fashion. spread_predictions() appends the new predictions to the data frame as separate columns. You can visualize the differences below, in which spread_predictions makes the data frame “wider” and gather_predictions() makes the data frame “taller”. grid &lt;- sim1 %&gt;% data_grid(x) grid ## # A tibble: 10 x 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 grid_add &lt;- grid %&gt;% add_predictions(sim1_mod_lm) grid_add ## # A tibble: 10 x 2 ## x pred ## &lt;int&gt; &lt;dbl&gt; ## 1 1 6.27 ## 2 2 8.32 ## 3 3 10.4 ## 4 4 12.4 ## 5 5 14.5 ## 6 6 16.5 ## 7 7 18.6 ## 8 8 20.6 ## 9 9 22.7 ## 10 10 24.7 grid_gather &lt;- grid %&gt;% gather_predictions(sim1_mod_lm, sim1_mod_loess) grid_gather ## # A tibble: 20 x 3 ## model x pred ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sim1_mod_lm 1 6.27 ## 2 sim1_mod_lm 2 8.32 ## 3 sim1_mod_lm 3 10.4 ## 4 sim1_mod_lm 4 12.4 ## 5 sim1_mod_lm 5 14.5 ## 6 sim1_mod_lm 6 16.5 ## 7 sim1_mod_lm 7 18.6 ## 8 sim1_mod_lm 8 20.6 ## 9 sim1_mod_lm 9 22.7 ## 10 sim1_mod_lm 10 24.7 ## 11 sim1_mod_loess 1 5.34 ## 12 sim1_mod_loess 2 8.27 ## 13 sim1_mod_loess 3 10.8 ## 14 sim1_mod_loess 4 12.8 ## 15 sim1_mod_loess 5 14.6 ## 16 sim1_mod_loess 6 16.6 ## 17 sim1_mod_loess 7 18.7 ## 18 sim1_mod_loess 8 20.8 ## 19 sim1_mod_loess 9 22.6 ## 20 sim1_mod_loess 10 24.0 grid_spread &lt;- grid %&gt;% spread_predictions(sim1_mod_lm, sim1_mod_loess) grid_spread ## # A tibble: 10 x 3 ## x sim1_mod_lm sim1_mod_loess ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 6.27 5.34 ## 2 2 8.32 8.27 ## 3 3 10.4 10.8 ## 4 4 12.4 12.8 ## 5 5 14.5 14.6 ## 6 6 16.5 16.6 ## 7 7 18.6 18.7 ## 8 8 20.6 20.8 ## 9 9 22.7 22.6 ## 10 10 24.7 24.0 3. What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important? geom_ref_line() adds either a horizontal or vertical line at a specified position to your ggplot, of a specified color (default white). It comes from modelr. This is useful when plotting residuals because ideally the residuals should be centered around 0. Having a reference line helps the viewer judge how the residuals behave. Conceptually, we can think of the horizontal line in the residuals plot as the prediction, and of the residual as how far off the true value is from that prediction. 4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals? The residuals, ideally, should approximately be normally distributed. Examining the frequency polygon is useful as a visual assessment for whether or not the residuals follow the Normal distribution. This graph will also more easily capture any abnormal pattern in the residuals, in which there are over-representations of either + or - residuals. The cons are that this graph masks some of the variability in the residuals by binning them, and you lose the relationship between the residual and the predictor variable. This is best paired with a scatterplot of the residuals so you can observe exactly where each point lies in relation to the predictor variable. # example from book ggplot(sim1, aes(resid)) + geom_freqpoly(binwidth = 0.5) # make an example where residuals approximate normal distribution x = seq(1:1000) y = 12 + 5 * x + rnorm(1000,0,100) # add random white noise mysim &lt;- as_tibble(cbind(x, y)) #fit model mysim_mod &lt;- lm(y ~ x, data = mysim) summary(mysim_mod) ## ## Call: ## lm(formula = y ~ x, data = mysim) ## ## Residuals: ## Min 1Q Median 3Q Max ## -297.929 -70.571 -0.186 72.620 296.649 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.14761 6.47572 2.339 0.0195 * ## x 5.00035 0.01121 446.146 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 102.3 on 998 degrees of freedom ## Multiple R-squared: 0.995, Adjusted R-squared: 0.995 ## F-statistic: 1.99e+05 on 1 and 998 DF, p-value: &lt; 2.2e-16 # make predictions mygrid &lt;- mysim %&gt;% data_grid(x) mygrid ## # A tibble: 1,000 x 1 ## x ## &lt;dbl&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 ## # … with 990 more rows mygrid &lt;- mygrid %&gt;% add_predictions(mysim_mod) mygrid ## # A tibble: 1,000 x 2 ## x pred ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 20.1 ## 2 2 25.1 ## 3 3 30.1 ## 4 4 35.1 ## 5 5 40.1 ## 6 6 45.1 ## 7 7 50.2 ## 8 8 55.2 ## 9 9 60.2 ## 10 10 65.2 ## # … with 990 more rows mysim &lt;- mysim %&gt;% add_residuals(mysim_mod) mysim ## # A tibble: 1,000 x 3 ## x y resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 110. 89.4 ## 2 2 45.6 20.5 ## 3 3 162. 132. ## 4 4 78.7 43.6 ## 5 5 -9.92 -50.1 ## 6 6 135. 90.3 ## 7 7 -151. -201. ## 8 8 94.4 39.3 ## 9 9 162. 101. ## 10 10 -39.7 -105. ## # … with 990 more rows # plot prediction line ggplot(mysim, aes(x)) + geom_point(aes(y = y)) + geom_line(aes(y = pred), data = mygrid, colour = &quot;red&quot;, size = 1) # plot freqpoly() of residuals ggplot(mysim, aes(resid)) + geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # plot scatterplot of residuals ggplot(mysim, aes (x = x, y = resid))+ geom_ref_line(h=0)+ geom_point() 23.4.5 Exercises For categorical variables, the book performs a similar prediction workflow: # use the sim2 dataset from modelr sim2 ## # A tibble: 40 x 2 ## x y ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1.94 ## 2 a 1.18 ## 3 a 1.24 ## 4 a 2.62 ## 5 a 1.11 ## 6 a 0.866 ## 7 a -0.910 ## 8 a 0.721 ## 9 a 0.687 ## 10 a 2.07 ## # … with 30 more rows # examine how the data are distributed ggplot(sim2) + geom_point(aes(x, y)) # generate a model (R automatically recognizes that the predictor variables are categorical) mod2 &lt;- lm(y ~ x, data = sim2) # generate predictions based on model grid &lt;- sim2 %&gt;% data_grid(x) %&gt;% add_predictions(mod2) grid ## # A tibble: 4 x 2 ## x pred ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1.15 ## 2 b 8.12 ## 3 c 6.13 ## 4 d 1.91 # plot the predictions overlaid on the graph ggplot(sim2, aes(x)) + geom_point(aes(y = y)) + geom_point(data = grid, aes(y = pred), colour = &quot;red&quot;, size = 4) For using more than one predictor variable (can be a combination of categorical and continuous variables), a similar approach is used: # examine data and build models sim3 ## # A tibble: 120 x 5 ## x1 x2 rep y sd ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 a 1 -0.571 2 ## 2 1 a 2 1.18 2 ## 3 1 a 3 2.24 2 ## 4 1 b 1 7.44 2 ## 5 1 b 2 8.52 2 ## 6 1 b 3 7.72 2 ## 7 1 c 1 6.51 2 ## 8 1 c 2 5.79 2 ## 9 1 c 3 6.07 2 ## 10 1 d 1 2.11 2 ## # … with 110 more rows mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) # inputting multiple variables into data_grid results in it returning all the possible combinations between x1 and x2 grid &lt;- sim3 %&gt;% data_grid(x1, x2) %&gt;% gather_predictions(mod1, mod2) grid ## # A tibble: 80 x 4 ## model x1 x2 pred ## &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 mod1 1 a 1.67 ## 2 mod1 1 b 4.56 ## 3 mod1 1 c 6.48 ## 4 mod1 1 d 4.03 ## 5 mod1 2 a 1.48 ## 6 mod1 2 b 4.37 ## 7 mod1 2 c 6.28 ## 8 mod1 2 d 3.84 ## 9 mod1 3 a 1.28 ## 10 mod1 3 b 4.17 ## # … with 70 more rows Inputting multiple continuous variables into data_grid will also return all possible combinations, unless you manually specify the combinations as done in the book chapter. # examine data and build models sim4 ## # A tibble: 300 x 4 ## x1 x2 rep y ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -1 -1 1 4.25 ## 2 -1 -1 2 1.21 ## 3 -1 -1 3 0.353 ## 4 -1 -0.778 1 -0.0467 ## 5 -1 -0.778 2 4.64 ## 6 -1 -0.778 3 1.38 ## 7 -1 -0.556 1 0.975 ## 8 -1 -0.556 2 2.50 ## 9 -1 -0.556 3 2.70 ## 10 -1 -0.333 1 0.558 ## # … with 290 more rows mod1 &lt;- lm(y ~ x1 + x2, data = sim4) mod2 &lt;- lm(y ~ x1 * x2, data = sim4) # with manual specification of range grid &lt;- sim4 %&gt;% data_grid( x1 = seq_range(x1, 5), x2 = seq_range(x2, 5) ) %&gt;% gather_predictions(mod1, mod2) grid ## # A tibble: 50 x 4 ## model x1 x2 pred ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mod1 -1 -1 0.996 ## 2 mod1 -1 -0.5 -0.395 ## 3 mod1 -1 0 -1.79 ## 4 mod1 -1 0.5 -3.18 ## 5 mod1 -1 1 -4.57 ## 6 mod1 -0.5 -1 1.91 ## 7 mod1 -0.5 -0.5 0.516 ## 8 mod1 -0.5 0 -0.875 ## 9 mod1 -0.5 0.5 -2.27 ## 10 mod1 -0.5 1 -3.66 ## # … with 40 more rows # without manual specification of range, provides all combinations of values grid2 &lt;- sim4 %&gt;% data_grid( x1, x2 ) grid2 ## # A tibble: 100 x 2 ## x1 x2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1 -1 ## 2 -1 -0.778 ## 3 -1 -0.556 ## 4 -1 -0.333 ## 5 -1 -0.111 ## 6 -1 0.111 ## 7 -1 0.333 ## 8 -1 0.556 ## 9 -1 0.778 ## 10 -1 1 ## # … with 90 more rows 1. What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions? To fit a model to sim2 without an intercept, use “-1” with the formula, as shown. We observe that the model matrix loses the “intercept” column when we use “-1”. We then generate predictions using gather_predictions() on both models. We find that both models generate the same predictions! This is because in both cases, the mean of the observations for each categorical possibility is the optimal fit. mod1 &lt;- lm(y ~ x, data = sim2) model_matrix(sim2, y ~x) ## # A tibble: 40 x 4 ## `(Intercept)` xb xc xd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 ## 2 1 0 0 0 ## 3 1 0 0 0 ## 4 1 0 0 0 ## 5 1 0 0 0 ## 6 1 0 0 0 ## 7 1 0 0 0 ## 8 1 0 0 0 ## 9 1 0 0 0 ## 10 1 0 0 0 ## # … with 30 more rows mod2 &lt;- lm(y ~ x - 1, data = sim2) model_matrix(sim2, y ~x -1) ## # A tibble: 40 x 4 ## xa xb xc xd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 ## 2 1 0 0 0 ## 3 1 0 0 0 ## 4 1 0 0 0 ## 5 1 0 0 0 ## 6 1 0 0 0 ## 7 1 0 0 0 ## 8 1 0 0 0 ## 9 1 0 0 0 ## 10 1 0 0 0 ## # … with 30 more rows grid &lt;- sim2 %&gt;% data_grid(x) %&gt;% gather_predictions(mod1, mod2) grid ## # A tibble: 8 x 3 ## model x pred ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 a 1.15 ## 2 mod1 b 8.12 ## 3 mod1 c 6.13 ## 4 mod1 d 1.91 ## 5 mod2 a 1.15 ## 6 mod2 b 8.12 ## 7 mod2 c 6.13 ## 8 mod2 d 1.91 ggplot(sim2, aes(x)) + geom_point(aes(y = y)) + geom_point(data = grid, aes(y = pred, shape = model, color = model), size = 4) 2. Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good shorthand for interaction? The model matrix for y ~ x1 + x2 for sim3 has an intercept, an x1 column, and 3 columns for x2, corresponding to each of the possibilities for the categories in x2. The model matrix for y ~ x1 * x2 has an these columns as well, but also has x1:x2b, x1:x2c, and x1:x2d, which correspond to the interaction between x1 and each of the categories in x2. For sim4, since the values of the predictor variables x1 and x2 are both continuous variables, the x2 column does not need to be split up as in sim3. The model matrix consists of 3 columns, an intercept, x1, and x2. Similarly, y ~ x1 * x2 uses a model matrix of 4 columns, which consists of the 3 mentioned previously and an additional column x1:x2. is a good shorthand for this interaction because the additional columns that it adds are products. it suggests that changes in values of one variable will affect the value of the other, which is what it is trying to model. sim3 ## # A tibble: 120 x 5 ## x1 x2 rep y sd ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 a 1 -0.571 2 ## 2 1 a 2 1.18 2 ## 3 1 a 3 2.24 2 ## 4 1 b 1 7.44 2 ## 5 1 b 2 8.52 2 ## 6 1 b 3 7.72 2 ## 7 1 c 1 6.51 2 ## 8 1 c 2 5.79 2 ## 9 1 c 3 6.07 2 ## 10 1 d 1 2.11 2 ## # … with 110 more rows # models fit to sim3 model_matrix(sim3, y ~ x1 + x2 ) ## # A tibble: 120 x 5 ## `(Intercept)` x1 x2b x2c x2d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 ## 2 1 1 0 0 0 ## 3 1 1 0 0 0 ## 4 1 1 1 0 0 ## 5 1 1 1 0 0 ## 6 1 1 1 0 0 ## 7 1 1 0 1 0 ## 8 1 1 0 1 0 ## 9 1 1 0 1 0 ## 10 1 1 0 0 1 ## # … with 110 more rows model_matrix(sim3, y ~ x1 * x2 ) ## # A tibble: 120 x 8 ## `(Intercept)` x1 x2b x2c x2d `x1:x2b` `x1:x2c` `x1:x2d` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 0 0 0 ## 2 1 1 0 0 0 0 0 0 ## 3 1 1 0 0 0 0 0 0 ## 4 1 1 1 0 0 1 0 0 ## 5 1 1 1 0 0 1 0 0 ## 6 1 1 1 0 0 1 0 0 ## 7 1 1 0 1 0 0 1 0 ## 8 1 1 0 1 0 0 1 0 ## 9 1 1 0 1 0 0 1 0 ## 10 1 1 0 0 1 0 0 1 ## # … with 110 more rows sim4 ## # A tibble: 300 x 4 ## x1 x2 rep y ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -1 -1 1 4.25 ## 2 -1 -1 2 1.21 ## 3 -1 -1 3 0.353 ## 4 -1 -0.778 1 -0.0467 ## 5 -1 -0.778 2 4.64 ## 6 -1 -0.778 3 1.38 ## 7 -1 -0.556 1 0.975 ## 8 -1 -0.556 2 2.50 ## 9 -1 -0.556 3 2.70 ## 10 -1 -0.333 1 0.558 ## # … with 290 more rows # models fit to sim4 model_matrix(sim4, y ~ x1 + x2 ) ## # A tibble: 300 x 3 ## `(Intercept)` x1 x2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -1 -1 ## 2 1 -1 -1 ## 3 1 -1 -1 ## 4 1 -1 -0.778 ## 5 1 -1 -0.778 ## 6 1 -1 -0.778 ## 7 1 -1 -0.556 ## 8 1 -1 -0.556 ## 9 1 -1 -0.556 ## 10 1 -1 -0.333 ## # … with 290 more rows model_matrix(sim4, y ~ x1 * x2 ) ## # A tibble: 300 x 4 ## `(Intercept)` x1 x2 `x1:x2` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -1 -1 1 ## 2 1 -1 -1 1 ## 3 1 -1 -1 1 ## 4 1 -1 -0.778 0.778 ## 5 1 -1 -0.778 0.778 ## 6 1 -1 -0.778 0.778 ## 7 1 -1 -0.556 0.556 ## 8 1 -1 -0.556 0.556 ## 9 1 -1 -0.556 0.556 ## 10 1 -1 -0.333 0.333 ## # … with 290 more rows 3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.) The formulas below use the model matricies that were described in exercise 2 of this section. We can re-create the model matricies by writing a function that accepts the predictor variables as input, as well as the type of operator. In the example below, I input sim3 along with either “+” or “*” and show that the model matricies generated match those made by the modelr function model_matrix(). mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) # check to see if the column to add is a categorical variable (factor), and split it up if true add_column &lt;- function ( mycolumn, colname ) { # test whether the columns are factors or not if (is.factor (mycolumn)){ my_levels &lt;- levels(mycolumn) # split into separate columns output &lt;- vector(&quot;list&quot;) for (i in 2: length(my_levels)) { level_name &lt;- str_c(colname, my_levels[i]) output[[level_name]] &lt;- 1*(mycolumn == my_levels[i]) } } # if not factor, return the column as-is else { output &lt;- list( mycolumn) names(output) &lt;- colname } output } # check the type of operator supplied (+ or *) and create the columns as necessary, calling the add_column function make_matrix &lt;- function (data, operator) { my_colnames &lt;- c(&quot;x1&quot;, &quot;x2&quot;) # store the columns of the model matrix in &quot;mm&quot; mm &lt;- list() # make the default intercept column mm$intercept &lt;- rep(1, length(data$x2)) # add the base columns using add_column() for (item in my_colnames) { mm &lt;- c(mm, add_column (data[[item]], item)) } mm &lt;- bind_cols(mm) # if the operator is *, add the appropriate columns based on vector multiplication if (operator == &quot;*&quot;) { x1cols &lt;- colnames(mm)[grep(&quot;x1&quot;, colnames(mm))] x2cols &lt;- colnames(mm)[grep(&quot;x2&quot;, colnames(mm))] newcols &lt;- vector(&quot;list&quot;) for (i in x1cols) { print(i) for (j in x2cols) { print(j) new_level_name &lt;- str_c(i,j, sep = &quot;:&quot;) print(new_level_name) newcols[[new_level_name]] &lt;- mm[[i]]* mm[[j]] } } newcols &lt;- bind_cols(newcols) mm &lt;- cbind(mm, newcols) } mm } make_matrix( sim3, &quot;+&quot;) ## # A tibble: 120 x 5 ## intercept x1 x2b x2c x2d ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 ## 2 1 1 0 0 0 ## 3 1 1 0 0 0 ## 4 1 1 1 0 0 ## 5 1 1 1 0 0 ## 6 1 1 1 0 0 ## 7 1 1 0 1 0 ## 8 1 1 0 1 0 ## 9 1 1 0 1 0 ## 10 1 1 0 0 1 ## # … with 110 more rows model_matrix( sim3, y ~ x1 + x2) ## # A tibble: 120 x 5 ## `(Intercept)` x1 x2b x2c x2d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 ## 2 1 1 0 0 0 ## 3 1 1 0 0 0 ## 4 1 1 1 0 0 ## 5 1 1 1 0 0 ## 6 1 1 1 0 0 ## 7 1 1 0 1 0 ## 8 1 1 0 1 0 ## 9 1 1 0 1 0 ## 10 1 1 0 0 1 ## # … with 110 more rows make_matrix( sim3, &quot;*&quot;) ## [1] &quot;x1&quot; ## [1] &quot;x2b&quot; ## [1] &quot;x1:x2b&quot; ## [1] &quot;x2c&quot; ## [1] &quot;x1:x2c&quot; ## [1] &quot;x2d&quot; ## [1] &quot;x1:x2d&quot; ## intercept x1 x2b x2c x2d x1:x2b x1:x2c x1:x2d ## 1 1 1 0 0 0 0 0 0 ## 2 1 1 0 0 0 0 0 0 ## 3 1 1 0 0 0 0 0 0 ## 4 1 1 1 0 0 1 0 0 ## 5 1 1 1 0 0 1 0 0 ## 6 1 1 1 0 0 1 0 0 ## 7 1 1 0 1 0 0 1 0 ## 8 1 1 0 1 0 0 1 0 ## 9 1 1 0 1 0 0 1 0 ## 10 1 1 0 0 1 0 0 1 ## 11 1 1 0 0 1 0 0 1 ## 12 1 1 0 0 1 0 0 1 ## 13 1 2 0 0 0 0 0 0 ## 14 1 2 0 0 0 0 0 0 ## 15 1 2 0 0 0 0 0 0 ## 16 1 2 1 0 0 2 0 0 ## 17 1 2 1 0 0 2 0 0 ## 18 1 2 1 0 0 2 0 0 ## 19 1 2 0 1 0 0 2 0 ## 20 1 2 0 1 0 0 2 0 ## 21 1 2 0 1 0 0 2 0 ## 22 1 2 0 0 1 0 0 2 ## 23 1 2 0 0 1 0 0 2 ## 24 1 2 0 0 1 0 0 2 ## 25 1 3 0 0 0 0 0 0 ## 26 1 3 0 0 0 0 0 0 ## 27 1 3 0 0 0 0 0 0 ## 28 1 3 1 0 0 3 0 0 ## 29 1 3 1 0 0 3 0 0 ## 30 1 3 1 0 0 3 0 0 ## 31 1 3 0 1 0 0 3 0 ## 32 1 3 0 1 0 0 3 0 ## 33 1 3 0 1 0 0 3 0 ## 34 1 3 0 0 1 0 0 3 ## 35 1 3 0 0 1 0 0 3 ## 36 1 3 0 0 1 0 0 3 ## 37 1 4 0 0 0 0 0 0 ## 38 1 4 0 0 0 0 0 0 ## 39 1 4 0 0 0 0 0 0 ## 40 1 4 1 0 0 4 0 0 ## 41 1 4 1 0 0 4 0 0 ## 42 1 4 1 0 0 4 0 0 ## 43 1 4 0 1 0 0 4 0 ## 44 1 4 0 1 0 0 4 0 ## 45 1 4 0 1 0 0 4 0 ## 46 1 4 0 0 1 0 0 4 ## 47 1 4 0 0 1 0 0 4 ## 48 1 4 0 0 1 0 0 4 ## 49 1 5 0 0 0 0 0 0 ## 50 1 5 0 0 0 0 0 0 ## 51 1 5 0 0 0 0 0 0 ## 52 1 5 1 0 0 5 0 0 ## 53 1 5 1 0 0 5 0 0 ## 54 1 5 1 0 0 5 0 0 ## 55 1 5 0 1 0 0 5 0 ## 56 1 5 0 1 0 0 5 0 ## 57 1 5 0 1 0 0 5 0 ## 58 1 5 0 0 1 0 0 5 ## 59 1 5 0 0 1 0 0 5 ## 60 1 5 0 0 1 0 0 5 ## 61 1 6 0 0 0 0 0 0 ## 62 1 6 0 0 0 0 0 0 ## 63 1 6 0 0 0 0 0 0 ## 64 1 6 1 0 0 6 0 0 ## 65 1 6 1 0 0 6 0 0 ## 66 1 6 1 0 0 6 0 0 ## 67 1 6 0 1 0 0 6 0 ## 68 1 6 0 1 0 0 6 0 ## 69 1 6 0 1 0 0 6 0 ## 70 1 6 0 0 1 0 0 6 ## 71 1 6 0 0 1 0 0 6 ## 72 1 6 0 0 1 0 0 6 ## 73 1 7 0 0 0 0 0 0 ## 74 1 7 0 0 0 0 0 0 ## 75 1 7 0 0 0 0 0 0 ## 76 1 7 1 0 0 7 0 0 ## 77 1 7 1 0 0 7 0 0 ## 78 1 7 1 0 0 7 0 0 ## 79 1 7 0 1 0 0 7 0 ## 80 1 7 0 1 0 0 7 0 ## 81 1 7 0 1 0 0 7 0 ## 82 1 7 0 0 1 0 0 7 ## 83 1 7 0 0 1 0 0 7 ## 84 1 7 0 0 1 0 0 7 ## 85 1 8 0 0 0 0 0 0 ## 86 1 8 0 0 0 0 0 0 ## 87 1 8 0 0 0 0 0 0 ## 88 1 8 1 0 0 8 0 0 ## 89 1 8 1 0 0 8 0 0 ## 90 1 8 1 0 0 8 0 0 ## 91 1 8 0 1 0 0 8 0 ## 92 1 8 0 1 0 0 8 0 ## 93 1 8 0 1 0 0 8 0 ## 94 1 8 0 0 1 0 0 8 ## 95 1 8 0 0 1 0 0 8 ## 96 1 8 0 0 1 0 0 8 ## 97 1 9 0 0 0 0 0 0 ## 98 1 9 0 0 0 0 0 0 ## 99 1 9 0 0 0 0 0 0 ## 100 1 9 1 0 0 9 0 0 ## 101 1 9 1 0 0 9 0 0 ## 102 1 9 1 0 0 9 0 0 ## 103 1 9 0 1 0 0 9 0 ## 104 1 9 0 1 0 0 9 0 ## 105 1 9 0 1 0 0 9 0 ## 106 1 9 0 0 1 0 0 9 ## 107 1 9 0 0 1 0 0 9 ## 108 1 9 0 0 1 0 0 9 ## 109 1 10 0 0 0 0 0 0 ## 110 1 10 0 0 0 0 0 0 ## 111 1 10 0 0 0 0 0 0 ## 112 1 10 1 0 0 10 0 0 ## 113 1 10 1 0 0 10 0 0 ## 114 1 10 1 0 0 10 0 0 ## 115 1 10 0 1 0 0 10 0 ## 116 1 10 0 1 0 0 10 0 ## 117 1 10 0 1 0 0 10 0 ## 118 1 10 0 0 1 0 0 10 ## 119 1 10 0 0 1 0 0 10 ## 120 1 10 0 0 1 0 0 10 model_matrix( sim3, y ~ x1 * x2) ## # A tibble: 120 x 8 ## `(Intercept)` x1 x2b x2c x2d `x1:x2b` `x1:x2c` `x1:x2d` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 0 0 0 ## 2 1 1 0 0 0 0 0 0 ## 3 1 1 0 0 0 0 0 0 ## 4 1 1 1 0 0 1 0 0 ## 5 1 1 1 0 0 1 0 0 ## 6 1 1 1 0 0 1 0 0 ## 7 1 1 0 1 0 0 1 0 ## 8 1 1 0 1 0 0 1 0 ## 9 1 1 0 1 0 0 1 0 ## 10 1 1 0 0 1 0 0 1 ## # … with 110 more rows 4. For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim? To test which model does a better job, we can look at the residuals for each model by subtracting the predicted values for each model from the true values. A measurement to compare the models would be to calculate the RMSE using these residuals, and choose the model with the lower RMSE. Doing so, we find that mod2 seems to be a better fit. The RMSE for mod2 is 2.0636 whereas the RMSE for mod1 is slightly higher (worse fit), at 2.0998. mod1 &lt;- lm(y ~ x1 + x2, data = sim4) mod2 &lt;- lm(y ~ x1 * x2, data = sim4) sim4 &lt;- sim4 %&gt;% spread_residuals (mod1, mod2) sim4 ## # A tibble: 300 x 6 ## x1 x2 rep y mod1 mod2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1 -1 1 4.25 3.25 2.30 ## 2 -1 -1 2 1.21 0.210 -0.743 ## 3 -1 -1 3 0.353 -0.643 -1.60 ## 4 -1 -0.778 1 -0.0467 -0.425 -1.17 ## 5 -1 -0.778 2 4.64 4.26 3.52 ## 6 -1 -0.778 3 1.38 0.999 0.258 ## 7 -1 -0.556 1 0.975 1.22 0.687 ## 8 -1 -0.556 2 2.50 2.74 2.21 ## 9 -1 -0.556 3 2.70 2.95 2.42 ## 10 -1 -0.333 1 0.558 1.42 1.10 ## # … with 290 more rows ggplot(sim4, aes(x1, mod1)) + geom_ref_line(h = 0) + geom_point() ggplot(sim4, aes(x1, mod2)) + geom_ref_line(h = 0) + geom_point() sim4 %&gt;% summarize ( mod1_rmse = sqrt(mean(mod1^2)), mod2_rmse = sqrt(mean(mod2^2))) ## # A tibble: 1 x 2 ## mod1_rmse mod2_rmse ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.10 2.06 "],
["chapter-24-model-building.html", "Chapter 24 - Model building 24.2.3 Exercises 24.3.5 Exercises", " Chapter 24 - Model building For this chapter, the following packages and modifications to datasets were used: options(na.action = na.warn) library(nycflights13) library(lubridate) diamonds2 &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% mutate(lprice = log2(price), lcarat = log2(carat)) 24.2.3 Exercises 1. In the plot of lcarat vs. lprice, there are some bright vertical strips. What do they represent? The color of each hexagon in geom_hex corresponds to the number of observations that lie within the hexagon, which means that the bright vertical strips represent highly concentrated areas containing large amounts of observations relative to the dim hexagons. Hadley alludes to the possible reason why these stripes exist in the previous chapters, where he mentions that humans are inclined to report “pretty” intervals such as 1.0, 1.5, 2.0, etc, resulting in more observations lying on these intervals. ggplot(diamonds2, aes(lcarat, lprice)) + geom_hex(bins = 50) 2. If log(price) = a_0 + a_1 * log(carat), what does that say about the relationship between price and carat? This equation suggests that there is a linear relationsihp between log(price) and log(carat), because the equation can be interpreted as a_0 being an intercept and a_1 being a “slope”. It is harder to discern the relationship between the non-logged price and carat given this equation. The relationship between the original non-transformed variables may not necessarily be linear. 3. Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors? Hadley extracts the diamonds with high / low residuals (abs(lresid2) &gt; 1) in the code below. After examining these, we find that most of the outlier diamonds that are priced much lower than predicted have a specific flaw associated with them. For example, $1262 predicted to be $2644 has a huge carat size but has a clarity of I1 (worst clarity), of which there are not that many observations for in the dataset (707 observations for grade I1 vs 13055 observations for grade SI1). The diamonds in this list usually have a combination of very good qualities as well as very bad qualities. It could be that there is some multicollinearity in the model, in which some of the predictor variables (lcarat, color, cut, and clarity) are correlated with one another. This may result in the model breaking down when something when the values of two variables which usually are correlated do not follow the trend for any specific observation. For example, the diamond priced at 2366 is predicted to only be 774, but this is likely due to the fact that the diamond has one of the best clarity values, but has the worst possible color. Looking at a density plot of color vs clarity, we find that there are very few observations which have this combination of color and clarity, which may be why the model breaks down. mod_diamond2 &lt;- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2) diamonds2 &lt;- diamonds2 %&gt;% add_residuals(mod_diamond2, &quot;lresid2&quot;) ggplot(diamonds2, aes(lcarat, lresid2)) + geom_hex(bins = 50) diamonds2 %&gt;% filter(abs(lresid2) &gt; 1) %&gt;% add_predictions(mod_diamond2) %&gt;% mutate(pred = round(2 ^ pred)) %&gt;% select(price, pred, carat:table, x:z, lresid2) %&gt;% arrange(price) ## # A tibble: 16 x 12 ## price pred carat cut color clarity depth table x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1013 264 0.25 Fair F SI2 54.4 64 4.3 4.23 2.32 ## 2 1186 284 0.25 Prem… G SI2 59 60 5.33 5.28 3.12 ## 3 1186 284 0.25 Prem… G SI2 58.8 60 5.33 5.28 3.12 ## 4 1262 2644 1.03 Fair E I1 78.2 54 5.72 5.59 4.42 ## 5 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 ## 6 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 ## 7 1715 576 0.32 Fair F VS2 59.6 60 4.42 4.34 2.61 ## 8 1776 412 0.290 Fair F SI1 55.8 60 4.48 4.41 2.48 ## 9 2160 314 0.34 Fair F I1 55.8 62 4.72 4.6 2.6 ## 10 2366 774 0.3 Very… D VVS2 60.6 58 4.33 4.35 2.63 ## 11 3360 1373 0.51 Prem… F SI1 62.7 62 5.09 4.96 3.15 ## 12 3807 1540 0.61 Good F SI2 62.5 65 5.36 5.29 3.33 ## 13 3920 1705 0.51 Fair F VVS2 65.4 60 4.98 4.9 3.23 ## 14 4368 1705 0.51 Fair F VVS2 60.7 66 5.21 5.11 3.13 ## 15 10011 4048 1.01 Fair D SI2 64.6 58 6.25 6.2 4.02 ## 16 10470 23622 2.46 Prem… E SI2 59.7 59 8.82 8.76 5.25 ## # … with 1 more variable: lresid2 &lt;dbl&gt; diamonds2 %&gt;% group_by(clarity) %&gt;% summarize (num = n()) ## # A tibble: 8 x 2 ## clarity num ## &lt;ord&gt; &lt;int&gt; ## 1 I1 707 ## 2 SI2 9116 ## 3 SI1 13055 ## 4 VS2 12256 ## 5 VS1 8169 ## 6 VVS2 5066 ## 7 VVS1 3655 ## 8 IF 1790 diamonds2 %&gt;% ggplot(aes(color, clarity))+ geom_bin2d() 4. Does the final model, mod_diamond2, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond? Judging by the plot of the residuals, the model does a decent job at removing the patterns in the data (fairly flat, only a handful of residuals &gt; 1 st. deviation away from 0) for the log-transformed version of the data. The model could be improved to reduce the variance of the residuals (compress the points toward h=0), in order to get more accurate predictions. However, since we aren’t dealing with log-transformed money when buying diamonds in the real world, we should examine how the residuals look when transformed back into their true values. To do so, we calculate subtract the non-logged prediction value from the true value to get the non-transformed residual. Plotting these non-transformed residuals shows that the variability in residuals increases as carat increases (same is true for lcarat). I would have moderate faith in the model for diamonds less than 1 carat, but for diamonds greater than one carat, I would be cautious. The model would be useful to determine whether you were being completely scammed, but it may not be that good for determining small variations in price. nontransformed_residual &lt;- diamonds2 %&gt;% filter(abs(lresid2) &lt; 1) %&gt;% add_predictions(mod_diamond2) %&gt;% mutate(pred = round(2 ^ pred)) %&gt;% mutate(resid = price - pred) nontransformed_residual %&gt;% ggplot( aes (carat, resid) )+ geom_hex() nontransformed_residual %&gt;% summarize (resid_mean = mean(resid), resid_sd = sd(resid), resid_interval_low = mean(resid) - 1.96* sd(resid), resid_interval_high = mean(resid) + 1.96* sd(resid), limit_upper = max(resid), limit_lower = min(resid), maxprice = max(price), minprice = min(price)) ## # A tibble: 1 x 8 ## resid_mean resid_sd resid_interval_… resid_interval_… limit_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 49.0 731. -1385. 1483. 8994 ## # … with 3 more variables: limit_lower &lt;dbl&gt;, maxprice &lt;dbl&gt;, ## # minprice &lt;dbl&gt; 24.3.5 Exercises daily &lt;- flights %&gt;% mutate(date = make_date(year, month, day)) %&gt;% group_by(date) %&gt;% summarise(n = n()) daily ## # A tibble: 365 x 2 ## date n ## &lt;date&gt; &lt;int&gt; ## 1 2013-01-01 842 ## 2 2013-01-02 943 ## 3 2013-01-03 914 ## 4 2013-01-04 915 ## 5 2013-01-05 720 ## 6 2013-01-06 832 ## 7 2013-01-07 933 ## 8 2013-01-08 899 ## 9 2013-01-09 902 ## 10 2013-01-10 932 ## # … with 355 more rows ggplot(daily, aes(date, n)) + geom_line() daily &lt;- daily %&gt;% mutate(wday = wday(date, label = TRUE)) mod &lt;- lm(n ~ wday, data = daily) daily &lt;- daily %&gt;% add_residuals(mod) daily %&gt;% ggplot(aes(date, resid)) + geom_ref_line(h = 0) + geom_line() term &lt;- function(date) { cut(date, breaks = ymd(20130101, 20130605, 20130825, 20140101), labels = c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;) ) } daily &lt;- daily %&gt;% mutate(term = term(date)) 1. Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalise to another year? These days are all the day before major US holidays (Sep 2, 2013 is Labor Day) which fall exclusively on Mondays. There may be fewer than expected flights due to the extended weekend / other holiday-associated factors. For another year, these days can be found by looking for the day before the third Monday in January, fourth Monday in May, and first Monday in September. 2. What do the three days with high positive residuals represent? How would these days generalise to another year? These days look like are also associated with major US holidays, with 11/30/2013 and 12/01/2013 being the weekend after Thanksgiving, and 12/28/2013 being the weekend after Christmas. The reason flights peak on these days may be due to families who had visited their relatives leaving to go back home. These can be generalized to another year by looking for the weekends after these holidays, which typically fall after the 4th week of November and December. daily %&gt;% top_n(3, resid) ## # A tibble: 3 x 5 ## date n wday resid term ## &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 2013-11-30 857 Sat 112. fall ## 2 2013-12-01 987 Sun 95.5 fall ## 3 2013-12-28 814 Sat 69.4 fall 3. Create a new variable that splits the wday variable into terms, but only for Saturdays, i.e. it should have Thurs, Fri, but Sat-summer, Sat-spring, Sat-fall. How does this model compare with the model with every combination of wday and term? In order to split the saturdays into terms, I wrote an annotate_sat() function that takes the wday column and term column from daily and applies the appropriate suffix to each “Sat”. Fitting this model (mod3) and comparing it to the mod1 and mod2 described in the chapter shows that there is a slight improvement from mod 1 (no term variable), but that the model does slightly worse than mod2 (each day is termed). The RMSE is midway between mod1 and mod2. annotate_sat &lt;- function (wday, term) { index &lt;- which (wday == &quot;Sat&quot;) wday &lt;- as.character(wday) wday[index] &lt;- str_c(&quot;Sat-&quot;, as.character(term)[index]) wday } daily &lt;- daily %&gt;% mutate (wday_sat = annotate_sat(wday,term)) daily ## # A tibble: 365 x 6 ## date n wday resid term wday_sat ## &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; ## 1 2013-01-01 842 Tue -109. spring Tue ## 2 2013-01-02 943 Wed -19.7 spring Wed ## 3 2013-01-03 914 Thu -51.8 spring Thu ## 4 2013-01-04 915 Fri -52.5 spring Fri ## 5 2013-01-05 720 Sat -24.6 spring Sat-spring ## 6 2013-01-06 832 Sun -59.5 spring Sun ## 7 2013-01-07 933 Mon -41.8 spring Mon ## 8 2013-01-08 899 Tue -52.4 spring Tue ## 9 2013-01-09 902 Wed -60.7 spring Wed ## 10 2013-01-10 932 Thu -33.8 spring Thu ## # … with 355 more rows mod1 &lt;- lm(n ~ wday, data = daily) mod2 &lt;- lm(n ~ wday * term, data = daily) mod3 &lt;- lm(n ~ wday_sat, data = daily) daily %&gt;% gather_residuals(no_term = mod1, all_cominbations = mod2, only_sat_term = mod3) %&gt;% ggplot(aes(date, resid, colour = model)) + geom_line(alpha = 0.75) sigma(mod1) ## [1] 48.79656 sigma(mod2) ## [1] 46.16568 sigma(mod3) ## [1] 47.35969 4. Create a new wday variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like? The code below adds a column to the data frame that indicates which days are holidays (I’ve chosen the main US corporate holidays for 2013). Fitting a model to this results in an rmse value that is lower than the model in which only the saturdays are termed (goes down from 47.36 to 42.94), suggesting that we are doing at least a slightly better job. Looking at the graph, the residuals that spike along the holidays are smaller (but still quite visible). There could be more ways to optimize the model to minimize these residuals, suchs as annotating wday with the few days before/after each holiday. daily ## # A tibble: 365 x 6 ## date n wday resid term wday_sat ## &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; ## 1 2013-01-01 842 Tue -109. spring Tue ## 2 2013-01-02 943 Wed -19.7 spring Wed ## 3 2013-01-03 914 Thu -51.8 spring Thu ## 4 2013-01-04 915 Fri -52.5 spring Fri ## 5 2013-01-05 720 Sat -24.6 spring Sat-spring ## 6 2013-01-06 832 Sun -59.5 spring Sun ## 7 2013-01-07 933 Mon -41.8 spring Mon ## 8 2013-01-08 899 Tue -52.4 spring Tue ## 9 2013-01-09 902 Wed -60.7 spring Wed ## 10 2013-01-10 932 Thu -33.8 spring Thu ## # … with 355 more rows annotate_sat_holiday &lt;- function (date, wday, term) { index &lt;- which (wday == &quot;Sat&quot;) wday &lt;- as.character(wday) wday[index] &lt;- str_c(&quot;Sat-&quot;, as.character(term)[index]) holidays &lt;- ymd(20130101, 20130527, 20130704, 20130902, 20131111, 20131128, 20131225) holday_index &lt;- which (date %in% holidays) wday[which (date %in% holidays)] &lt;- &quot;holiday&quot; wday } daily &lt;- daily %&gt;% mutate( wday_sat_holiday = annotate_sat_holiday(date,wday,term)) daily ## # A tibble: 365 x 7 ## date n wday resid term wday_sat wday_sat_holiday ## &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013-01-01 842 Tue -109. spring Tue holiday ## 2 2013-01-02 943 Wed -19.7 spring Wed Wed ## 3 2013-01-03 914 Thu -51.8 spring Thu Thu ## 4 2013-01-04 915 Fri -52.5 spring Fri Fri ## 5 2013-01-05 720 Sat -24.6 spring Sat-spring Sat-spring ## 6 2013-01-06 832 Sun -59.5 spring Sun Sun ## 7 2013-01-07 933 Mon -41.8 spring Mon Mon ## 8 2013-01-08 899 Tue -52.4 spring Tue Tue ## 9 2013-01-09 902 Wed -60.7 spring Wed Wed ## 10 2013-01-10 932 Thu -33.8 spring Thu Thu ## # … with 355 more rows mod4 &lt;- lm(n ~ wday_sat_holiday, data = daily) daily %&gt;% gather_residuals(only_sat_term = mod3, sat_with_holiday = mod4) %&gt;% ggplot(aes(date, resid, colour = model)) + geom_line(alpha = 0.75) sigma(mod3) ## [1] 47.35969 sigma(mod4) ## [1] 42.94494 5. What happens if you fit a day of week effect that varies by month (i.e. n ~ wday * month)? Why is this not very helpful? Adding the month variable to the model reduces the values of the residuals by a small amount (and subsequently, the rmse). I would say that it is slightly helpful to include. I can see why it is not extremely helpful, because it may be over-fitting the data, as there is not necessarily a good reason to assume that each individual month is associated with their own unique day-of-the-week trends. This adds a large amount of predictor variables to the model that are not all necessarily independent from each other. # add month column to daily daily &lt;- daily %&gt;% mutate(month = factor(month(date))) daily ## # A tibble: 365 x 8 ## date n wday resid term wday_sat wday_sat_holiday month ## &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 2013-01-01 842 Tue -109. spring Tue holiday 1 ## 2 2013-01-02 943 Wed -19.7 spring Wed Wed 1 ## 3 2013-01-03 914 Thu -51.8 spring Thu Thu 1 ## 4 2013-01-04 915 Fri -52.5 spring Fri Fri 1 ## 5 2013-01-05 720 Sat -24.6 spring Sat-spring Sat-spring 1 ## 6 2013-01-06 832 Sun -59.5 spring Sun Sun 1 ## 7 2013-01-07 933 Mon -41.8 spring Mon Mon 1 ## 8 2013-01-08 899 Tue -52.4 spring Tue Tue 1 ## 9 2013-01-09 902 Wed -60.7 spring Wed Wed 1 ## 10 2013-01-10 932 Thu -33.8 spring Thu Thu 1 ## # … with 355 more rows mod5 &lt;- lm(n ~ wday*month, data = daily) daily %&gt;% gather_residuals(original_model = mod1, with_month = mod5) %&gt;% ggplot(aes(date, resid, colour = model)) + geom_line(alpha = 0.75) #summary(mod5) sigma(mod1) ## [1] 48.79656 sigma(mod5) ## [1] 42.0489 6. What would you expect the model n ~ wday + ns(date, 5) to look like? Knowing what you know about the data, why would you expect it to be not particularly effective? The book displays what the model n ~ wday * ns(date, 5) looks like, which factors in the relationship between wday and the time of year into the model. The model n ~ wday + ns(date, 5) uses the “+” operator instead of “*”, which means that this model does not account for the relationship between these two variables. I would expect that n ~ wday + ns(date, 5) does a worse job as a predictive model. Testing this out below, we see that the residuals are indeed larger. The number of predictor variables are also much fewer. library(splines) mod6 &lt;- MASS::rlm(n ~ wday * ns(date, 5), data = daily) mod7 &lt;- MASS::rlm(n ~ wday + ns(date, 5), data = daily) summary(mod6) ## ## Call: rlm(formula = n ~ wday * ns(date, 5), data = daily) ## Residuals: ## Min 1Q Median 3Q Max ## -339.23 -8.00 1.33 6.94 119.10 ## ## Coefficients: ## Value Std. Error t value ## (Intercept) 839.1368 3.2049 261.8314 ## wday.L -58.0966 8.7636 -6.6293 ## wday.Q -177.5614 8.4931 -20.9065 ## wday.C -76.1745 8.5575 -8.9015 ## wday^4 -106.5457 8.6304 -12.3454 ## wday^5 14.7852 8.3463 1.7715 ## wday^6 -18.4661 8.0673 -2.2890 ## ns(date, 5)1 90.3704 4.0032 22.5743 ## ns(date, 5)2 133.7194 5.1333 26.0493 ## ns(date, 5)3 41.3817 3.9584 10.4542 ## ns(date, 5)4 180.6645 8.1033 22.2952 ## ns(date, 5)5 28.9177 3.5076 8.2443 ## wday.L:ns(date, 5)1 -35.0805 10.6922 -3.2809 ## wday.Q:ns(date, 5)1 11.6486 10.5964 1.0993 ## wday.C:ns(date, 5)1 -6.8751 10.6179 -0.6475 ## wday^4:ns(date, 5)1 23.7107 10.6477 2.2268 ## wday^5:ns(date, 5)1 -21.0685 10.5431 -1.9983 ## wday^6:ns(date, 5)1 6.6103 10.4507 0.6325 ## wday.L:ns(date, 5)2 19.8440 13.8192 1.4360 ## wday.Q:ns(date, 5)2 66.9609 13.5972 4.9246 ## wday.C:ns(date, 5)2 54.5911 13.6393 4.0025 ## wday^4:ns(date, 5)2 59.4655 13.7181 4.3348 ## wday^5:ns(date, 5)2 -21.2690 13.4571 -1.5805 ## wday^6:ns(date, 5)2 8.7439 13.2506 0.6599 ## wday.L:ns(date, 5)3 -104.5352 10.4834 -9.9715 ## wday.Q:ns(date, 5)3 -83.6926 10.4858 -7.9815 ## wday.C:ns(date, 5)3 -77.0187 10.4500 -7.3702 ## wday^4:ns(date, 5)3 -36.3447 10.5211 -3.4545 ## wday^5:ns(date, 5)3 -25.0391 10.4165 -2.4038 ## wday^6:ns(date, 5)3 0.8459 10.4801 0.0807 ## wday.L:ns(date, 5)4 -29.5596 22.0451 -1.3409 ## wday.Q:ns(date, 5)4 58.6436 21.4762 2.7306 ## wday.C:ns(date, 5)4 32.5922 21.5929 1.5094 ## wday^4:ns(date, 5)4 61.0040 21.7796 2.8010 ## wday^5:ns(date, 5)4 -41.3648 21.1311 -1.9575 ## wday^6:ns(date, 5)4 13.1682 20.5795 0.6399 ## wday.L:ns(date, 5)5 39.3654 9.1661 4.2947 ## wday.Q:ns(date, 5)5 59.7652 9.3101 6.4194 ## wday.C:ns(date, 5)5 39.0233 9.1742 4.2536 ## wday^4:ns(date, 5)5 22.0797 9.3456 2.3626 ## wday^5:ns(date, 5)5 2.1498 9.1824 0.2341 ## wday^6:ns(date, 5)5 -4.9030 9.4987 -0.5162 ## ## Residual standard error: 10.69 on 323 degrees of freedom summary(mod7) ## ## Call: rlm(formula = n ~ wday + ns(date, 5), data = daily) ## Residuals: ## Min 1Q Median 3Q Max ## -339.2832 -8.1235 0.1509 9.3763 111.5557 ## ## Coefficients: ## Value Std. Error t value ## (Intercept) 843.7636 3.5622 236.8666 ## wday.L -80.1706 2.1137 -37.9291 ## wday.Q -156.9939 2.1125 -74.3180 ## wday.C -68.1163 2.1111 -32.2664 ## wday^4 -78.8676 2.1143 -37.3026 ## wday^5 -6.3087 2.1084 -2.9922 ## wday^6 -11.5275 2.1095 -5.4646 ## ns(date, 5)1 87.4028 4.4681 19.5615 ## ns(date, 5)2 121.4169 5.7198 21.2276 ## ns(date, 5)3 53.7103 4.4159 12.1630 ## ns(date, 5)4 169.0229 9.0113 18.7567 ## ns(date, 5)5 18.4859 3.9036 4.7356 ## ## Residual standard error: 13.24 on 353 degrees of freedom daily %&gt;% gather_residuals(multiplicative_splines = mod6, additive_splines = mod7) %&gt;% ggplot(aes(date, resid, colour = model)) + geom_line(alpha = 0.75) sigma(mod6) ## [1] 43.92417 sigma(mod7) ## [1] 44.30197 7. We hypothesised that people leaving on Sundays are more likely to be business travellers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away. To explore this hypothesis, we first generate a new data frame from flights that contains a factor splitting up each day into early morning (12am-8am, 8am-12pm, 12pm-5pm, and 5pm-12am). We can then summarize the data to obtain the mean distance of flights that occurred during each of those intervals grouped by day. Plotting this using boxplots, we observe that flights taking off between 1am and 8am on Sundays have a much higher mean distance compared to all other categories. Contrary to our hypothesis that evening flights (5pm-12am) would be the longest distance-wise, we instead see that there are more Sunday early morning flights between 12am-8am. # add distance and time of flight to daily matrix timeofday &lt;- function(date) { cut(date, breaks = c(0,7, 12, 17, 23), labels = c(&quot;12am-8am&quot;, &quot;8am-12pm&quot;, &quot;12pm-5pm&quot;,&quot;5pm-12am&quot;) ) } flights2 &lt;- flights %&gt;% mutate (date = make_date (year, month, day), timeofday = timeofday(hour)) %&gt;% group_by(date, timeofday) %&gt;% summarize (n = n(), mean_dist = mean(distance)) %&gt;% mutate(wday = wday(date, label = TRUE)) flights2 ## # A tibble: 1,460 x 5 ## # Groups: date [365] ## date timeofday n mean_dist wday ## &lt;date&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;ord&gt; ## 1 2013-01-01 12am-8am 107 1234. Tue ## 2 2013-01-01 8am-12pm 246 1074. Tue ## 3 2013-01-01 12pm-5pm 301 1040. Tue ## 4 2013-01-01 5pm-12am 188 1051. Tue ## 5 2013-01-02 12am-8am 146 1132. Wed ## 6 2013-01-02 8am-12pm 274 1045. Wed ## 7 2013-01-02 12pm-5pm 318 1023. Wed ## 8 2013-01-02 5pm-12am 205 1054. Wed ## 9 2013-01-03 12am-8am 144 1118. Thu ## 10 2013-01-03 8am-12pm 262 1037. Thu ## # … with 1,450 more rows ggplot(flights2, aes (x = wday, y = mean_dist))+ geom_boxplot(aes(color = timeofday)) # # flights %&gt;% mutate (date = make_date (year, month, day), timeofday = timeofday(hour)) %&gt;% # mutate(wday = wday(date, label = TRUE))%&gt;% # ggplot( aes (x = wday, y = distance))+ # geom_boxplot(aes(color = timeofday)) 8. It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday. The levels of a factor can be re-ordered using the factor() function and redefining the order of the levels by manually passing them into the levels argument. Below is an example of a function reordering the factors so that the week starts on monday. # before reordering the factors ggplot(daily, aes(wday, n)) + geom_boxplot() reorder_week &lt;- function (mydata) { mydata &lt;- factor(mydata, levels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)) mydata } # after reordering the factors daily %&gt;% mutate (wday2 = reorder_week(wday)) %&gt;% ggplot( aes(wday2, n)) + geom_boxplot() "],
["chapter-25-many-models.html", "Chapter 25 - Many models 25.2.5 Exercises 25.4.5 Exercises 25.5.3 Exercises", " Chapter 25 - Many models First, we’ll load and prepare the data used for the exercises in this chapter. library(gapminder) by_country &lt;- gapminder %&gt;% group_by(country, continent) %&gt;% nest() by_country ## # A tibble: 142 x 3 ## country continent data ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; ## 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; ## 2 Albania Europe &lt;tibble [12 × 4]&gt; ## 3 Algeria Africa &lt;tibble [12 × 4]&gt; ## 4 Angola Africa &lt;tibble [12 × 4]&gt; ## 5 Argentina Americas &lt;tibble [12 × 4]&gt; ## 6 Australia Oceania &lt;tibble [12 × 4]&gt; ## 7 Austria Europe &lt;tibble [12 × 4]&gt; ## 8 Bahrain Asia &lt;tibble [12 × 4]&gt; ## 9 Bangladesh Asia &lt;tibble [12 × 4]&gt; ## 10 Belgium Europe &lt;tibble [12 × 4]&gt; ## # … with 132 more rows ## wrapper function to model lifeExp by year country_model &lt;- function(df) { lm(lifeExp ~ year, data = df) } ## add the model as a list-column to the nested data by_country &lt;- by_country %&gt;% mutate(model = map(data, country_model)) ## Error in mutate_impl(.data, dots): Evaluation error: cannot coerce type &#39;closure&#39; to vector of type &#39;character&#39;. by_country ## # A tibble: 142 x 3 ## country continent data ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; ## 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; ## 2 Albania Europe &lt;tibble [12 × 4]&gt; ## 3 Algeria Africa &lt;tibble [12 × 4]&gt; ## 4 Angola Africa &lt;tibble [12 × 4]&gt; ## 5 Argentina Americas &lt;tibble [12 × 4]&gt; ## 6 Australia Oceania &lt;tibble [12 × 4]&gt; ## 7 Austria Europe &lt;tibble [12 × 4]&gt; ## 8 Bahrain Asia &lt;tibble [12 × 4]&gt; ## 9 Bangladesh Asia &lt;tibble [12 × 4]&gt; ## 10 Belgium Europe &lt;tibble [12 × 4]&gt; ## # … with 132 more rows ## calculate residuals based on the nested data and the associated model by_country &lt;- by_country %&gt;% mutate( resids = map2(data, model, add_residuals) ) ## Error in mutate_impl(.data, dots): Evaluation error: object &#39;model&#39; not found. by_country ## # A tibble: 142 x 3 ## country continent data ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; ## 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; ## 2 Albania Europe &lt;tibble [12 × 4]&gt; ## 3 Algeria Africa &lt;tibble [12 × 4]&gt; ## 4 Angola Africa &lt;tibble [12 × 4]&gt; ## 5 Argentina Americas &lt;tibble [12 × 4]&gt; ## 6 Australia Oceania &lt;tibble [12 × 4]&gt; ## 7 Austria Europe &lt;tibble [12 × 4]&gt; ## 8 Bahrain Asia &lt;tibble [12 × 4]&gt; ## 9 Bangladesh Asia &lt;tibble [12 × 4]&gt; ## 10 Belgium Europe &lt;tibble [12 × 4]&gt; ## # … with 132 more rows resids &lt;- unnest(by_country, resids) ## Error in mutate_impl(.data, dots): Binding not found: resids. resids %&gt;% group_by(country) %&gt;% summarise (rsme = sqrt(mean(resid^2))) ## Error in eval(lhs, parent, parent): object &#39;resids&#39; not found 25.2.5 Exercises 1. A linear trend seems to be slightly too simple for the overall trend. Can you do better with a quadratic polynomial? How can you interpret the coefficients of the quadratic? (Hint you might want to transform year so that it has mean zero.) We can fit a quadratic polynomial either by using the equation y ~ poly(x,2) or writing it out as y ~ I(x^2) + x. When we fit this model to the data, we see that the majority of the rmse values calculated using the residuals by country are reduced compared to the original linear model used in this chapter. This suggests that we are doing better. country_model_poly &lt;- function(df) { lm(lifeExp ~ poly(year,2), data = df) # alternatively can use lm(lifeExp ~ I(year^2)+year, data = df) for raw polynomial } country_model_poly_centered &lt;- function(df) { lm(lifeExp ~ poly(year-mean(year),2), data = df) } by_country &lt;- by_country %&gt;% mutate(model2 = map(data, country_model_poly), model3 = map(data, country_model_poly_centered)) %&gt;% mutate(resids2 = map2(data, model2, add_residuals)) ## Error in mutate_impl(.data, dots): Evaluation error: cannot coerce type &#39;closure&#39; to vector of type &#39;character&#39;. ## residual freq-poly plot resids2 &lt;- unnest(by_country, resids2) ## Error in mutate_impl(.data, dots): Binding not found: resids2. resids2 %&gt;% ggplot(aes(year, resid)) + geom_line(aes(group = country), alpha = 1 / 3) + geom_smooth(se = FALSE) ## Error in eval(lhs, parent, parent): object &#39;resids2&#39; not found ## R-squared value glance &lt;- by_country %&gt;% mutate(glance = map(model2, broom::glance)) %&gt;% unnest(glance, .drop = TRUE) ## Error in mutate_impl(.data, dots): Evaluation error: object &#39;model2&#39; not found. glance %&gt;% ggplot(aes(continent, r.squared)) + geom_jitter(width = 0.5) ## Error in eval(lhs, parent, parent): object &#39;glance&#39; not found # rmse using the polynomial model resids2 %&gt;% group_by(country) %&gt;% summarise (rmse = sqrt(mean(resid^2))) ## Error in eval(lhs, parent, parent): object &#39;resids2&#39; not found # rmse using the original linear model resids %&gt;% group_by(country) %&gt;% summarise (rmse = sqrt(mean(resid^2))) ## Error in eval(lhs, parent, parent): object &#39;resids&#39; not found When we compare the model summaries for the centered (mean 0) model vs the non-centered model, the estimates for the coefficients are the same. Poly() creates orthogonal polynomials for the fit. The coefficients may be interpreted as the weight associated with a unit of change in year. summary(by_country$model2[[1]]) ## Warning: Unknown or uninitialised column: &#39;model2&#39;. ## Length Class Mode ## 0 NULL NULL summary(by_country$model3[[1]]) ## Warning: Unknown or uninitialised column: &#39;model3&#39;. ## Length Class Mode ## 0 NULL NULL 2. Explore other methods for visualising the distribution of R2 per continent. You might want to try the ggbeeswarm package, which provides similar methods for avoiding overlaps as jitter, but uses deterministic methods. To use the ggbeeswarm package, simply replace geom_jitter() with geom_beeswarm(). The output is below. We can augment the graph by adding additional graphics such as a boxplot. library(ggbeeswarm) glance %&gt;% ggplot(aes(continent, r.squared)) + geom_beeswarm() ## Error in eval(lhs, parent, parent): object &#39;glance&#39; not found glance %&gt;% ggplot(aes(continent, r.squared)) + geom_boxplot( aes(color = continent))+ geom_beeswarm() ## Error in eval(lhs, parent, parent): object &#39;glance&#39; not found 3. To create the last plot (showing the data for the countries with the worst model fits), we needed two steps: we created a data frame with one row per country and then semi-joined it to the original dataset. It’s possible to avoid this join if we use unnest() instead of unnest(.drop = TRUE). How? Instead of using unnest(glance,.drop = TRUE), using unnest(glance) will retain the other columns in the dataset in addition to unnesting the glance column. This means that the list-column “data” will still be associated with the glance output, which contains the year and lifeExp variables that are needed to plot the final graph. We can then perform the filtering on the r.squared value, unnest the data, and then plot the graph, as shown below, without needing to semi-join. glance &lt;- by_country %&gt;% mutate(glance = map(model, broom::glance)) %&gt;% unnest(glance) ## Error in mutate_impl(.data, dots): Evaluation error: object &#39;model&#39; not found. glance ## Error in eval(expr, envir, enclos): object &#39;glance&#39; not found bad_fit &lt;- filter(glance, r.squared &lt; 0.25) ## Error in filter(glance, r.squared &lt; 0.25): object &#39;glance&#39; not found bad_fit ## Error in eval(expr, envir, enclos): object &#39;bad_fit&#39; not found bad_fit %&gt;% unnest (data) %&gt;% ggplot(aes(year, lifeExp, colour = country)) + geom_line() ## Error in eval(lhs, parent, parent): object &#39;bad_fit&#39; not found 25.4.5 Exercises 1. List all the functions that you can think of that take a atomic vector and return a list. enframe() or as_tibble() will convert an atomic vector into a list. The map() function will return a list in which it applies a function that you specify on each element of the atomic vector (examples below). stringr functions return lists of strings, such as str_split(). You may also encounter package-specific functions which may create their own types of objects. For example, in the bioinformatics world, you may have DESeq2 objects (bulk RNA-sequencing) or Seurat objects (single cell RNA-sequencing), which are S4 objects which contain several data types, including lists. Other functions that return lists are those shown in this chapter, such as broom::glance(), although in this instance the input is a model and not an atomic vector. split() also returns a list when applied to a dataframe. my_atomic_vector &lt;- c(&quot;hello&quot;,&quot;world&quot;) map(my_atomic_vector, length) ## Error: &#39;helloMapEnv&#39; is not an exported object from &#39;namespace:maps&#39; str_split(my_atomic_vector, &quot;l&quot;) ## [[1]] ## [1] &quot;he&quot; &quot;&quot; &quot;o&quot; ## ## [[2]] ## [1] &quot;wor&quot; &quot;d&quot; typeof(as_tibble(my_atomic_vector)) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `enframe(name = NULL)` instead. ## This warning is displayed once per session. ## [1] &quot;list&quot; typeof(enframe(my_atomic_vector)) ## [1] &quot;list&quot; summary(my_atomic_vector) ## Length Class Mode ## 2 character character mtcars %&gt;% split(.$cyl) ## $`4` ## mpg cyl disp hp drat wt qsec vs am gear carb ## Datsun 710 22.8 4 1.769807 93 3.85 2.320 18.61 1 manual 4 1 ## Merc 240D 24.4 4 2.403988 62 3.69 3.190 20.00 1 auto 4 2 ## Merc 230 22.8 4 2.307304 95 3.92 3.150 22.90 1 auto 4 2 ## Fiat 128 32.4 4 1.289665 66 4.08 2.200 19.47 1 manual 4 1 ## Honda Civic 30.4 4 1.240503 52 4.93 1.615 18.52 1 manual 4 2 ## Toyota Corolla 33.9 4 1.165123 65 4.22 1.835 19.90 1 manual 4 1 ## Toyota Corona 21.5 4 1.968091 97 3.70 2.465 20.01 1 auto 3 1 ## Fiat X1-9 27.3 4 1.294581 66 4.08 1.935 18.90 1 manual 4 1 ## Porsche 914-2 26.0 4 1.971368 91 4.43 2.140 16.70 0 manual 5 2 ## Lotus Europa 30.4 4 1.558413 113 3.77 1.513 16.90 1 manual 5 2 ## Volvo 142E 21.4 4 1.982839 109 4.11 2.780 18.60 1 manual 4 2 ## ## $`6` ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 2.621936 110 3.90 2.620 16.46 0 manual 4 4 ## Mazda RX4 Wag 21.0 6 2.621936 110 3.90 2.875 17.02 0 manual 4 4 ## Hornet 4 Drive 21.4 6 4.227872 110 3.08 3.215 19.44 1 auto 3 1 ## Valiant 18.1 6 3.687098 105 2.76 3.460 20.22 1 auto 3 1 ## Merc 280 19.2 6 2.746478 123 3.92 3.440 18.30 1 auto 4 4 ## Merc 280C 17.8 6 2.746478 123 3.92 3.440 18.90 1 auto 4 4 ## Ferrari Dino 19.7 6 2.376130 175 3.62 2.770 15.50 0 manual 5 6 ## ## $`8` ## mpg cyl disp hp drat wt qsec vs am gear ## Hornet Sportabout 18.7 8 5.899356 175 3.15 3.440 17.02 0 auto 3 ## Duster 360 14.3 8 5.899356 245 3.21 3.570 15.84 0 auto 3 ## Merc 450SE 16.4 8 4.519562 180 3.07 4.070 17.40 0 auto 3 ## Merc 450SL 17.3 8 4.519562 180 3.07 3.730 17.60 0 auto 3 ## Merc 450SLC 15.2 8 4.519562 180 3.07 3.780 18.00 0 auto 3 ## Cadillac Fleetwood 10.4 8 7.734711 205 2.93 5.250 17.98 0 auto 3 ## Lincoln Continental 10.4 8 7.538066 215 3.00 5.424 17.82 0 auto 3 ## Chrysler Imperial 14.7 8 7.210324 230 3.23 5.345 17.42 0 auto 3 ## Dodge Challenger 15.5 8 5.211098 150 2.76 3.520 16.87 0 auto 3 ## AMC Javelin 15.2 8 4.981678 150 3.15 3.435 17.30 0 auto 3 ## Camaro Z28 13.3 8 5.735485 245 3.73 3.840 15.41 0 auto 3 ## Pontiac Firebird 19.2 8 6.554840 175 3.08 3.845 17.05 0 auto 3 ## Ford Pantera L 15.8 8 5.751872 264 4.22 3.170 14.50 0 manual 5 ## Maserati Bora 15.0 8 4.932517 335 3.54 3.570 14.60 0 manual 5 ## carb ## Hornet Sportabout 2 ## Duster 360 4 ## Merc 450SE 3 ## Merc 450SL 3 ## Merc 450SLC 3 ## Cadillac Fleetwood 4 ## Lincoln Continental 4 ## Chrysler Imperial 4 ## Dodge Challenger 2 ## AMC Javelin 2 ## Camaro Z28 4 ## Pontiac Firebird 2 ## Ford Pantera L 4 ## Maserati Bora 8 2. Brainstorm useful summary functions that, like quantile(), return multiple values. Summary functions in addition to quantile() include: summary(), range(), dim(), and coef() for linear models. x &lt;- 1:10 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 3.25 5.50 5.50 7.75 10.00 range(x) ## [1] 1 10 dim(mtcars) ## [1] 32 11 coef(lm(mpg ~cyl, data = mtcars)) ## (Intercept) cyl ## 37.88458 -2.87579 3. What’s missing in the following data frame? How does quantile() return that missing piece? Why isn’t that helpful here? The data frame shows the output of unnesting the column q, which contains the quantile() values for each cylinder type in mtcars stored as a list-column. The important information that is missing is the label corresponding to each of the quantile values. For example, 21.4 (the first entry) corresponds to the 0% quantile. Without knowing how the function works, someone looking at the table would not know this important information. quantile() returns this information by naming the output vector with these labels. This is not helpful when creating list-columns, because the names are not stored automatically. # what quantile should return quantile(mtcars[which(mtcars$cyl==4),&quot;mpg&quot;]) ## 0% 25% 50% 75% 100% ## 21.4 22.8 26.0 30.4 33.9 names(quantile(mtcars[which(mtcars$cyl==4),&quot;mpg&quot;])) ## [1] &quot;0%&quot; &quot;25%&quot; &quot;50%&quot; &quot;75%&quot; &quot;100%&quot; # quantile is missing the labels for each of the statistics (0%, 25%, ...) mtcars %&gt;% group_by(cyl) %&gt;% summarise(q = list(quantile(mpg))) %&gt;% unnest() ## # A tibble: 15 x 2 ## cyl q ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 21.4 ## 2 4 22.8 ## 3 4 26 ## 4 4 30.4 ## 5 4 33.9 ## 6 6 17.8 ## 7 6 18.6 ## 8 6 19.7 ## 9 6 21 ## 10 6 21.4 ## 11 8 10.4 ## 12 8 14.4 ## 13 8 15.2 ## 14 8 16.2 ## 15 8 19.2 mtcars %&gt;% group_by(cyl) %&gt;% summarise(q = list(quantile(mpg))) %&gt;% unnest() ## # A tibble: 15 x 2 ## cyl q ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 21.4 ## 2 4 22.8 ## 3 4 26 ## 4 4 30.4 ## 5 4 33.9 ## 6 6 17.8 ## 7 6 18.6 ## 8 6 19.7 ## 9 6 21 ## 10 6 21.4 ## 11 8 10.4 ## 12 8 14.4 ## 13 8 15.2 ## 14 8 16.2 ## 15 8 19.2 4. What does this code do? Why might might it be useful? This code will group the dataset mtcars by cylinder type (in this case, 4, 6, or 8), then aggregate the values for each of the columns into a list. Specifically, there are 11 rows in the dataset that have a cyl value of 4, 7 rows that have a cyl value of 6, and 14 rows with a cyl value of 8. This is why the entries now look like , , and , respectively. Retrieving the value of at the first row of column mpg, for example, will return the 11 values associated with cylinder 4. mtcars2 &lt;- mtcars %&gt;% group_by(cyl) %&gt;% summarise_each(funs(list)) ## `summarise_each()` is deprecated. ## Use `summarise_all()`, `summarise_at()` or `summarise_if()` instead. ## To map `funs` over all variables, use `summarise_all()` mtcars2 ## # A tibble: 3 x 11 ## cyl mpg disp hp drat wt qsec vs am gear carb ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;lis&gt; &lt;lis&gt; ## 1 4 &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;fct … &lt;dbl… &lt;dbl… ## 2 6 &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;fct … &lt;dbl… &lt;dbl… ## 3 8 &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;fct … &lt;dbl… &lt;dbl… mtcars2$mpg[1] ## [[1]] ## [1] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26.0 30.4 21.4 25.5.3 Exercises 1. Why might the lengths() function be useful for creating atomic vector columns from list-columns? The lengths() function will return the length of each element in a list. This differs from the length() function which will return the number of elements contained in a list. For an example of the differences, see the code chunk below. However, for whatever reason, when lengths() is used with mutate() on a list-column, it will return the number of elements in the list rather than returning the vector with the lengths for each element in the list. In other words, when used with mutate(), lengths() performs what we normally would have thought length() would do. Even more confusing is how when length() is used with mutate, it breaks down and now returns the number of rows in the data frame regardless of what is stored in each row. lengths(list(a = 1:10, b = 2, z = 5)) ## a b z ## 10 1 1 length(mtcars) ## [1] 11 lengths(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 32 32 32 32 32 32 32 32 32 32 32 df &lt;- tribble( ~x, list(a = 1:10, b = 2, z = 5), list(a = 2:5, c = 4), list(a = 1, b = 2) ) # lengths() returns the number of elements in the list-column df %&gt;% mutate( length_of_a = lengths(x) ) ## # A tibble: 3 x 2 ## x length_of_a ## &lt;list&gt; &lt;int&gt; ## 1 &lt;list [3]&gt; 3 ## 2 &lt;list [2]&gt; 2 ## 3 &lt;list [2]&gt; 2 # length() returns the number of rows regardless of the value in the list-column df %&gt;% mutate( length_of_a = length(x) ) ## # A tibble: 3 x 2 ## x length_of_a ## &lt;list&gt; &lt;int&gt; ## 1 &lt;list [3]&gt; 3 ## 2 &lt;list [2]&gt; 3 ## 3 &lt;list [2]&gt; 3 2. List the most common types of vector found in a data frame. What makes lists different? In a data frame, I usually encounter numerics, characters, and factors, in which there is just a single value at any row x column designation. Lists are different because they can contain any number of data types, and multiple values for each. You can even have a list of lists! This means that a very diverse data set can be stored within lists in a single column of your data frame. That is the beauty of list-columns that this chapter tries to highlight. "],
["chapter-27-r-markdown.html", "Chapter 27 - R Markdown 27.2.1 Exercises 27.3.1 Exercises 27.4.7 Exercises", " Chapter 27 - R Markdown 27.2.1 Exercises 1. Create a new notebook using File &gt; New File &gt; R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output. This document is an example of this! 2. Create a new R Markdown document with File &gt; New File &gt; R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update. This document was knit as a github_document, which will output a markdown file that is compatible with for display on github. 3. Compare and contrast the R notebook and R markdown files you created above. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other? R notebooks create and update a separate .html file as you execute the chunks. In contrast, the R markdown file will not create the .html counterpart unless the document is knit and the output is specified to be html. There are also differences in how the output is displayed in RStudio as the chunks are run, in which notebooks show output directly after each chunk. When you copy the YAML header from R notebook to an R markdown, the document now turns into an R notebook. 4. Create one new R Markdown document for each of the three built-in formats: HTML, PDF and Word. Knit each of the three documents. How does the output differ? How does the input differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.) The output is generally the same, except that each document is of a different type. The input differs in how you specify the output in the YAML header, or on the option you select when manually clicking the knit button. 27.3.1 Exercises 1. Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold. I’ve intentionally left this question unanswered! But since R markdown is like any other markdown language with a set of formatting rules and options, a very simple but well-organized CV can be made in Rstudio. 2. Using the R Markdown quick reference, figure out how to: Add a footnote. Add a horizontal rule. Add a block quote. How to do the above: To add a footnote, use [^footnote-name] coupled with [^footnote-name]: at the bottom of the document, in which the text of the footnote comes after the colon. A horizontal rule can be added using at least three hyphens in succession (or asterisks, whichever you prefer): ---. A block quote can be added by prefacing the text using using &gt;. 3. Copy and paste the contents of diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown in to a local R markdown document. Check that you can run it, then add text after the frequency polygon that describes its most striking features. Below are the contents of diamond-sizes.Rmd. smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) We have data about 53940 diamonds. Only 126 are larger than 2.5 carats. The distribution of the remainder is shown below: Here is my commentary on the output: From the graph, we observe immediately that there are spikes along the frequency polygon, representing diamonds of a specific carat that are over-represented in the dataset. Looking closely, we observe that the spikes lie where there are “round” values of carat size, such as 0.5, 1, 1.5, etc. It is likely that this is a result of human rounding tendencies. 27.4.7 Exercises 1. Add a section that explores how diamond sizes vary by cut, colour, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo = FALSE on each chunk, set a global option. To set a global option to hide code chunks, we can include the following code chunk: knitr::opts_chunk$set( echo = FALSE ) For the learning experiences of you readers out there, I will continue to display the code that generates the graphs that explore how diamond sizes vary by cut, colour, and clarity. # diamond size by cut diamonds %&gt;% ggplot (aes(x = cut, y = carat))+ geom_boxplot() # diamond size by colour diamonds %&gt;% ggplot (aes(x = color, y = carat))+ geom_boxplot() # diamond size by clarity diamonds %&gt;% ggplot (aes(x = clarity, y = carat))+ geom_boxplot() 2. Download diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes. To filter out the top 20 largest diamonds, we can arrange the data table using arrange() so that the largest diamonds are at the top, then take the first 20 entries using head(). To display a table with their most important attributes, we can use knitr::kable() for carat, cut, color, clarity, and price, which I believe to be important based on the qualities they convey about the diamond. # filter out the largest 20 diamonds largest &lt;- diamonds %&gt;% arrange(desc(carat)) %&gt;% head (20) # display a table, using kable() to make it prettier largest %&gt;% select (carat, cut, color, clarity, price) %&gt;% knitr::kable (caption = &quot;important qualities of top 20 diamonds&quot;) Table 1: important qualities of top 20 diamonds carat cut color clarity price 5.01 Fair J I1 18018 4.50 Fair J I1 18531 4.13 Fair H I1 17329 4.01 Premium I I1 15223 4.01 Premium J I1 15223 4.00 Very Good I I1 15984 3.67 Premium I I1 16193 3.65 Fair H I1 11668 3.51 Premium J VS2 18701 3.50 Ideal H I1 12587 3.40 Fair D I1 15964 3.24 Premium H I1 12300 3.22 Ideal I I1 12545 3.11 Fair J I1 9823 3.05 Premium E I1 10453 3.04 Very Good I SI2 15354 3.04 Premium I SI2 18559 3.02 Fair I I1 10577 3.01 Premium I I1 8040 3.01 Premium F I1 9925 3. Modify diamonds-sizes.Rmd to use comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats. Below is the formatted output, using the comma() function specified in the chapter with an additional sentence stating the percentage of diamonds that are larger than 2.5 carats (the new sentence is bolded). comma &lt;- function(x) format(x, digits = 2, big.mark = &quot;,&quot;) smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) We have data about 53,940 diamonds. Only 126 are larger than 2.5 carats. The percentage of diamonds that are larger than 2.5 carats is 0.2335929%. The distribution of the remainder is shown below: 4. Set up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache = TRUE, then verify your understanding of caching. a_variable &lt;- 2 print(paste(&quot;a:&quot;, a_variable)) ## [1] &quot;a: 2&quot; lubridate::now() ## [1] &quot;2020-01-21 21:48:01 PST&quot; b_variable &lt;- 5 * a_variable print(paste(&quot;b:&quot;, b_variable)) ## [1] &quot;b: 10&quot; lubridate::now() ## [1] &quot;2020-01-21 21:48:01 PST&quot; c_variable &lt;- 10 * a_variable print(paste(&quot;c:&quot;, c_variable)) ## [1] &quot;c: 20&quot; lubridate::now() ## [1] &quot;2020-01-21 21:48:01 PST&quot; d_product &lt;- b_variable * c_variable print(paste(&quot;d:&quot;, d_product)) ## [1] &quot;d: 200&quot; lubridate::now() ## [1] &quot;2020-01-21 21:48:01 PST&quot; The output of these code chunks will be cached since we set cache = TRUE. If we re-knit this document without changing anything in the code chunks, the value of lubridate::now() that is printed to the screen should not change, since the cached values will be used. "],
["chapter-28-graphics-for-communication.html", "Chapter 28 - Graphics for communication 28.2.1 Exercises 28.3.1 Exercises 28.4.4 Exercises", " Chapter 28 - Graphics for communication 28.2.1 Exercises 1. Create one plot on the fuel economy data with customised title, subtitle, caption, x, y, and colour labels. To do this, we use add labs() to the ggplot along with the title, subtitle, caption, x, y, and color arguments. I’ve filled them in using what was provided in the book. ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = class)) + geom_smooth(se = FALSE) + labs( title = &quot;Fuel efficiency generally decreases with engine size&quot;, subtitle = &quot;Two seaters (sports cars) are an exception because of their light weight&quot;, caption = &quot;Data from fueleconomy.gov&quot;, x = &quot;Engine displacement (L)&quot;, y = &quot;Highway fuel economy (mpg)&quot;, color = &quot;Car type&quot; ) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 2. The geom_smooth() is somewhat misleading because the hwy for large engines is skewed upwards due to the inclusion of lightweight sports cars with big engines. Use your modelling tools to fit and display a better model. A better model would be to use a simple linear model (straight line fit) between hwy and displ. To do this the fast way, we could just specify geom_smooth(method = 'lm'). If we wanted to use our modeling tools, we could manually specify the model using lm(), then fit the predicted values on top of the graph. There are even two ways to do this, either by manually plotting the slope and intercept using geom_abline(), or by generating predictions for a grid of displ values and plotting the line using geom_line(). # using geom_smooth(method = &quot;lm&quot;) ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point(aes(color = class)) + geom_smooth(se = FALSE, method=&quot;lm&quot;) + labs( title = &quot;Plotted using geom_smooth(method = &#39;lm&#39;, se = F)&quot; ) # using geom_line() mpg_mod &lt;- lm(hwy ~ displ, data = mpg) grid &lt;- mpg %&gt;% data_grid(displ) %&gt;% add_predictions(mpg_mod) grid ## # A tibble: 35 x 2 ## displ pred ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.6 30.0 ## 2 1.8 29.3 ## 3 1.9 29.0 ## 4 2 28.6 ## 5 2.2 27.9 ## 6 2.4 27.2 ## 7 2.5 26.9 ## 8 2.7 26.2 ## 9 2.8 25.8 ## 10 3 25.1 ## # … with 25 more rows ggplot(mpg, aes(x=displ)) + geom_point(aes(y = hwy, color = class)) + geom_line(aes(y = pred), data = grid) + labs( title = &quot;Plotted using lm(), grid(), add_predictions(), and geom_line()&quot; ) # using geom_abline() ggplot(mpg, aes (x=displ, y=hwy)) + geom_point(aes(color = class))+ geom_abline(intercept = mpg_mod$coefficients[1], slope = mpg_mod$coefficients[2]) + labs( title = &quot;Plotted using lm() and geom_abline() with the coefficients of the model&quot;, subtitle = &quot;notice how the line extends to the edges of the graph&quot; ) 3. Take an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand. I’ve taken a plot from earlier in this document comparing diamond size by quality of cut and plotted it with labels to make it easier to understand. diamonds %&gt;% ggplot (aes(x = cut, y = carat))+ geom_boxplot(aes(color = cut))+ labs(title = &quot;Diamonds with higher quality cuts are generally smaller&quot;, subtitle = &quot;Boxplot of diamond carat size vs quality of cut&quot;, x = &quot;Quality of cut&quot;, y = &quot;Carat&quot;, caption = &quot;data obtained from ggplot2 built-in dataset &#39;diamonds&#39;&quot;) 28.3.1 Exercises 1. Use geom_text() with infinite positions to place text at the four corners of the plot. The example in the chapter uses a tibble to store one label with its associated parameters, which hints to us that we can populate this tibble with multiple labels with any parameters of our choosing. For this exercise, we want to place the labels in each of four corners. To do this, we can add more entries (rows) to the tibble that correspond to these labels and their locations. Then, we can pass this tibble of labels into geom_text(). label &lt;- tibble( displ = c(-Inf, Inf, -Inf, Inf), hwy = c(Inf, Inf, -Inf, -Inf), label = c(&quot;topleft&quot;, &quot;topright&quot;, &quot;bottomleft&quot;, &quot;bottomright&quot;), hjust = c(&quot;left&quot;, &quot;right&quot;, &quot;left&quot;, &quot;right&quot;), vjust = c(&quot;top&quot;, &quot;top&quot;, &quot;bottom&quot;, &quot;bottom&quot;) ) label ## # A tibble: 4 x 5 ## displ hwy label hjust vjust ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -Inf Inf topleft left top ## 2 Inf Inf topright right top ## 3 -Inf -Inf bottomleft left bottom ## 4 Inf -Inf bottomright right bottom ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_text(aes(label = label, hjust = hjust, vjust = vjust), data = label) 2. Read the documentation for annotate(). How can you use it to add a text label to a plot without having to create a tibble? annotate() adds a geom of your choice to the plot, but instead of supplying the parameters in a data frame, you can supply it using vectors. This is useful if you just want to add a simple text label to a part of the plot without having to make a tibble with one row. An example is below. ggplot(mpg, aes(displ, hwy)) + geom_point(aes(color = cyl)) + annotate(&quot;text&quot;, x = 6, y = 40, label = &quot;I added this using annotate()&quot;) 3. How do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: think about the underlying data.) Labels using geom_text() get placed at the same location on each facet if the x / y coordinates are hard coded (in this instance, they are hard coded as x = 6, y = 40). If only a single label is provided, it is duplicated for each facet. # add to all facets label &lt;- tibble( displ = 6, hwy = 40, label = &quot;my_label&quot; ) label ## # A tibble: 1 x 3 ## displ hwy label ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 40 my_label # labels get placed at the same location on each facet if the x / y coordinates are hard coded ggplot (mpg, aes(displ, hwy))+ geom_point()+ geom_text(data = label, aes(label = label))+ facet_wrap(~cyl) In order to add the label to just one of the facets, the tibble containing your labels should include the variable you are facetting on (in this case, cyl). We set the value of cyl to 4, in order for the label to show up only in the graph for cyl=4. # add to a single facet label &lt;- tibble( displ = 6, hwy = 40, cyl = 4, label = &quot;my_label for cyl=4&quot; ) label ## # A tibble: 1 x 4 ## displ hwy cyl label ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 40 4 my_label for cyl=4 ggplot (mpg, aes(displ, hwy))+ geom_point()+ geom_text(data = label, aes(label = label))+ facet_wrap(~cyl) To add a different label to each facet, we can expand the tibble in the above example to have a row for each value of cyl, along with the corresponding unique label. # add unique labels to each facet label &lt;- tibble( displ = 6, hwy = 40, cyl = c(4,5,6,8), label = c(&quot;cyl=4&quot;,&quot;cyl=5&quot;,&quot;cyl=6&quot;,&quot;cyl=8&quot;) ) label ## # A tibble: 4 x 4 ## displ hwy cyl label ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 40 4 cyl=4 ## 2 6 40 5 cyl=5 ## 3 6 40 6 cyl=6 ## 4 6 40 8 cyl=8 ggplot (mpg, aes(displ, hwy))+ geom_point()+ geom_text(data = label, aes(label = label))+ facet_wrap(~cyl) 4. What arguments to geom_label() control the appearance of the background box? The fill aesthetic dictates the color of the background of the label. In the example below, I label highest 5 displ data points and fill based on auto or manual. You can also color the text and border of the label using color. To get rid of the black border (but keep the fill), use label.size = 0. It may be helpful to call geom_label() before geom_point(), or else the label will be placed over the data point and mask it. label &lt;- mpg %&gt;% arrange(desc(displ)) %&gt;% head(5) label ## # A tibble: 5 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 chevrolet corve… 7 2008 8 manu… r 15 24 p 2sea… ## 2 chevrolet k1500… 6.5 1999 8 auto… 4 14 17 d suv ## 3 chevrolet corve… 6.2 2008 8 manu… r 16 26 p 2sea… ## 4 chevrolet corve… 6.2 2008 8 auto… r 15 25 p 2sea… ## 5 jeep grand… 6.1 2008 8 auto… 4 11 14 p suv # color code background based on category, if label comes after geom_point(), may mask the point as demonstrated below ggplot (mpg, aes(displ, hwy))+ geom_point()+ geom_label(data = label, aes(label = trans, fill = trans)) # color the text and border, call label before geom_point(), now the points show up over the labels ggplot (mpg, aes(displ, hwy))+ geom_label(data = label, aes(label = trans, color = trans))+ geom_point() 5. What are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options. To add an arrow to your plot, you can use geom_segment() with the arrow argument. The arrow option of geom_segment() receives the output of arrow(), which has four arguments: angle which describes the width of the arrow head, length which is the length of the arrow head from tip to base, ends which indicates which sides to put the arrow heads on, and type which indicates whether the arrow head should be an open or closed triangle. Below are examples of how to customize your arrows by toggling the arguments to arrow(). I initially tried to compile all the variations of the arrow arguments in order to plot the different types of arrows all at once using one geom_segment() call (similar to how you can lump multiple labels together at different locations using geom_text()). However it doesn’t seem like you can pass a data frame of arguments into the arrow() function by putting it inside aes(). The code below errors and produces segments wihtout arrowheads. So, I resorted to just calling geom_segment 4 times, in order to compare the different arrow types. label &lt;- tibble( displ = 6, hwy = c(30,33,36,39,42), x = 4, y = c(30,33,36,39,42), xend = 5, yend = c(30,33,36,39,42), angle = c(30, 10, 30, 30, 30), length = c(unit(0.25, &quot;inches&quot;), unit(0.25, &quot;inches&quot;), unit(0.1, &quot;inches&quot;), unit(0.25, &quot;inches&quot;), unit(0.25, &quot;inches&quot;)), ends = c(&quot;last&quot;, &quot;last&quot;, &quot;last&quot;, &quot;both&quot;, &quot;last&quot;), type = c(&quot;open&quot;, &quot;open&quot;, &quot;open&quot;, &quot;open&quot;, &quot;closed&quot;), label = c(&quot;arrowhead with default settings&quot;, &quot;angle = 10 degrees&quot;, &quot;length = 0.1 inches&quot;, &quot;ends = both&quot;, &quot;type = closed&quot;) ) label ## # A tibble: 5 x 11 ## displ hwy x y xend yend angle length ends type label ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 6 30 4 30 5 30 30 0.25 last open arrowhead … ## 2 6 33 4 33 5 33 10 0.25 last open angle = 10… ## 3 6 36 4 36 5 36 30 0.1 last open length = 0… ## 4 6 39 4 39 5 39 30 0.25 both open ends = both ## 5 6 42 4 42 5 42 30 0.25 last closed type = clo… ggplot (mpg, aes(displ, hwy))+ geom_point()+ geom_segment(aes( x = 4, y =30, xend = 5, yend = 30), arrow = arrow())+ geom_segment(aes( x = 4, y =33, xend = 5, yend = 33), arrow = arrow(angle = 10))+ geom_segment(aes( x = 4, y =36, xend = 5, yend = 36), arrow = arrow(length = unit(0.1, &quot;inches&quot;)))+ geom_segment(aes( x = 4, y =39, xend = 5, yend = 39), arrow = arrow(ends = &quot;both&quot;))+ geom_segment(aes( x = 4, y =42, xend = 5, yend = 42), arrow = arrow(type = &quot;closed&quot;))+ geom_text(data = label, aes(label = label))+ labs (title = &quot;different types of arrows, plotted on top of a random graph&quot;) # doesn&#39;t work if you try to map the arrow arguments using aes() ggplot (mpg, aes(displ, hwy))+ geom_point()+ geom_segment(data = label, aes( x = x, y =y, xend = xend, yend = yend, arrow = arrow(angle = angle, ends = ends, type = type)))+ geom_text(data = label, aes(label = label))+ labs (title = &quot;cannot map arguments to arrow() using aes()&quot;) ## Warning: Ignoring unknown aesthetics: arrow ## Don&#39;t know how to automatically pick scale for object of type arrow. Defaulting to continuous. ## Warning in rep(x$angle, length.out = maxn): &#39;x&#39; is NULL so the result will ## be NULL ## Warning in rep(x$length, length.out = maxn): &#39;x&#39; is NULL so the result will ## be NULL ## Warning in rep(x$ends, length.out = maxn): &#39;x&#39; is NULL so the result will ## be NULL ## Warning in rep(x$type, length.out = maxn): &#39;x&#39; is NULL so the result will ## be NULL Here is a cleaned up version that isn’t randomly plotted on top of the mpg dataset. label &lt;- tibble( x = 6, y = c(17,14,11,8,5), label = c(&quot;arrowhead with default settings&quot;, &quot;angle = 10 degrees&quot;, &quot;length = 0.1 inches&quot;, &quot;ends = both&quot;, &quot;type = closed&quot;) ) label ## # A tibble: 5 x 3 ## x y label ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 17 arrowhead with default settings ## 2 6 14 angle = 10 degrees ## 3 6 11 length = 0.1 inches ## 4 6 8 ends = both ## 5 6 5 type = closed ggplot (data = label)+ geom_segment(aes( x = 4, y =17, xend = 5, yend = 17), arrow = arrow())+ geom_segment(aes( x = 4, y =14, xend = 5, yend = 14), arrow = arrow(angle = 10))+ geom_segment(aes( x = 4, y =11, xend = 5, yend = 11), arrow = arrow(length = unit(0.1, &quot;inches&quot;)))+ geom_segment(aes( x = 4, y =8, xend = 5, yend = 8), arrow = arrow(ends = &quot;both&quot;))+ geom_segment(aes( x = 4, y =5, xend = 5, yend = 5), arrow = arrow(type = &quot;closed&quot;))+ geom_text(data = label, aes(label = label, x = x, y = y))+ labs (title = &quot;Different types of arrows!&quot;)+ coord_cartesian(xlim = c(3,8), ylim = c(1, 20)) Although there wasn’t an exercise for this, here are some examples of using the other suggested additions such as: geom_hline() and geom_rect() # adding a horizontal line to focus on points above or below ggplot (mpg, aes(displ, hwy))+ geom_point(aes(color = cyl))+ geom_hline(yintercept = 35, color = &quot;red&quot;, size = 1) # boxing an area of the graph to emphasize what you want to focus on ggplot (mpg, aes(displ, hwy))+ geom_point(aes(color = cyl))+ geom_rect( xmin = 6, xmax = 7.2, ymin = 15, ymax = 30, fill = NA, color = &quot;red&quot;) # use geom_rect with aes() to draw boxes on top of points in a graph ggplot (mpg, aes(displ, hwy))+ geom_point(aes(color = cyl))+ geom_rect( aes (xmin = displ-0.1, xmax = displ+0.1, ymin = hwy-0.1, ymax = hwy+0.1)) 28.4.4 Exercises 1. Why doesn’t the following code override the default scale? This exercise follows from this code in the chapter, which provides the default coloring of the graph: df &lt;- tibble( x = rnorm(10000), y = rnorm(10000) ) ggplot(df, aes(x, y)) + geom_hex() + coord_fixed() The code provided in the exercise does not change the color gradient to white/red. This is because scale_fill_gradient() should be used instead of scale_color_gradient, since the geom_hex() graphics need to be “filled”. # does not change the color gradient because ggplot(df, aes(x, y)) + geom_hex() + scale_colour_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() # using scale_fill_gradient now changes the colors! ggplot(df, aes(x, y)) + geom_hex() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() 2. What is the first argument to every scale? How does it compare to labs()? Based on the documentation, the first argument to scale_x_continuous() is name. If the name is modified by calling scale_x_continuous(name = “my_name”), the modified name “my_name” is displayed instead of the default. The same can be done using labs(), by changing the appropriate label. In the example below, you can use either scale_x_continuous(name=“my_name”) or labs(x = “my_name”) to change the label of the x- axis. # change x axis label using scale_x_continuous ggplot(mpg, aes(displ, hwy))+ geom_point()+ scale_x_continuous(name = &quot;a new x-axis name using scale_x_continuous()&quot;) # change x axis label using labs() ggplot(mpg, aes(displ, hwy))+ geom_point()+ labs(x = &quot;a new x-axis name using labs()&quot;) 3. Change the display of the presidential terms by: Combining the two variants shown above. The two variants referred to by this question are: # variant 1 presidential %&gt;% mutate(id = 33 + row_number()) %&gt;% ggplot(aes(start, id)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + scale_x_date(NULL, breaks = presidential$start, date_labels = &quot;&#39;%y&quot;) # variant 2 presidential %&gt;% mutate(id = 33 + row_number()) %&gt;% ggplot(aes(start, id, colour = party)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + scale_colour_manual(values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;)) We can see that version 1 has the customized breaks on the x-axis but no coloration, and version 2 has coloration but the breaks on the x-axis are not customized. The combined version with both a customized x-axis as well as coloration is below. # combining variant 1 and 2 presidential %&gt;% mutate(id = 33 + row_number()) %&gt;% ggplot(aes(start, id, color = party)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + scale_x_date(NULL, breaks = presidential$start, date_labels = &quot;&#39;%y&quot;) + scale_colour_manual(values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;)) Improving the display of the y axis. The y-axis can be improved by showing each tick corresponding to a single presidential number using scale_y_continuous and setting the breaks. presidential2 &lt;- presidential %&gt;% mutate(id = 33 + row_number()) presidential2 %&gt;% ggplot(aes(start, id, color = party)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + scale_x_date(NULL, breaks = presidential$start, date_labels = &quot;&#39;%y&quot;) + scale_colour_manual(values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;)) + scale_y_continuous(NULL, breaks = presidential2$id) Labelling each term with the name of the president. The names of each president are already in the data frame in the $name column, so we can use geom_text() setting the labels to presidential$name. The default is to put the label where the geom_point() is, which is quite ugly. So, we can change this by adjusting the vjust and hjust parameters, placing the labels below and to the right of each point. presidential2 %&gt;% ggplot(aes(start, id, color = party)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + scale_x_date(NULL, breaks = presidential$start, date_labels = &quot;&#39;%y&quot;) + scale_colour_manual(values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;)) + scale_y_continuous(NULL, breaks = presidential2$id)+ geom_text(aes(label = presidential$name), vjust = 1.2, hjust = -0.5, show.legend = F) Adding informative plot labels. To do this, we remove the “NULL” from scale_x_date() and scale_y_continuous(), then add the labels using labs(). presidential2 %&gt;% ggplot(aes(start, id, color = party)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + scale_x_date(breaks = presidential$start, date_labels = &quot;&#39;%y&quot;) + scale_colour_manual(values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;)) + scale_y_continuous(breaks = presidential2$id)+ geom_text(aes(label = presidential$name), vjust = 1.2, hjust = -0.5, show.legend = F)+ labs (title = &quot;Presidential terms, labeled by party affiliation&quot;, caption = &quot;data from built-in R dataset: presidential&quot;, x = &quot;year&quot;, y = &quot;President number&quot;, color = &quot;Political party&quot;) Placing breaks every 4 years (this is trickier than it seems!). I’m assuming that this refers to placing a tick 4 years after the start of any presidential term that lasts longer than 4 years. For example, if a presidential term is 8 years, there should be a tick in the middle. If a term was cut short and was only 5 years, there should still be a tick 4 years after the start date. To do this, we can calculate the diference between each adjacent point in presidential$start. If it is longer than 4 years, we insert a tick at 4 years after the start date. To implement this, we can use a simple for loop a long with some conditional statements to check whether each term lasted longer than 4 years. years &lt;- lubridate::year(presidential$start) appended_years &lt;- vector() for (i in 1:(length(years)-1)) { appended_years &lt;- c(appended_years, presidential$start[i]) if (years[i+1] - years[i] &gt;4) appended_years &lt;- c(appended_years, presidential$start[i] + dyears(4)) if(i == (length(years)-1)) appended_years &lt;- c(appended_years, presidential$start[i+1]) } appended_years ## [1] -6190 -4730 -3268 -2232 -772 -346 1114 1681 2576 4037 5497 ## [12] 6959 8420 9880 11342 12802 14264 #convert back to a date-time class(appended_years) &lt;- &quot;Date&quot; presidential2 %&gt;% ggplot(aes(start, id, color = party)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + scale_x_date(breaks = appended_years, date_labels = &quot;&#39;%y&quot;) + scale_colour_manual(values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;)) + scale_y_continuous(breaks = presidential2$id)+ geom_text(aes(label = presidential$name), vjust = 1.2, hjust = -0.5, show.legend = F)+ labs (title = &quot;Presidential terms, labeled by party affiliation&quot;, caption = &quot;data from built-in R dataset: presidential&quot;, x = &quot;year&quot;, y = &quot;President number&quot;, color = &quot;Political party&quot;) 4. Use override.aes to make the legend on the following plot easier to see. We can use the guides() function with guide_legend(override.aes = list(alpha=1)) to “override” the alpha = 1/20 for the legend. I turn it back to a value of 1 in order to make the points in the legend easier to see. # points are very faint on the legend ggplot(diamonds, aes(carat, price)) + geom_point(aes(colour = cut), alpha = 1/20) # change the alpha back to 1 just for the legend to make points visible ggplot(diamonds, aes(carat, price)) + geom_point(aes(colour = cut), alpha = 1/20)+ guides(color = guide_legend(override.aes = list (alpha = 1))) "]
]
