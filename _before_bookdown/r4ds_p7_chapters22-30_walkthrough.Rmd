---
title: "R for Data Science Walkthrough Chapters 22-30"
author: "Erick Lu"
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
    fig_width: 5
    fig_height: 3.5
---

This my walkthrough for chapters 22-30 for the book: _R for Data Science_ by Hadley Wickham and Garrett Grolemund. As a learning exercise, I provide solutions to their exercises and some of my own notes and explorations.

# Chapter 22 - No Exercises

# Chapter 23

```{r}
library(tidyverse)
library(modelr)
```

The datasets that are used in this chapter are simulated datasets, such as the one shown below (sim1)

```{r}
head(sim1)
```

Below are the functions used in this chapter, written by Hadley for demonstration purposes:

```{r chapter23_examples}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

head(grid)
dim(grid)

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```


## 23.2.1 Exercises

### 1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

In the simulated dataset, there are a couple of outliers that are far displaced from the rest of the points. These outliers can skew the linear approximation, because these points are so 'distant' from the other points in the dataset. Because the linear model tries to minimize the distance between each point and the fitted model (the "residuals"), these outliers will skew the approximation, pulling the line closer to them. The larger the residual, the more it contributes to the RMSE. We notice that the fitted line is slightly skewed towards the direction of the outlying point.

```{r exercise_23-2-1_1}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

# first, take a look at the data
sim1a
ggplot(sim1a, aes (x, y)) +
  geom_point()

mod <- lm(y~x, data = sim1a)
summary(mod)

# add the fitted linear model to the scatterplot
ggplot(sim1a, aes (x, y)) +
  geom_point()+
  geom_abline(intercept = mod$coefficients[1], slope = mod$coefficients[2])

# compare with the baseR lm with geom_smooth() overlay, looks like they overlap, as expected
ggplot(sim1a, aes (x, y)) +
  geom_point()+
  geom_abline(intercept = mod$coefficients[1], slope = mod$coefficients[2], size=3, color = "red")+
  geom_smooth(method = 'lm')

```


### 2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r exercise_23-2-1_2}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}
# use optim() function to 
best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par

# compare the parameters from optim() with the parameters obtained from lm()
mod <- lm (y~x, data = sim1a)
coef(mod)

# plot the two lines on the scatterplot to observe differences in fit
ggplot(data = sim1a, aes(x, y))+
  geom_point()+
  geom_abline(slope = mod$coefficients[2], intercept = mod$coefficients[1], color = "red")+
  geom_abline(slope = best$par[2], intercept = best$par[1], color = "blue")+
  labs(title = "Red = root-mean-squared distance fit using lm() \n Blue = mean-absolute distance fit using optim()")

```

Use optim() to fit this model to the simulated data above and compare it to the linear model.

The measure_distance() function provided above uses the absolute-mean distance (mean(abs(diff))) instead of the root-mean-squared distance, sqrt(mean(diff^2)). Using optim() and the absolute-mean distance, we find that the line is less skewed by the outlying points. The red line is "pulled" more towards the outliers, whereas the blue line remains more embbeded with the bulk of the data. This is because squaring the residuals results in much greater values when the residuals are large, so minimizing the residuals for outliers takes more priority when using the squared distance.

### 3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?

A quadratic or higher order function may have more than one local minimum / maximum. This may result in the optim() function providing an unideal result. In the provided function, since a[1] and a[3] are both constants that are not multiplied by a column in data (such as ```data$x```), they can be added together and represent the intercept of the line. This results in the sum of a[1] and a[3] equalling the intercept we found before using the equation ```a[1] + data$x * a[2]```.

a[1] and a[3] can therefore have infinite possibilites of values, as long as the sum of a[1] and a[3] are equal to the local optimum of ```a[1] + data$x * a[2]```. In the example below, if we use the dataset sim1, we find that a[1] and a[3] must sum to 4.220074. So, depending on where you start with the optim() function, a[1] and a[3] will have differing values, but still add up to 4.220074.

We see in the graph that the optim() function and lm() again provde the same result.

```{r exercise_23-2-1_3}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

best <- optim(c(0, 0, 0), measure_distance, data = sim1)
best$par
best <- optim(c(0, 0, 1), measure_distance, data = sim1)
best$par

# since in the model above, a[1] and a[3] may be theoretically combined to represent the intercept of the line, we can graph it as such:
ggplot(data = sim1, aes(x, y))+
  geom_point()+
  geom_smooth(method = "lm", color = "red", size = 2)+
  geom_abline(slope = best$par[2], intercept = best$par[1] + best$par[3], color = "blue")+
  labs(title = "Red = using lm() \n Blue = optim() using the provided 3 parameter model")

```


## 23.3.3 Exercises

### 1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?

Using the loess() function instead of lm(), the line curves more towards the direction of variation and is not strictly a straight line. The default method for fitting using geom_smooth() is loess(), so the line that is superimposed on the ggplot is the same as the line generated by the predictions using the loess() model. When we superimpose all 3 options (loess() prediction, lm() prediction, and geom_smooth()), we see that geom_smooth() and the loess() prediction precisely overlap, whereas the lm() prediction does not.

```{r exercise_23-3-3_1-1}
# using lm()
sim1_mod_lm <- lm(y ~ x, data = sim1)

grid <- sim1 %>% 
  data_grid(x) 
grid
grid <- grid %>% 
  add_predictions(sim1_mod_lm) 
grid

sim1 <- sim1 %>% 
  add_residuals(sim1_mod_lm)
sim1

ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)

ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 

```


```{r exercise_23-3-3_1-2}
# using loess()
sim1_mod_loess <- loess(y ~ x, data = sim1)

grid <- sim1 %>% 
  data_grid(x) 
grid
grid <- grid %>% 
  add_predictions(sim1_mod_loess) 
grid

sim1 <- sim1 %>% 
  add_residuals(sim1_mod_loess)
sim1

# residuals plot
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 

# plot the regression line
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)

# compare to geom_smooth() and lm()
ggplot(sim1, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 3)+
  geom_smooth(color = "blue", se = F)+
  geom_smooth(method = "lm", color = "green", se = F)

```

### 2. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ?

add_predictions() can only work with one supplied model, in which it will generate a new column named "pred" in your data frame. gather_predictions() and spread_predictions() can work with multiple supplied models and generate predictions for each model supplied. gather_predictions() appends the model name to the data frame along with the predictions as new rows to the data frame, in a tidy fashion. spread_predictions() appends the new predictions to the data frame as separate columns. You can visualize the differences below, in which spread_predictions makes the data frame "wider" and gather_predictions() makes the data frame "taller".

```{r exercise_23-3-3_2}
grid <- sim1 %>% 
  data_grid(x) 
grid

grid_add <- grid %>% 
  add_predictions(sim1_mod_lm) 
grid_add

grid_gather <- grid %>% 
  gather_predictions(sim1_mod_lm, sim1_mod_loess) 
grid_gather

grid_spread <- grid %>% 
  spread_predictions(sim1_mod_lm, sim1_mod_loess) 
grid_spread
```


### 3. What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

geom_ref_line() adds either a horizontal or vertical line at a specified position to your ggplot, of a specified color (default white). It comes from ```modelr```. This is useful when plotting residuals because ideally the residuals should be centered around 0. Having a reference line helps the viewer judge how the residuals behave. Conceptually, we can think of the horizontal line in the residuals plot as the prediction, and of the residual as how far off the true value is from that prediction.

### 4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

The residuals, ideally, should approximately be normally distributed. Examining the frequency polygon is useful as a visual assessment for whether or not the residuals follow the Normal distribution. This graph will also more easily capture any abnormal pattern in the residuals, in which there are over-representations of either + or - residuals. The cons are that this graph masks some of the variability in the residuals by binning them, and you lose the relationship between the residual and the predictor variable. This is best paired with a scatterplot of the residuals so you can observe exactly where each point lies in relation to the predictor variable.

```{r exercise_23-3-3_4}
# example from book
ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)

# make an example where residuals approximate normal distribution
x = seq(1:1000)
y = 12 + 5 * x + rnorm(1000,0,100) # add random white noise
mysim <- as_tibble(cbind(x, y))

#fit model
mysim_mod <- lm(y ~ x, data = mysim)
summary(mysim_mod)

# make predictions
mygrid <- mysim %>% 
  data_grid(x) 
mygrid

mygrid <- mygrid %>% add_predictions(mysim_mod)
mygrid

mysim <- mysim %>% add_residuals(mysim_mod)
mysim

# plot prediction line
ggplot(mysim, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data =  mygrid, colour = "red", size = 1)

# plot freqpoly() of residuals
ggplot(mysim, aes(resid)) + 
  geom_freqpoly()

# plot scatterplot of residuals
ggplot(mysim, aes (x = x, y = resid))+
  geom_ref_line(h=0)+
  geom_point()

```


## 23.4.5 Exercises

For categorical variables, the book performs a similar prediction workflow:

```{r chapter23-4-5_examples_1}
# use the sim2 dataset from modelr
sim2

# examine how the data are distributed
ggplot(sim2) + 
  geom_point(aes(x, y))

# generate a model (R automatically recognizes that the predictor variables are categorical)
mod2 <- lm(y ~ x, data = sim2)

# generate predictions based on model
grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)
grid

# plot the predictions overlaid on the graph
ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```

For using more than one predictor variable (can be a combination of categorical and continuous variables), a similar approach is used:

```{r}
# examine data and build models
sim3
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

# inputting multiple variables into data_grid results in it returning all the possible combinations between x1 and x2
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
grid

```

Inputting multiple continuous variables into data_grid will also return all possible combinations, unless you manually specify the combinations as done in the book chapter.

```{r}
# examine data and build models
sim4
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

# with manual specification of range
grid <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5), 
    x2 = seq_range(x2, 5) 
  ) %>% 
  gather_predictions(mod1, mod2)
grid

# without manual specification of range, provides all combinations of values
grid2 <- sim4 %>% 
  data_grid(
    x1, 
    x2
  ) 
grid2
```


### 1. What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions?

To fit a model to sim2 without an intercept, use "-1" with the formula, as shown. We observe that the model matrix loses the "intercept" column when we use "-1". We then generate predictions using gather_predictions() on both models. We find that both models generate the same predictions! This is because in both cases, the mean of the observations for each categorical possibility is the optimal fit. 

```{r chapter23-4-5_1}
mod1 <- lm(y ~ x, data = sim2)
model_matrix(sim2, y ~x)

mod2 <- lm(y ~ x - 1, data = sim2)
model_matrix(sim2, y ~x -1)

grid <- sim2 %>% 
  data_grid(x) %>% 
  gather_predictions(mod1, mod2)
grid

ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred, shape = model, color = model), size = 4)
```


### 2. Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is \* a good shorthand for interaction?

The model matrix for y ~ x1 + x2 for sim3 has an intercept, an x1 column, and 3 columns for x2, corresponding to each of the possibilities for the categories in x2. The model matrix for y ~ x1 \* x2 has an these columns as well, but also has x1:x2b, x1:x2c, and x1:x2d, which correspond to the interaction between x1 and each of the categories in x2.

For sim4, since the values of the predictor variables x1 and x2 are both continuous variables, the x2 column does not need to be split up as in sim3. The model matrix consists of 3 columns, an intercept, x1, and x2. Similarly, y ~ x1 \* x2 uses a model matrix of 4 columns, which consists of the 3 mentioned previously and an additional column x1:x2.

 is a good shorthand for this interaction because the additional columns that it adds are products. it suggests that changes in values of one variable will affect the value of the other, which is what it is trying to model.


```{r}
sim3
# models fit to sim3
model_matrix(sim3, y ~ x1 + x2 )
model_matrix(sim3, y ~ x1 * x2 )

sim4
# models fit to sim4
model_matrix(sim4, y ~ x1 + x2 )
model_matrix(sim4, y ~ x1 * x2 )

```


### 3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

The formulas below use the model matricies that were described in exercise 2 of this section. We can re-create the model matricies by writing a function that accepts the predictor variables as input, as well as the type of operator. In the example below, I input sim3 along with either "+" or "\*" and show that the model matricies generated match those made by the modelr function model_matrix().

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

# check to see if the column to add is a categorical variable (factor), and split it up if true
add_column <- function ( mycolumn, colname ) {
  # test whether the columns are factors or not
  if (is.factor (mycolumn)){
    my_levels <- levels(mycolumn)
    
    # split into separate columns
    output <- vector("list")
    for (i in 2: length(my_levels)) {
      level_name <- str_c(colname, my_levels[i])
      output[[level_name]] <- 1*(mycolumn == my_levels[i])
    }
  }
  # if not factor, return the column as-is
  else {
    output <- list( mycolumn)
    names(output) <- colname
  }
  output
}

# check the type of operator supplied (+ or *) and create the columns as necessary, calling the add_column function
make_matrix <- function (data, operator) {
  my_colnames <- c("x1", "x2")
  # store the columns of the model matrix in "mm"
  mm <- list()
  
  # make the default intercept column
  mm$intercept <- rep(1, length(data$x2))
  
  # add the base columns using add_column()
  for (item in my_colnames) {
    mm <- c(mm, add_column (data[[item]], item))
  }
  mm <- bind_cols(mm)
  
  # if the operator is *, add the appropriate columns based on vector multiplication
  if (operator == "*") {
    x1cols <- colnames(mm)[grep("x1", colnames(mm))]
    x2cols <- colnames(mm)[grep("x2", colnames(mm))]
    newcols <- vector("list")
    for (i in x1cols) {
      print(i)
      for (j in x2cols) {
        print(j)
        new_level_name <- str_c(i,j, sep = ":")
        print(new_level_name)
        newcols[[new_level_name]] <- mm[[i]]* mm[[j]]
      }
    }
    newcols <- bind_cols(newcols)
    mm <- cbind(mm, newcols)
  }
  mm
}

make_matrix( sim3, "+")
model_matrix( sim3, y ~ x1 + x2)

make_matrix( sim3, "*")
model_matrix( sim3, y ~ x1 * x2)
```

### 4. For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim?

To test which model does a better job, we can look at the residuals for each model by subtracting the predicted values for each model from the true values. A measurement to compare the models would be to calculate the RMSE using these residuals, and choose the model with the lower RMSE. Doing so, we find that mod2 seems to be a better fit. The RMSE for mod2 is 2.0636 whereas the RMSE for mod1 is slightly higher (worse fit), at 2.0998.

```{r chapter23-4-5_4}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

sim4 <- sim4 %>%
  spread_residuals (mod1, mod2)
sim4

ggplot(sim4, aes(x1, mod1)) + 
  geom_ref_line(h = 0) +
  geom_point() 

ggplot(sim4, aes(x1, mod2)) + 
  geom_ref_line(h = 0) +
  geom_point() 

sim4 %>% summarize ( mod1_rmse = sqrt(mean(mod1^2)),
                     mod2_rmse = sqrt(mean(mod2^2)))
```



# Chapter 24

For this chapter, the following packages and modifications to datasets were used:

```{r}
options(na.action = na.warn)
library(nycflights13)
library(lubridate)

diamonds2 <- diamonds %>% 
  filter(carat <= 2.5) %>% 
  mutate(lprice = log2(price), lcarat = log2(carat))
```


## 24.2.3 Exercises

### 1. In the plot of lcarat vs. lprice, there are some bright vertical strips. What do they represent?

The color of each hexagon in geom_hex corresponds to the number of observations that lie within the hexagon, which means that the bright vertical strips represent highly concentrated areas containing large amounts of observations relative to the dim hexagons. Hadley alludes to the possible reason why these stripes exist in the previous chapters, where he mentions that humans are inclined to report "pretty" intervals such as 1.0, 1.5, 2.0, etc, resulting in more observations lying on these intervals.

```{r chapter24-2-3_1}
ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50)
```

### 2. If log(price) = a_0 + a_1 \* log(carat), what does that say about the relationship between price and carat?

This equation suggests that there is a linear relationsihp between log(price) and log(carat), because the equation can be interpreted as a_0 being an intercept and a_1 being a "slope". It is harder to discern the relationship between the non-logged price and carat given this equation. The relationship between the original non-transformed variables may not necessarily be linear.

### 3. Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors?

Hadley extracts the diamonds with high / low residuals (abs(lresid2) > 1) in the code below. After examining these, we find that most of the outlier diamonds that are priced much lower than predicted have a specific flaw associated with them. For example, \$1262 predicted to be \$2644 has a huge carat size but has a clarity of I1 (worst clarity), of which there are not that many observations for in the dataset (707 observations for grade I1 vs 13055 observations for grade SI1). 

The diamonds in this list usually have a combination of very good qualities as well as very bad qualities. It could be that there is some multicollinearity in the model, in which some of the predictor variables (lcarat, color, cut, and clarity) are correlated with one another. This may result in the model breaking down when something when the values of two variables which usually are correlated do not follow the trend for any specific observation. For example, the diamond priced at 2366 is predicted to only be 774, but this is likely due to the fact that the diamond has one of the best clarity values, but has the worst possible color. Looking at a density plot of color vs clarity, we find that there are very few observations which have this combination of color and clarity, which may be why the model breaks down.

```{r chapter24-2-3_3}
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2)

diamonds2 <- diamonds2 %>% 
  add_residuals(mod_diamond2, "lresid2")

ggplot(diamonds2, aes(lcarat, lresid2)) + 
  geom_hex(bins = 50)

diamonds2 %>% 
  filter(abs(lresid2) > 1) %>% 
  add_predictions(mod_diamond2) %>% 
  mutate(pred = round(2 ^ pred)) %>% 
  select(price, pred, carat:table, x:z, lresid2) %>% 
  arrange(price)

diamonds2 %>% group_by(clarity) %>% summarize (num = n())

diamonds2 %>% ggplot(aes(color, clarity))+
  geom_bin2d()
```


### 4. Does the final model, mod_diamond2, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond?

Judging by the plot of the residuals, the model does a decent job at removing the patterns in the data (fairly flat, only a handful of residuals > 1 st. deviation away from 0) for the log-transformed version of the data. The model could be improved to reduce the variance of the residuals (compress the points toward h=0), in order to get more accurate predictions. However, since we aren't dealing with log-transformed money when buying diamonds in the real world, we should examine how the residuals look when transformed back into their true values. To do so, we calculate subtract the non-logged prediction value from the true value to get the non-transformed residual. Plotting these non-transformed residuals shows that the variability in residuals increases as carat increases (same is true for lcarat). I would have moderate faith in the model for diamonds less than 1 carat, but for diamonds greater than one carat, I would be cautious.  The model would be useful to determine whether you were being completely scammed, but it may not be that good for determining small variations in price.

```{r chapter24-2-3_4}
nontransformed_residual <- diamonds2 %>%   
    filter(abs(lresid2) < 1) %>% 
    add_predictions(mod_diamond2) %>% 
    mutate(pred = round(2 ^ pred)) %>%
    mutate(resid = price - pred) 

nontransformed_residual %>% ggplot( aes (carat, resid) )+
  geom_hex()

nontransformed_residual %>% 
  summarize (resid_mean = mean(resid),
             resid_sd = sd(resid),
             resid_interval_low = mean(resid) - 1.96* sd(resid),
             resid_interval_high = mean(resid) + 1.96* sd(resid),
             limit_upper = max(resid),
             limit_lower = min(resid),
             maxprice = max(price),
             minprice = min(price))

```


## 24.3.5 Exercises

```{r chapter_24-3-5_examples}
daily <- flights %>% 
  mutate(date = make_date(year, month, day)) %>% 
  group_by(date) %>% 
  summarise(n = n())
daily

ggplot(daily, aes(date, n)) + 
  geom_line()

daily <- daily %>% 
  mutate(wday = wday(date, label = TRUE))
mod <- lm(n ~ wday, data = daily)

daily <- daily %>% 
  add_residuals(mod)
daily %>% 
  ggplot(aes(date, resid)) + 
  geom_ref_line(h = 0) + 
  geom_line()

term <- function(date) {
  cut(date, 
    breaks = ymd(20130101, 20130605, 20130825, 20140101),
    labels = c("spring", "summer", "fall") 
  )
}

daily <- daily %>% 
  mutate(term = term(date)) 

```

### 1. Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalise to another year?

These days are all the day before major US holidays (Sep 2, 2013 is Labor Day) which fall exclusively on Mondays. There may be fewer than expected flights due to the extended weekend / other holiday-associated factors. For another year, these days can be found by looking for the day before the third Monday in January, fourth Monday in May, and first Monday in September.

### 2. What do the three days with high positive residuals represent? How would these days generalise to another year?

These days look like are also associated with major US holidays, with 11/30/2013 and 12/01/2013 being the weekend after Thanksgiving, and 12/28/2013 being the weekend after Christmas. The reason flights peak on these days may be due to families who had visited their relatives leaving to go back home. These can be generalized to another year by looking for the weekends after these holidays, which typically fall after the 4th week of November and December.

```{r}
daily %>% 
  top_n(3, resid)
```

### 3. Create a new variable that splits the wday variable into terms, but only for Saturdays, i.e. it should have Thurs, Fri, but Sat-summer, Sat-spring, Sat-fall. How does this model compare with the model with every combination of wday and term?

In order to split the saturdays into terms, I wrote an annotate_sat() function that takes the wday column and term column from daily and applies the appropriate suffix to each "Sat". Fitting this model (mod3) and comparing it to the mod1 and mod2 described in the chapter shows that there is a slight improvement from mod 1 (no term variable), but that the model does slightly worse than mod2 (each day is termed). The RMSE is midway between mod1 and mod2. 

```{r chapter24-3-5_3}
annotate_sat <- function (wday, term) {
  index <- which (wday == "Sat")
  wday <- as.character(wday)
  wday[index] <- str_c("Sat-", as.character(term)[index])
  wday
}

daily <- daily %>% mutate (wday_sat = annotate_sat(wday,term))
daily

mod1 <- lm(n ~ wday, data = daily)
mod2 <- lm(n ~ wday * term, data = daily)
mod3 <- lm(n ~ wday_sat, data = daily)

daily %>% 
  gather_residuals(no_term = mod1, all_cominbations = mod2, only_sat_term = mod3) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

sigma(mod1)
sigma(mod2)
sigma(mod3)

```


### 4. Create a new wday variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like?

The code below adds a column to the data frame that indicates which days are holidays (I've chosen the main US corporate holidays for 2013). Fitting a model to this results in an rmse value that is lower than the model in which only the saturdays are termed (goes down from 47.36 to 42.94), suggesting that we are doing at least a slightly better job. Looking at the graph, the residuals that spike along the holidays are smaller (but still quite visible). There could be more ways to optimize the model to minimize these residuals, suchs as annotating wday with the few days before/after each holiday.

```{r chapter24-3-5_4}
daily
annotate_sat_holiday <- function (date, wday, term) {
  index <- which (wday == "Sat")
  wday <- as.character(wday)
  wday[index] <- str_c("Sat-", as.character(term)[index])
  holidays <- ymd(20130101, 20130527, 20130704, 20130902, 20131111, 20131128, 20131225)
  holday_index <- which (date %in% holidays)
  wday[which (date %in% holidays)] <- "holiday"
  wday
}

daily <- daily %>% mutate( wday_sat_holiday = annotate_sat_holiday(date,wday,term))
daily

mod4 <- lm(n ~ wday_sat_holiday, data = daily)

daily %>% 
  gather_residuals(only_sat_term = mod3, sat_with_holiday = mod4) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

sigma(mod3)
sigma(mod4)

```

### 5. What happens if you fit a day of week effect that varies by month (i.e. n ~ wday * month)? Why is this not very helpful?

Adding the month variable to the model reduces the values of the residuals by a small amount (and subsequently, the rmse). I would say that it is slightly helpful to include. I can see why it is not extremely helpful, because it may be over-fitting the data, as there is not necessarily a good reason to assume that each individual month is associated with their own unique day-of-the-week trends. This adds a large amount of predictor variables to the model that are not all necessarily independent from each other.

```{r chapter24-3-5_5}
# add month column to daily
daily <- daily %>% mutate(month = factor(month(date)))
daily

mod5 <- lm(n ~ wday*month, data = daily)

daily %>% 
  gather_residuals(original_model = mod1,  with_month = mod5) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

#summary(mod5)
sigma(mod1)
sigma(mod5)

```


### 6. What would you expect the model n ~ wday + ns(date, 5) to look like? Knowing what you know about the data, why would you expect it to be not particularly effective?

The book displays what the model n ~ wday \* ns(date, 5) looks like, which factors in the relationship between wday and the time of year into the model. The model n ~ wday + ns(date, 5) uses the "+" operator instead of "\*", which means that this model does not account for the relationship between these two variables. I would expect that n ~ wday + ns(date, 5) does a worse job as a predictive model. Testing this out below, we see that the residuals are indeed larger. The number of predictor variables are also much fewer.

```{r chapter24-3-5_6}
library(splines)
mod6 <- MASS::rlm(n ~ wday * ns(date, 5), data = daily)
mod7 <- MASS::rlm(n ~ wday + ns(date, 5), data = daily)

summary(mod6)
summary(mod7)

daily %>% 
  gather_residuals(multiplicative_splines = mod6,  additive_splines = mod7) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)

sigma(mod6)
sigma(mod7)
```


### 7. We hypothesised that people leaving on Sundays are more likely to be business travellers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away.

To explore this hypothesis, we first generate a new data frame from flights that contains a factor splitting up each day into early morning (12am-8am, 8am-12pm, 12pm-5pm, and 5pm-12am). We can then summarize the data to obtain the mean distance of flights that occurred during each of those intervals grouped by day. Plotting this using boxplots, we observe that flights taking off between 1am and 8am on Sundays have a much higher mean distance compared to all other categories. Contrary to our hypothesis that evening flights (5pm-12am) would be the longest distance-wise, we instead see that there are more Sunday early morning flights between 12am-8am.

```{r chapter24-3-5_7}
# add distance and time of flight to daily matrix
timeofday <- function(date) {
  cut(date, 
    breaks = c(0,7, 12, 17, 23),
    labels = c("12am-8am", "8am-12pm", "12pm-5pm","5pm-12am") 
  )
}

flights2 <- flights %>% mutate (date = make_date (year, month, day), timeofday = timeofday(hour)) %>% 
  group_by(date, timeofday) %>% 
  summarize (n = n(),
             mean_dist = mean(distance)) %>%
  mutate(wday = wday(date, label = TRUE))

flights2

ggplot(flights2, aes (x = wday, y = mean_dist))+
  geom_boxplot(aes(color = timeofday))
# 
# flights %>% mutate (date = make_date (year, month, day), timeofday = timeofday(hour)) %>% 
#   mutate(wday = wday(date, label = TRUE))%>%
#   ggplot( aes (x = wday, y = distance))+
#   geom_boxplot(aes(color = timeofday))
```


### 8. It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday.

The levels of a factor can be re-ordered using the factor() function and redefining the order of the levels by manually passing them into the levels argument. Below is an example of a function reordering the factors so that the week starts on monday.

```{r chapter24-3-5_8}
# before reordering the factors
ggplot(daily, aes(wday, n)) + 
  geom_boxplot()

reorder_week <- function (mydata) {
  mydata <- factor(mydata, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))
  mydata
}

# after reordering the factors
daily %>% mutate (wday2 = reorder_week(wday)) %>% ggplot( aes(wday2, n)) + 
  geom_boxplot()
```



# Chapter 25

```{r}
library(gapminder)
by_country <- gapminder %>% 
  group_by(country, continent) %>% 
  nest()

by_country

## wrapper function to model lifeExp by year
country_model <- function(df) {
  lm(lifeExp ~ year, data = df)
}

## add the model as a list-column to the nested data
by_country <- by_country %>% 
  mutate(model = map(data, country_model))
by_country

## calculate residuals based on the nested data and the associated model
by_country <- by_country %>% 
  mutate(
    resids = map2(data, model, add_residuals)
  )
by_country
resids <- unnest(by_country, resids)
resids %>% group_by(country) %>% summarise (rsme = sqrt(mean(resid^2)))

```


## 25.2.5 Exercises

### 1. A linear trend seems to be slightly too simple for the overall trend. Can you do better with a quadratic polynomial? How can you interpret the coefficients of the quadratic? (Hint you might want to transform year so that it has mean zero.)

We can fit a quadratic polynomial either by using the equation y ~ poly(x,2) or writing it out as y ~ I(x^2) + x. When we fit this model to the data, we see that the majority of the rmse values calculated using the residuals by country are reduced compared to the original linear model used in this chapter. This suggests that we are doing better.

```{r chapter25-2-5_1}
country_model_poly <- function(df) {
  lm(lifeExp ~ poly(year,2), data = df)
  # alternatively can use lm(lifeExp ~ I(year^2)+year, data = df) for raw polynomial
}

country_model_poly_centered <- function(df) {
  lm(lifeExp ~ poly(year-mean(year),2), data = df)
}

by_country <- by_country %>% 
  mutate(model2 = map(data, country_model_poly),
         model3 = map(data, country_model_poly_centered)) %>%
  mutate(resids2 = map2(data, model2, add_residuals))

## residual freq-poly plot
resids2 <- unnest(by_country, resids2)
resids2 %>% 
  ggplot(aes(year, resid)) +
    geom_line(aes(group = country), alpha = 1 / 3) + 
    geom_smooth(se = FALSE)

## R-squared value
glance <- by_country %>% 
  mutate(glance = map(model2, broom::glance)) %>% 
  unnest(glance, .drop = TRUE) 

glance %>% ggplot(aes(continent, r.squared)) + 
    geom_jitter(width = 0.5)
  
# rmse using the polynomial model
resids2 %>% group_by(country) %>% summarise (rmse = sqrt(mean(resid^2)))
# rmse using the original linear model
resids %>% group_by(country) %>% summarise (rmse = sqrt(mean(resid^2)))
```

When we compare the model summaries for the centered (mean 0) model vs the non-centered model, the estimates for the coefficients are the same. Poly() creates orthogonal polynomials for the fit. The coefficients may be interpreted as the weight associated with a unit of change in year.
 
```{r}
summary(by_country$model2[[1]])
summary(by_country$model3[[1]])
```


### 2. Explore other methods for visualising the distribution of R2 per continent. You might want to try the ggbeeswarm package, which provides similar methods for avoiding overlaps as jitter, but uses deterministic methods.

To use the ggbeeswarm package, simply replace geom_jitter() with geom_beeswarm(). The output is below. We can augment the graph by adding additional graphics such as a boxplot.

```{r chapter25-2-5_2}
library(ggbeeswarm)
glance %>% 
  ggplot(aes(continent, r.squared)) + 
    geom_beeswarm()

glance %>% 
  ggplot(aes(continent, r.squared)) + 
    geom_boxplot( aes(color = continent))+
    geom_beeswarm()

```

### 3. To create the last plot (showing the data for the countries with the worst model fits), we needed two steps: we created a data frame with one row per country and then semi-joined it to the original dataset. It’s possible to avoid this join if we use unnest() instead of unnest(.drop = TRUE). How?

Instead of using unnest(glance,.drop = TRUE), using unnest(glance) will retain the other columns in the dataset in addition to unnesting the glance column. This means that the list-column "data" will still be associated with the glance output, which contains the `year` and `lifeExp` variables that are needed to plot the final graph. We can then perform the filtering on the r.squared value, unnest the data, and then plot the graph, as shown below, without needing to semi-join.

```{r chapter25-2-5_3}
glance <- by_country %>% 
  mutate(glance = map(model, broom::glance)) %>%
  unnest(glance)
glance

bad_fit <- filter(glance, r.squared < 0.25)
bad_fit

bad_fit %>% unnest (data) %>%
  ggplot(aes(year, lifeExp, colour = country)) +
    geom_line()
```


## 25.4.5 Exercises

### 1. List all the functions that you can think of that take a atomic vector and return a list.

enframe() or as_tibble() will convert an atomic vector into a list. The map() function will return a list in which it applies a function that you specify on each element of the atomic vector (examples below). stringr functions return lists of strings, such as str_split(). You may also encounter package-specific functions which may create their own types of objects. For example, in the bioinformatics world, you may have DESeq2 objects (bulk RNA-sequencing) or Seurat objects (single cell RNA-sequencing), which are S4 objects which contain several data types, including lists. Other functions that return lists are those shown in this chapter, such as broom::glance(), although in this instance the input is a model and not an atomic vector. split() also returns a list when applied to a dataframe.

```{r}
my_atomic_vector <- c("hello","world")

map(my_atomic_vector, length)
str_split(my_atomic_vector, "l")

typeof(as_tibble(my_atomic_vector))
typeof(enframe(my_atomic_vector))
summary(my_atomic_vector)

mtcars %>% 
  split(.$cyl)
```


### 2. Brainstorm useful summary functions that, like quantile(), return multiple values.

Summary functions in addition to quantile() include: summary(), range(), dim(), and coef() for linear models.

```{r}
x <- 1:10
summary(x)
range(x)
dim(mtcars)
coef(lm(mpg ~cyl, data = mtcars))
```


### 3. What’s missing in the following data frame? How does quantile() return that missing piece? Why isn’t that helpful here?

The data frame shows the output of unnesting the column `q`, which contains the quantile() values for each cylinder type in mtcars stored as a list-column. The important information that is missing is the label corresponding to each of the quantile values. For example, 21.4 (the first entry) corresponds to the 0% quantile. Without knowing how the function works, someone looking at the table would not know this important information. quantile() returns this information by naming the output vector with these labels. This is not helpful when creating list-columns, because the names are not stored automatically. 

```{r}
# what quantile should return
quantile(mtcars[which(mtcars$cyl==4),"mpg"])
names(quantile(mtcars[which(mtcars$cyl==4),"mpg"]))

# quantile is missing the labels for each of the statistics (0%, 25%, ...)
mtcars %>% 
  group_by(cyl) %>% 
  summarise(q = list(quantile(mpg))) %>% 
  unnest()

mtcars %>% 
  group_by(cyl) %>% 
  summarise(q = list(quantile(mpg))) %>% 
  unnest()
```

### 4. What does this code do? Why might might it be useful?

This code will group the dataset mtcars by cylinder type (in this case, 4, 6, or 8), then aggregate the values for each of the columns into a list. Specifically, there are 11 rows in the dataset that have a cyl value of 4, 7 rows that have a cyl value of 6, and 14 rows with a cyl value of 8. This is why the entries now look like <dbl [11]>, <dbl [7]>, and <dbl [14]>, respectively. Retrieving the value of <dbl[11]> at the first row of column `mpg`, for example, will return the 11 values associated with cylinder 4.

```{r}
mtcars2 <- mtcars %>% 
  group_by(cyl) %>% 
  summarise_each(funs(list))
mtcars2
mtcars2$mpg[1]
```

## 25.5.3 Exercises

### 1. Why might the lengths() function be useful for creating atomic vector columns from list-columns?

The lengths() function will return the length of each element in a list. This differs from the length() function which will return the number of elements contained in a list. For an example of the differences, see the code chunk below. However, for whatever reason, when lengths() is used with mutate() on a list-column, it will return the number of elements in the list rather than returning the vector with the lengths for each element in the list. In other words, when used with mutate(), lengths() performs what we normally would have thought length() would do. Even more confusing is how when length() is used with mutate, it breaks down and now returns the number of rows in the data frame regardless of what is stored in each row.

```{r}
lengths(list(a = 1:10, b = 2, z = 5))
length(mtcars)
lengths(mtcars)

df <- tribble(
  ~x,
  list(a = 1:10, b = 2, z = 5),
  list(a = 2:5, c = 4),
  list(a = 1, b = 2)
)

# lengths() returns the number of elements in the list-column
df %>% mutate(
  length_of_a = lengths(x)
)

# length() returns the number of rows regardless of the value in the list-column
df %>% mutate(
  length_of_a = length(x)
)

```

### 2. List the most common types of vector found in a data frame. What makes lists different?

In a data frame, I usually encounter numerics, characters, and factors, in which there is just a single value at any row x column designation. Lists are different because they can contain any number of data types, and multiple values for each. You can even have a list of lists! This means that a very diverse data set can be stored within lists in a single column of your data frame. That is the beauty of list-columns that this chapter tries to highlight.


# Chapter 26 - No Exercises

# Chapter 27

## 27.2.1 Exercises

### 1. Create a new notebook using File > New File > R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output.

This document is an example of this!

### 2. Create a new R Markdown document with File > New File > R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update.

This document was knit as a github_document, which will output a markdown file that is compatible with for display on github.

### 3. Compare and contrast the R notebook and R markdown files you created above. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other?

R notebooks create and update a separate .html file as you execute the chunks. In contrast, the R markdown file will not create the .html counterpart unless the document is knit and the output is specified to be html. There are also differences in how the output is displayed in RStudio as the chunks are run, in which notebooks show output directly after each chunk. When you copy the YAML header from R notebook to an R markdown, the document now turns into an R notebook.

### 4. Create one new R Markdown document for each of the three built-in formats: HTML, PDF and Word. Knit each of the three documents. How does the output differ? How does the input differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.)

The output is generally the same, except that each document is of a different type. The input differs in how you specify the output in the YAML header, or on the option you select when manually clicking the knit button.

## 27.3.1 Exercises

### 1. Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold.

I've intentionally left this question unanswered! But since R markdown is like any other markdown language with a set of formatting rules and options, a very simple but well-organized CV can be made in Rstudio.

### 2. Using the R Markdown quick reference, figure out how to:

* Add a footnote.
* Add a horizontal rule.
* Add a block quote.

How to do the above:

* To add a footnote, use `[^footnote-name]` coupled with `[^footnote-name]:` at the bottom of the document, in which the text of the footnote comes after the colon.
* A horizontal rule can be added using at least three hyphens in succession (or asterisks, whichever you prefer): `---`.
* A block quote can be added by prefacing the text using using `> `.


### 3. Copy and paste the contents of diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown in to a local R markdown document. Check that you can run it, then add text after the frequency polygon that describes its most striking features.

Below are the contents of diamond-sizes.Rmd.

```{r}
smaller <- diamonds %>% 
  filter(carat <= 2.5)
```

We have data about `r nrow(diamonds)` diamonds. Only 
`r nrow(diamonds) - nrow(smaller)` are larger than
2.5 carats. The distribution of the remainder is shown
below:

```{r diamond-sizes_example_chapter27-3-1_3, echo = FALSE}
smaller %>% 
  ggplot(aes(carat)) + 
  geom_freqpoly(binwidth = 0.01)+
  labs(title = "Amount of diamonds in inventory by carat size")
```

Here is my commentary on the output:

From the graph, we observe immediately that there are spikes along the frequency polygon, representing diamonds of a specific carat that are over-represented in the dataset. Looking closely, we observe that the spikes lie where there are "round" values of carat size, such as 0.5, 1, 1.5, etc. It is likely that this is a result of human rounding tendencies.


## 27.4.7 Exercises

### 1. Add a section that explores how diamond sizes vary by cut, colour, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo = FALSE on each chunk, set a global option.

To set a global option to hide code chunks, we can include the following code chunk:

```
knitr::opts_chunk$set(
  echo = FALSE
)
```

For the learning experiences of you readers out there, I will continue to display the code that generates the graphs that explore how diamond sizes vary by cut, colour, and clarity.

```{r chapter27-4-7_1}
# diamond size by cut
diamonds %>% ggplot (aes(x = cut, y = carat))+
  geom_boxplot()

# diamond size by colour
diamonds %>% ggplot (aes(x = color, y = carat))+
  geom_boxplot()

# diamond size by clarity
diamonds %>% ggplot (aes(x = clarity, y = carat))+
  geom_boxplot()

```


### 2. Download diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes.

To filter out the top 20 largest diamonds, we can arrange the data table using arrange() so that the largest diamonds are at the top, then take the first 20 entries using head(). To display a table with their most important attributes, we can use knitr::kable() for carat, cut, color, clarity, and price, which I believe to be important based on the qualities they convey about the diamond.

```{r}
# filter out the largest 20 diamonds

largest <- diamonds %>% arrange(desc(carat)) %>% head (20)

# display a table, using kable() to make it prettier

largest %>% select (carat, cut, color, clarity, price) %>% knitr::kable (caption = "important qualities of top 20 diamonds")
```


### 3. Modify diamonds-sizes.Rmd to use comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats.

Below is the formatted output, using the comma() function specified in the chapter with an additional sentence stating the percentage of diamonds that are larger than 2.5 carats (the new sentence is bolded).

```{r}
comma <- function(x) format(x, digits = 2, big.mark = ",")
```

```{r}
smaller <- diamonds %>% 
  filter(carat <= 2.5)
```

We have data about `r comma(nrow(diamonds))` diamonds. Only `r comma(nrow(diamonds) - nrow(smaller))` are larger than 2.5 carats. 
_The percentage of diamonds that are larger than 2.5 carats is `r (nrow(diamonds) - nrow(smaller))/nrow(diamonds)*100`%._
The distribution of the remainder is shown below:

```{r diamonds-sizes_chapter27-4-7_3, echo = FALSE}
smaller %>% 
  ggplot(aes(carat)) + 
  geom_freqpoly(binwidth = 0.01)+
  labs(title = "Amount of diamonds in inventory by carat size")
```


### 4. Set up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache = TRUE, then verify your understanding of caching.

```{r chunk-a, cache = TRUE}
a_variable <- 2
print(paste("a:", a_variable))
lubridate::now()
```

```{r chunk-b, cache = TRUE, dependson = c("chunk-a")}
b_variable <- 5 * a_variable
print(paste("b:", b_variable))
lubridate::now()
```

```{r chunk-c, cache = TRUE, dependson = c("chunk-a")}
c_variable <- 10 * a_variable
print(paste("c:", c_variable))
lubridate::now()
```

```{r chunk-d, cache = TRUE, dependson = c("chunk-b", "chunk-c")}
d_product <- b_variable * c_variable
print(paste("d:", d_product))
lubridate::now()
```

The output of these code chunks will be cached since we set `cache = TRUE`. If we re-knit this document without changing anything in the code chunks, the value of lubridate::now() that is printed to the screen should not change, since the cached values will be used.



# Chapter 28

## 28.2.1 Exercises

### 1. Create one plot on the fuel economy data with customised title, subtitle, caption, x, y, and colour labels.

To do this, we use add labs() to the ggplot along with the title, subtitle, caption, x, y, and color arguments. I've filled them in using what was provided in the book.

```{r chapter28-2-1_1}
ggplot(data = mpg, aes(x = displ, y = hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight",
    caption = "Data from fueleconomy.gov",
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    color = "Car type"
  )
```


### 2. The geom_smooth() is somewhat misleading because the hwy for large engines is skewed upwards due to the inclusion of lightweight sports cars with big engines. Use your modelling tools to fit and display a better model.

A better model would be to use a simple linear model (straight line fit) between hwy and displ. To do this the fast way, we could just specify `geom_smooth(method = 'lm')`. If we wanted to use our modeling tools, we could manually specify the model using lm(), then fit the predicted values on top of the graph. There are even two ways to do this, either by manually plotting the slope and intercept using geom_abline(), or by generating predictions for a grid of displ values and plotting the line using geom_line().

```{r chapter28-2-1_2}
# using geom_smooth(method = "lm")
ggplot(data = mpg, aes(x = displ, y = hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE, method="lm") +
  labs(
    title = "Plotted using geom_smooth(method = 'lm', se = F)"
  )


# using geom_line()
mpg_mod <- lm(hwy ~ displ, data = mpg)

grid <- mpg %>% 
  data_grid(displ) %>% 
  add_predictions(mpg_mod) 
grid

ggplot(mpg, aes(x=displ)) +
  geom_point(aes(y = hwy, color = class)) +
  geom_line(aes(y = pred), data = grid) +
  labs(
    title = "Plotted using lm(), grid(), add_predictions(), and geom_line()"
  )


# using geom_abline()
ggplot(mpg, aes (x=displ, y=hwy)) +
  geom_point(aes(color = class))+
  geom_abline(intercept = mpg_mod$coefficients[1], slope = mpg_mod$coefficients[2]) +
  labs(
    title = "Plotted using lm() and geom_abline() with the coefficients of the model",
    subtitle = "notice how the line extends to the edges of the graph"
  )

```


### 3. Take an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand.

I've taken a plot from earlier in this document comparing diamond size by quality of cut and plotted it with labels to make it easier to understand.

```{r chapter28-2-1_3}
diamonds %>% ggplot (aes(x = cut, y = carat))+
  geom_boxplot(aes(color = cut))+
  labs(title = "Diamonds with higher quality cuts are generally smaller",
       subtitle = "Boxplot of diamond carat size vs quality of cut",
       x = "Quality of cut",
       y = "Carat",
       caption = "data obtained from ggplot2 built-in dataset 'diamonds'")
```




## 28.3.1 Exercises

### 1. Use geom_text() with infinite positions to place text at the four corners of the plot.

The example in the chapter uses a tibble to store one label with its associated parameters, which hints to us that we can populate this tibble with multiple labels with any parameters of our choosing. For this exercise, we want to place the labels in each of four corners. To do this, we can add more entries (rows) to the tibble that correspond to these labels and their locations. Then, we can pass this tibble of labels into geom_text().

```{r chapter28-3-1_1}
label <- tibble(
  displ = c(-Inf, Inf, -Inf, Inf),
  hwy = c(Inf, Inf, -Inf, -Inf),
  label = c("topleft", "topright", "bottomleft", "bottomright"),
  hjust = c("left", "right", "left", "right"),
  vjust = c("top", "top", "bottom", "bottom")
)
label
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label, hjust = hjust, vjust = vjust), data = label)
  
```


### 2. Read the documentation for annotate(). How can you use it to add a text label to a plot without having to create a tibble?

annotate() adds a geom of your choice to the plot, but instead of supplying the parameters in a data frame, you can supply it using vectors. This is useful if you just want to add a simple text label to a part of the plot without having to make a tibble with one row. An example is below.

```{r chapter28-3-1_2}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = cyl)) +
  annotate("text", x = 6, y = 40, label = "I added this using annotate()")
```


### 3. How do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: think about the underlying data.)


Labels using geom_text() get placed at the same location on each facet if the x / y coordinates are hard coded (in this instance, they are hard coded as x = 6, y = 40). If only a single label is provided, it is duplicated for each facet.

```{r chapter28-3-1_3-1}
# add to all facets
label <- tibble(
  displ = 6,
  hwy = 40,
  label = "my_label"
)
label

# labels get placed at the same location on each facet if the x / y coordinates are hard coded
ggplot (mpg, aes(displ, hwy))+
  geom_point()+
  geom_text(data = label, aes(label = label))+
  facet_wrap(~cyl)
```

In order to add the label to just one of the facets, the tibble containing your labels should include the variable you are facetting on (in this case, cyl). We set the value of cyl to 4, in order for the label to show up only in the graph for cyl=4.

```{r chapter28-3-1_3-2}
# add to a single facet
label <- tibble(
  displ = 6,
  hwy = 40,
  cyl = 4,
  label = "my_label for cyl=4"
)
label

ggplot (mpg, aes(displ, hwy))+
  geom_point()+
  geom_text(data = label, aes(label = label))+
  facet_wrap(~cyl)
```

To add a different label to each facet, we can expand the tibble in the above example to have a row for each value of cyl, along with the corresponding unique label.

```{r chapter28-3-1_1-3}
# add unique labels to each facet
label <- tibble(
  displ = 6,
  hwy = 40,
  cyl = c(4,5,6,8),
  label = c("cyl=4","cyl=5","cyl=6","cyl=8")
)
label

ggplot (mpg, aes(displ, hwy))+
  geom_point()+
  geom_text(data = label, aes(label = label))+
  facet_wrap(~cyl)
```


### 4. What arguments to geom_label() control the appearance of the background box?

The fill aesthetic dictates the color of the background of the label. In the example below, I label highest 5 displ data points and fill based on auto or manual. You can also color the text and border of the label using `color`. To get rid of the black border (but keep the fill), use label.size = 0. It may be helpful to call geom_label() before geom_point(), or else the label will be placed over the data point and mask it.

```{r chapter28-3-1_4}
label <- mpg %>% arrange(desc(displ)) %>% head(5)
label

# color code background based on category, if label comes after geom_point(), may mask the point as demonstrated below
ggplot (mpg, aes(displ, hwy))+
  geom_point()+
  geom_label(data = label, aes(label = trans, fill = trans))

# color the text and border, call label before geom_point(), now the points show up over the labels
ggplot (mpg, aes(displ, hwy))+
  geom_label(data = label, aes(label = trans, color = trans))+
  geom_point()

```


### 5. What are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options.

To add an arrow to your plot, you can use geom_segment() with the arrow argument. The arrow option of geom_segment() receives the output of arrow(), which has four arguments: `angle` which describes the width of the arrow head, `length` which is the length of the arrow head from tip to base, `ends` which indicates which sides to put the arrow heads on, and `type` which indicates whether the arrow head should be an open or closed triangle.

Below are examples of how to customize your arrows by toggling the arguments to arrow(). I initially tried to compile all the variations of the arrow arguments in order to plot the different types of arrows all at once using one geom_segment() call (similar to how you can lump multiple labels together at different locations using geom_text()). However it doesn't seem like you can pass a data frame of arguments into the arrow() function by putting it inside aes(). The code below errors and produces segments wihtout arrowheads. So, I resorted to just calling geom_segment 4 times, in order to compare the different arrow types.

```{r chapter28-3-1_5-1}
label <- tibble(
  displ = 6,
  hwy = c(30,33,36,39,42),
  x = 4,
  y = c(30,33,36,39,42),
  xend = 5,
  yend = c(30,33,36,39,42),
  angle = c(30, 10, 30, 30, 30),
  length = c(unit(0.25, "inches"), unit(0.25, "inches"), unit(0.1, "inches"), unit(0.25, "inches"), unit(0.25, "inches")),
  ends = c("last", "last", "last", "both", "last"),
  type = c("open", "open", "open", "open", "closed"),
  label = c("arrowhead with default settings", "angle = 10 degrees", "length = 0.1 inches", "ends = both", "type = closed")
)
label

ggplot (mpg, aes(displ, hwy))+
  geom_point()+
  geom_segment(aes( x = 4, y =30, xend = 5, yend = 30), arrow = arrow())+
  geom_segment(aes( x = 4, y =33, xend = 5, yend = 33), arrow = arrow(angle = 10))+
  geom_segment(aes( x = 4, y =36, xend = 5, yend = 36), arrow = arrow(length = unit(0.1, "inches")))+
  geom_segment(aes( x = 4, y =39, xend = 5, yend = 39), arrow = arrow(ends = "both"))+
  geom_segment(aes( x = 4, y =42, xend = 5, yend = 42), arrow = arrow(type = "closed"))+
  geom_text(data = label, aes(label = label))+
  labs (title = "different types of arrows, plotted on top of a random graph")


# doesn't work if you try to map the arrow arguments using aes()
ggplot (mpg, aes(displ, hwy))+
  geom_point()+
  geom_segment(data = label, aes( x = x, y =y, xend = xend, yend = yend, arrow = arrow(angle = angle, ends = ends, type = type)))+
  geom_text(data = label, aes(label = label))+
  labs (title = "cannot map arguments to arrow() using aes()")
```

Here is a cleaned up version that isn't randomly plotted on top of the mpg dataset.

```{r chapter28-3-1_5-2}
label <- tibble(
  x = 6,
  y = c(17,14,11,8,5),
  label = c("arrowhead with default settings", "angle = 10 degrees", "length = 0.1 inches", "ends = both", "type = closed")
)
label

ggplot (data = label)+
  geom_segment(aes( x = 4, y =17, xend = 5, yend = 17), arrow = arrow())+
  geom_segment(aes( x = 4, y =14, xend = 5, yend = 14), arrow = arrow(angle = 10))+
  geom_segment(aes( x = 4, y =11, xend = 5, yend = 11), arrow = arrow(length = unit(0.1, "inches")))+
  geom_segment(aes( x = 4, y =8, xend = 5, yend = 8), arrow = arrow(ends = "both"))+
  geom_segment(aes( x = 4, y =5, xend = 5, yend = 5), arrow = arrow(type = "closed"))+
  geom_text(data = label, aes(label = label, x = x, y = y))+
  labs (title = "Different types of arrows!")+
  coord_cartesian(xlim = c(3,8), ylim = c(1, 20))

```


Although there wasn't an exercise for this, here are some examples of using the other suggested additions such as: geom_hline() and geom_rect()

```{r chapter28-3-1_5-3}
# adding a horizontal line to focus on points above or below
ggplot (mpg, aes(displ, hwy))+
  geom_point(aes(color = cyl))+
  geom_hline(yintercept = 35, color = "red", size = 1)

# boxing an area of the graph to emphasize what you want to focus on
ggplot (mpg, aes(displ, hwy))+
  geom_point(aes(color = cyl))+
  geom_rect( xmin = 6, xmax = 7.2, ymin = 15, ymax = 30, fill = NA, color = "red")

# use geom_rect with aes() to draw boxes on top of points in a graph
ggplot (mpg, aes(displ, hwy))+
  geom_point(aes(color = cyl))+
  geom_rect( aes (xmin = displ-0.1, xmax = displ+0.1, ymin = hwy-0.1, ymax = hwy+0.1))
```


## 28.4.4 Exercises

### 1. Why doesn’t the following code override the default scale?

This exercise follows from this code in the chapter, which provides the default coloring of the graph:

```{r chapter28-4-4_1-1}
df <- tibble(
  x = rnorm(10000),
  y = rnorm(10000)
)
ggplot(df, aes(x, y)) +
  geom_hex() +
  coord_fixed()
```

The code provided in the exercise does not change the color gradient to white/red. This is because scale_fill_gradient() should be used instead of scale_color_gradient, since the geom_hex() graphics need to be "filled".

```{r chapter28-4-4_1-2}
# does not change the color gradient because 
ggplot(df, aes(x, y)) +
  geom_hex() +
  scale_colour_gradient(low = "white", high = "red") +
  coord_fixed()

# using scale_fill_gradient now changes the colors!
ggplot(df, aes(x, y)) +
  geom_hex() +
  scale_fill_gradient(low = "white", high = "red") +
  coord_fixed()
```


### 2. What is the first argument to every scale? How does it compare to labs()?

Based on the documentation, the first argument to scale_x_continuous() is `name`. If the name is modified by calling scale_x_continuous(name = "my_name"), the modified name "my_name" is displayed instead of the default. The same can be done using labs(), by changing the appropriate label. In the example below, you can use either scale_x_continuous(name="my_name") or labs(x = "my_name") to change the label of the x- axis.

```{r chapter28-4-4_2}
# change x axis label using scale_x_continuous
ggplot(mpg, aes(displ, hwy))+
  geom_point()+
  scale_x_continuous(name = "a new x-axis name using scale_x_continuous()")

# change x axis label using labs()
ggplot(mpg, aes(displ, hwy))+
  geom_point()+
  labs(x = "a new x-axis name using labs()")
```


### 3. Change the display of the presidential terms by:

* Combining the two variants shown above.

The two variants referred to by this question are:

```{r chapter28-4-4_3-1-1}
# variant 1
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y")

# variant 2
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id, colour = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue"))
```

We can see that version 1 has the customized breaks on the x-axis but no coloration, and version 2 has coloration but the breaks on the x-axis are not customized. The combined version with both a customized x-axis as well as coloration is below.

```{r chapter28-4-4_3-1-2}
# combining variant 1 and 2
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id, color = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y") +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue"))

```

* Improving the display of the y axis.

The y-axis can be improved by showing each tick corresponding to a single presidential number using scale_y_continuous and setting the breaks.

```{r chapter28-4-4_3-2}
presidential2 <- presidential %>%
  mutate(id = 33 + row_number())

presidential2 %>%
  ggplot(aes(start, id, color = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y") +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue")) +
    scale_y_continuous(NULL, breaks = presidential2$id)

```


* Labelling each term with the name of the president.

The names of each president are already in the data frame in the `$name` column, so we can use geom_text() setting the labels to `presidential$name`. The default is to put the label where the geom_point() is, which is quite ugly. So, we can change this by adjusting the vjust and hjust parameters, placing the labels below and to the right of each point.

```{r chapter28-4-4_3-3}
presidential2 %>%
  ggplot(aes(start, id, color = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y") +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue")) +
    scale_y_continuous(NULL, breaks = presidential2$id)+
    geom_text(aes(label = presidential$name), vjust = 1.2, hjust = -0.5, show.legend = F)

```


* Adding informative plot labels.

To do this, we remove the "NULL" from scale_x_date() and scale_y_continuous(), then add the labels using labs().

```{r chapter28-4-4_3-4}
presidential2 %>%
  ggplot(aes(start, id, color = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(breaks = presidential$start, date_labels = "'%y") +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue")) +
    scale_y_continuous(breaks = presidential2$id)+
    geom_text(aes(label = presidential$name), vjust = 1.2, hjust = -0.5, show.legend = F)+
    labs (title = "Presidential terms, labeled by party affiliation",
          caption = "data from built-in R dataset: presidential",
          x = "year",
          y = "President number",
          color = "Political party")

```

* Placing breaks every 4 years (this is trickier than it seems!).

I'm assuming that this refers to placing a tick 4 years after the start of any presidential term that lasts longer than 4 years. For example, if a presidential term is 8 years, there should be a tick in the middle. If a term was cut short and was only 5 years, there should still be a tick 4 years after the start date. To do this, we can calculate the diference between each adjacent point in `presidential$start`. If it is longer than 4 years, we insert a tick at 4 years after the start date. To implement this, we can use a simple for loop a long with some conditional statements to check whether each term lasted longer than 4 years.

```{r chapter28-4-4_3-5}
years <- lubridate::year(presidential$start)
appended_years <- vector()

for (i in 1:(length(years)-1)) {
  appended_years <- c(appended_years, presidential$start[i])
  if (years[i+1] - years[i] >4)
    appended_years <- c(appended_years, presidential$start[i] + dyears(4))
  if(i == (length(years)-1))
    appended_years <- c(appended_years, presidential$start[i+1])
}
appended_years
#convert back to a date-time
class(appended_years) <- "Date"

presidential2 %>%
  ggplot(aes(start, id, color = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(breaks = appended_years, date_labels = "'%y") +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue")) +
    scale_y_continuous(breaks = presidential2$id)+
    geom_text(aes(label = presidential$name), vjust = 1.2, hjust = -0.5, show.legend = F)+
    labs (title = "Presidential terms, labeled by party affiliation",
          caption = "data from built-in R dataset: presidential",
          x = "year",
          y = "President number",
          color = "Political party")


```


### 4. Use override.aes to make the legend on the following plot easier to see.

We can use the guides() function with guide_legend(override.aes = list(alpha=1)) to "override" the alpha = 1/20 for the legend. I turn it back to a value of 1 in order to make the points in the legend easier to see.

```{r chapter28-4-4_4}
# points are very faint on the legend
ggplot(diamonds, aes(carat, price)) +
  geom_point(aes(colour = cut), alpha = 1/20)

# change the alpha back to 1 just for the legend to make points visible
ggplot(diamonds, aes(carat, price)) +
  geom_point(aes(colour = cut), alpha = 1/20)+
  guides(color = guide_legend(override.aes = list (alpha = 1)))

```

# Chapters 29, 30 - No Exercises

---

Thanks for reading! This concludes my walkthrough of the book.